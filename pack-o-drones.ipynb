{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# operating modes drive which nets are loaded, trained\n",
    "TURN = 1\n",
    "AVOID = 2\n",
    "ACQUIRE = 3\n",
    "HUNT = 4\n",
    "PACK = 5\n",
    "cur_mode = PACK\n",
    "use_existing_model = True\n",
    "show_sensors = True # Showing sensors and redrawing slows things down.\n",
    "draw_screen = True\n",
    "\n",
    "# object settings\n",
    "NUM_OBSTACLES = 6\n",
    "NUM_CATS = 4\n",
    "NUM_DRONES = 2\n",
    "\n",
    "# turn model settings\n",
    "TURN_NUM_SENSOR = 5 # front five sonar readings\n",
    "TURN_TOTAL_SENSORS = 10 # distance and color values from sonar readings \n",
    "TURN_NUM_OUTPUT = 5 # do nothing, two right turn, two left turn\n",
    "TURN_STATE_FRAMES = 3\n",
    "TURN_NUM_INPUT = TURN_STATE_FRAMES * TURN_TOTAL_SENSORS\n",
    "\n",
    "# avoid model settings\n",
    "AVOID_NUM_SENSOR = 7 # front five, rear two sonar distance readings\n",
    "AVOID_TOTAL_SENSORS = 16 # distance and color values from sonar readings, turn action, current speed\n",
    "AVOID_NUM_OUTPUT = 3 # 30, 50, 70\n",
    "AVOID_STATE_FRAMES = 3\n",
    "AVOID_NUM_INPUT = AVOID_STATE_FRAMES * AVOID_TOTAL_SENSORS\n",
    "SPEEDS = [30,50,70]\n",
    "\n",
    "# acquire model settings\n",
    "ACQUIRE_NUM_SENSOR = 2 # distance, angle\n",
    "ACQUIRE_TOTAL_SENSORS = 2\n",
    "ACQUIRE_NUM_OUTPUT = 5 # nothing, 2 right turn, 2 left turn\n",
    "ACQUIRE_STATE_FRAMES = 2\n",
    "ACQUIRE_NUM_INPUT = ACQUIRE_STATE_FRAMES * ACQUIRE_NUM_SENSOR\n",
    "\n",
    "# hunt model settings\n",
    "HUNT_NUM_SENSOR = 7 # all sonar distance / color readings\n",
    "HUNT_AVOID = 0\n",
    "HUNT_ACQUIRE = 1\n",
    "HUNT_TOTAL_SENSORS = 16 # seven sonar distance + color, target distance + heading\n",
    "HUNT_NUM_OUTPUT = 2 # avoid (model), acquire (model)\n",
    "HUNT_STATE_FRAMES = 3\n",
    "HUNT_NUM_INPUT = HUNT_STATE_FRAMES * HUNT_TOTAL_SENSORS\n",
    "\n",
    "# pack model settings\n",
    "NUM_TARGETS = 1\n",
    "TARGET_RADIUS = 14\n",
    "DRONE_NUM_SENSOR = (2 * NUM_OBSTACLES) + (2 * NUM_CATS) + (2 * NUM_DRONES) + (2 * NUM_TARGETS)\n",
    "# old model: 7 all sonar distance readings, subsequently reduced to four compass readings\n",
    "DRONE_TOTAL_SENSORS = DRONE_NUM_SENSOR\n",
    "PACK_TOTAL_SENSORS = DRONE_TOTAL_SENSORS\n",
    "# these are radian adjustments to first (lhs) and second (rhs) to heading to target. pos is left, neg in right.\n",
    "PACK_HEADING_ADJUST = [[0,0],[1,0],[-1,0],[0,1],[0,-1],[1,1],[-1,-1],[1,-1],[-1,1]]\n",
    "PACK_NUM_OUTPUT = 9\n",
    "PACK_STATE_FRAMES = 5\n",
    "PACK_EVAL_FRAMES = PACK_STATE_FRAMES\n",
    "PACK_NUM_INPUT = PACK_TOTAL_SENSORS\n",
    "START_PACK_ACTION = 0\n",
    "START_DRONE_ID = 0\n",
    "\n",
    "# neural net parms\n",
    "BATCHSIZE = 100\n",
    "PACK_STATEFUL_BATCH_SIZE = 1 \n",
    "TRAIN_BUFFER = 50000\n",
    "PACK_TRAIN_BUFFER = 1\n",
    "\n",
    "# initial settings\n",
    "START_SPEED = 50\n",
    "START_TURN_ACTION = 0\n",
    "START_SPEED_ACTION = 1\n",
    "START_DISTANCE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Neural Nets\n",
    "\n",
    "Section below defines Keras / Theano neural network schemas. Given function decomp, networks have max three hidden layers. LTSM not implemented here. Network \"memory\" managed in learning module via appended state frames. Base design comes from: http://outlace.com/Reinforcement-Learning-Part-3/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "\n",
    "def turn_net(num_inputs, params, num_outputs, load=''):\n",
    "    model = Sequential()\n",
    "\n",
    "    # First layer.\n",
    "    model.add(Dense(params[0], init='lecun_uniform', input_shape=(num_inputs,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Second layer.\n",
    "    model.add(Dense(params[1], init='lecun_uniform'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Output layer.\n",
    "    model.add(Dense(num_outputs, init='lecun_uniform'))\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    rms = RMSprop()\n",
    "    model.compile(loss='mse', optimizer=rms)\n",
    "\n",
    "    if load:\n",
    "        model.load_weights(load)\n",
    "\n",
    "    return model\n",
    "\n",
    "def avoid_net(num_inputs, params, num_outputs, load=''):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First layer.\n",
    "    model.add(Dense(params[0], init='lecun_uniform', input_shape=(num_inputs,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Second layer.\n",
    "    model.add(Dense(params[1], init='lecun_uniform'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Third layer.\n",
    "    model.add(Dense(params[2], init='lecun_uniform'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Output layer.\n",
    "    model.add(Dense(num_outputs, init='lecun_uniform'))\n",
    "    model.add(Activation('linear'))\n",
    "    \n",
    "    rms = RMSprop()\n",
    "    model.compile(loss='mse', optimizer=rms)\n",
    "    \n",
    "    if load:\n",
    "        model.load_weights(load)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def acquire_net(num_inputs, params, num_outputs, load=''):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First layer.\n",
    "    model.add(Dense(params[0], init='lecun_uniform', input_shape=(num_inputs,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Output layer.\n",
    "    model.add(Dense(num_outputs, init='lecun_uniform'))\n",
    "    model.add(Activation('linear'))\n",
    "    \n",
    "    rms = RMSprop()\n",
    "    model.compile(loss='mse', optimizer=rms)\n",
    "    \n",
    "    if load:\n",
    "        model.load_weights(load)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def hunt_net(num_inputs, params, num_outputs, load=''):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First layer.\n",
    "    model.add(Dense(params[0], init='lecun_uniform', input_shape=(num_inputs,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Second layer.\n",
    "    model.add(Dense(params[1], init='lecun_uniform'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Third layer.\n",
    "    model.add(Dense(params[2], init='lecun_uniform'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Output layer.\n",
    "    model.add(Dense(num_outputs, init='lecun_uniform'))\n",
    "    model.add(Activation('linear'))\n",
    "    \n",
    "    rms = RMSprop()\n",
    "    model.compile(loss='mse', optimizer=rms)\n",
    "    \n",
    "    if load:\n",
    "        model.load_weights(load)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# implementation of a stateful LSTM RNN\n",
    "    '''A stateful recurrent model is one for which the internal states (memories) obtained after \n",
    "    processing a batch of samples are reused as initial states for the samples of the next batch. \n",
    "    This allows to process longer sequences while keeping computational complexity manageable.'''\n",
    "\n",
    "    # expected input batch shape: (batch_size, timesteps, data_dim)\n",
    "    # note that we have to provide the full batch_input_shape since the network is stateful.\n",
    "    # the sample of index i in batch k is the follow-up for the sample i in batch k-1.\n",
    "\n",
    "def pack_net(batchsize, num_frames, num_inputs, params, num_outputs, load=''):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(params[0], batch_input_shape=(batchsize, num_frames, num_inputs), \n",
    "                   return_sequences=True, stateful=True))\n",
    "    \n",
    "    # model.add(LSTM(params[0], batch_input_shape=(batchsize, frames, num_inputs)\n",
    "    \n",
    "    # return_sequences=True, if you want this layer to feed the next layer w/ frames/batch intact\n",
    "    # try stateful = True w/ process_pack_minibatch to format x and y in batch context\n",
    "    # If a RNN is stateful, a complete input_shape must be provided (including batch size)\n",
    "    # init='lecun_uniform'?\n",
    "    # activation='sigmoid' (or 'relu', 'linear'), inner_activation='hard_sigmoid'\n",
    "    # add dropout\n",
    "    \n",
    "    model.add(LSTM(params[1], return_sequences=True, stateful=True))\n",
    "    \n",
    "    model.add(LSTM(params[2], stateful=True))\n",
    "    \n",
    "    model.add(Dense(num_outputs, activation='softmax'))\n",
    "\n",
    "    # for multi-class classification http://keras.io/getting-started/sequential-model-guide/\n",
    "    rms = RMSprop()\n",
    "    model.compile(loss='mse', optimizer=rms)\n",
    "    \n",
    "    #model.compile(loss='categorical_crossentropy', optimizer = 'rmsprop') #, metrics=['accuracy']\n",
    "    \n",
    "    if load:\n",
    "        model.load_weights(load)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    turn_model = turn_model_30 = turn_model_50 = turn_model_70 = avoid_model = acquire_model = acquire_model_30 = acquire_model_50 = acquire_model_70 = hunt_model = pack_model = 0\n",
    "        \n",
    "    if cur_mode in [TURN, AVOID, HUNT, PACK]:\n",
    "        nn_param = [TURN_NUM_INPUT*25, TURN_NUM_INPUT*10]\n",
    "        params = {\"batchSize\": BATCHSIZE, \"buffer\": TRAIN_BUFFER, \"nn\": nn_param}\n",
    "            \n",
    "        if cur_mode == TURN and use_existing_model == False:\n",
    "            turn_model = turn_net(TURN_NUM_INPUT, nn_param, TURN_NUM_OUTPUT)\n",
    "        \n",
    "        else:\n",
    "            saved_model = 'models/turn/saved/turn-750-300-100-50000-30-600000.h5'\n",
    "            turn_model_30 = turn_net(TURN_NUM_INPUT, nn_param, TURN_NUM_OUTPUT, saved_model)\n",
    "            saved_model = 'models/turn/saved/turn-750-300-100-50000-50-600000.h5'\n",
    "            turn_model_50 = turn_net(TURN_NUM_INPUT, nn_param, TURN_NUM_OUTPUT, saved_model)\n",
    "            saved_model = 'models/turn/saved/turn-750-300-100-50000-70-600000.h5'\n",
    "            turn_model_70 = turn_net(TURN_NUM_INPUT, nn_param,TURN_NUM_OUTPUT, saved_model)\n",
    "    \n",
    "    if cur_mode in [AVOID, HUNT, PACK]:\n",
    "        nn_param = [AVOID_NUM_INPUT * 25, AVOID_NUM_INPUT * 5, AVOID_NUM_INPUT]\n",
    "        params = {\"batchSize\": BATCHSIZE, \"buffer\": TRAIN_BUFFER, \"nn\": nn_param}\n",
    "            \n",
    "        if cur_mode == AVOID and use_existing_model == False:\n",
    "            avoid_model = avoid_net(AVOID_NUM_INPUT, nn_param, AVOID_NUM_OUTPUT)\n",
    "            \n",
    "        else:\n",
    "            saved_model = 'models/avoid/saved/avoid-1200-240-48-100-50000-700000-old-3L-2022.h5'\n",
    "            avoid_model = avoid_net(AVOID_NUM_INPUT, nn_param, AVOID_NUM_OUTPUT, saved_model)\n",
    "\n",
    "    if cur_mode in [ACQUIRE, HUNT, PACK]:\n",
    "        nn_param = [ACQUIRE_NUM_INPUT * 15, ACQUIRE_NUM_INPUT * 5]\n",
    "        params = {\"batchSize\": BATCHSIZE, \"buffer\": TRAIN_BUFFER, \"nn\": nn_param}\n",
    "            \n",
    "        if cur_mode == ACQUIRE and use_existing_model == False:\n",
    "            acquire_model = acquire_net(ACQUIRE_NUM_INPUT, nn_param, ACQUIRE_NUM_OUTPUT)\n",
    "        \n",
    "        else:\n",
    "            saved_model = 'models/acquire/saved/acquire-60-20-100-50000-50-350000.h5'\n",
    "            # using 50 until time to re-train at 30\n",
    "            acquire_model_30 = acquire_net(ACQUIRE_NUM_INPUT, nn_param,\n",
    "                                           ACQUIRE_NUM_OUTPUT, saved_model)\n",
    "            saved_model = 'models/acquire/saved/acquire-60-20-100-50000-50-350000.h5'\n",
    "            acquire_model_50 = acquire_net(ACQUIRE_NUM_INPUT, nn_param,\n",
    "                                           ACQUIRE_NUM_OUTPUT, saved_model)\n",
    "            saved_model = 'models/acquire/saved/acquire-60-20-100-50000-70-350000.h5'\n",
    "            acquire_model_70 = acquire_net(ACQUIRE_NUM_INPUT, nn_param,\n",
    "                                           ACQUIRE_NUM_OUTPUT, saved_model)\n",
    "        \n",
    "    if cur_mode in [HUNT, PACK]:\n",
    "        nn_param = [HUNT_NUM_INPUT * 25, HUNT_NUM_INPUT * 5, HUNT_NUM_INPUT]\n",
    "        params = {\"batchSize\": BATCHSIZE, \"buffer\": TRAIN_BUFFER, \"nn\": nn_param}\n",
    "            \n",
    "        if cur_mode == HUNT and use_existing_model == False:\n",
    "            hunt_model = hunt_net(HUNT_NUM_INPUT, nn_param, HUNT_NUM_OUTPUT)\n",
    "            \n",
    "        else:\n",
    "            saved_model = 'models/hunt/saved/hunt-1200-240-48-100-50000-300000-40-50-avoid.h5'\n",
    "            hunt_model = hunt_net(HUNT_NUM_INPUT, nn_param, HUNT_NUM_OUTPUT, saved_model)\n",
    "        \n",
    "    if cur_mode == PACK:\n",
    "        nn_param = [PACK_NUM_INPUT * 20, PACK_NUM_INPUT * 5, PACK_NUM_INPUT]\n",
    "        params = {\"batchSize\": PACK_STATEFUL_BATCH_SIZE, \"buffer\": PACK_TRAIN_BUFFER, \"nn\": nn_param}\n",
    "            \n",
    "        if cur_mode == PACK and use_existing_model == False:\n",
    "            pack_model = pack_net(PACK_STATEFUL_BATCH_SIZE, PACK_STATE_FRAMES, \n",
    "                                  PACK_NUM_INPUT, nn_param, PACK_NUM_OUTPUT)\n",
    "        else:\n",
    "            saved_model = 'models/pack/saved/pack-520-130-26-1-1-1000000.h5'\n",
    "            pack_model = pack_net(PACK_STATEFUL_BATCH_SIZE, PACK_STATE_FRAMES, \n",
    "                                  PACK_NUM_INPUT, nn_param, PACK_NUM_OUTPUT, saved_model)\n",
    "\n",
    "    return turn_model, turn_model_30, turn_model_50, turn_model_70, avoid_model, acquire_model, acquire_model_30, acquire_model_50, acquire_model_70, hunt_model, pack_model, params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game\n",
    " \n",
    "1. Establishes the Pygame / Pymunk game state including the board, walls, objects and their characteristics i.e., gravity, elasticity, etc. Pygame \"movements\" are made by updating the \"screen\" (the bottom graphic level) and image \"surfaces\" (layers) above that level. Bind the \"surfaces\" to the screen by \"blit\"-ing. Then \"flip\" the combinded package to print it to the output screen. So, you'll see screen here. You'll also see a variety of surfaces (sometimes called \"grids\") to accomplish those layers.\n",
    "\n",
    "2. Defines changes for each frame (or step). For each frame (or flip or step) of the game, the drone and obstacles are moved based on trajectories and speeds either determined by the game parameters (in the case of \"obstacles\" and \"cats\") or by the neural networks (in the case of \"drones\". Drone moves are in response to game states output from the function below to the neural networks. Game state variables returned depend on the network being trained. For example, the \"turn\" network uses \"sonar\" sensor readings emitted from the drone as it moves. Those sensor readings capture the distance to objects and the color of the objects detected. The network learns risk and specific evasive actions accordingly.\n",
    "\n",
    "Note: while python assumes (0,0) is in the lower left of the screen, pygame assumes (0,0) in the upper left. Therefore, y+ moves DOWN the y axis. Here is example code that illustrates how to handle angles in that environment: https://github.com/mgold/Python-snippets/blob/master/pygame_angles.py. In this implementation, I have flipped the screen so that Y+ moves UP the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chipmunk for Darwin (64bit) [/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/pymunk/libchipmunk.dylib]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pygame\n",
    "from pygame.color import THECOLORS\n",
    "import pymunk\n",
    "from pymunk.vec2d import Vec2d\n",
    "from pymunk.pygame_util import draw\n",
    "import time\n",
    "from math import atan2, degrees, pi, sqrt\n",
    "\n",
    "# ***** initialize variables *****\n",
    "# PyGame init\n",
    "width = 1000\n",
    "height = 700\n",
    "pygame.init()\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "# display surface\n",
    "screen = pygame.display.set_mode((width, height))\n",
    "BACK_COLOR = \"black\"\n",
    "WALL_COLOR = \"red\"\n",
    "DRONE_COLOR = \"green\"\n",
    "SMILE_COLOR = \"blue\"\n",
    "OBSTACLE_COLOR = \"purple\"\n",
    "CAT_COLOR = \"orange\"\n",
    "DRONE_BODY_DIAM = 12\n",
    "SONAR_ARM_LEN = 20\n",
    "OBSTACLE_SIZES = [60, 60, 60]\n",
    "\n",
    "# acquire model settngs\n",
    "target_grid = pygame.Surface((width, height), pygame.SRCALPHA, 32)\n",
    "target_grid.convert_alpha()\n",
    "path_grid = pygame.Surface((width, height))\n",
    "PATH_COLOR = \"grey\"\n",
    "ACQUIRE_PIXEL_COLOR = \"green\"\n",
    "ACQUIRED_PIXEL_COLOR = \"yellow\"\n",
    "ACQUIRE_PIXEL_SIZE = 2\n",
    "\n",
    "if cur_mode == ACQUIRE:\n",
    "    ACQUIRE_PIXEL_SEPARATION = 25\n",
    "    ACQUIRE_MARGIN = 50\n",
    "else:\n",
    "    ACQUIRE_PIXEL_SEPARATION = 5\n",
    "    ACQUIRE_MARGIN = 75\n",
    "\n",
    "# hunt model settings\n",
    "STATS_BUFFER = 2000\n",
    "\n",
    "# Turn off alpha since we don't use it.\n",
    "screen.set_alpha(None)\n",
    "\n",
    "# ***** instantiate the game *****\n",
    "class GameState:\n",
    "    def __init__(self):\n",
    "        # initialize space\n",
    "        self.space = pymunk.Space()\n",
    "        self.space.gravity = pymunk.Vec2d(0., 0.)\n",
    "        \n",
    "        # initialize counters\n",
    "        self.total_frame_ctr = 0\n",
    "        self.replay_frame_ctr = 0\n",
    "        self.acquire_frame_ctr = 0\n",
    "        self.num_off_scrn = 0\n",
    "        \n",
    "        # create drones\n",
    "        self.drones = []\n",
    "        \n",
    "        for drone_id in range(NUM_DRONES):\n",
    "            self.drones.append(self.create_drone(random.randint(400,600),\n",
    "                                                 random.randint(300,400), 0.5))\n",
    "        \n",
    "        self.last_x = np.empty([NUM_DRONES,1])\n",
    "        self.last_y = np.empty([NUM_DRONES,1])\n",
    "        for drone_id in range(len(self.drones)):\n",
    "            x, y = self.drones[drone_id].position\n",
    "            self.last_x[drone_id] = x + 2; self.last_y[drone_id] = y + 2\n",
    "        \n",
    "        # create walls\n",
    "        static = [pymunk.Segment(self.space.static_body,(0, 1), (0, height), 1),\n",
    "                  pymunk.Segment(self.space.static_body,(1, height), (width, height), 1),\n",
    "                  pymunk.Segment(self.space.static_body,(width-1, height), (width-1, 1), 1),\n",
    "                  pymunk.Segment(self.space.static_body,(1, 1), (width, 1), 1)]\n",
    "        \n",
    "        for s in static:\n",
    "            s.friction = 1.\n",
    "            s.group = 1\n",
    "            s.collision_type = 1\n",
    "            s.color = THECOLORS[WALL_COLOR]\n",
    "        self.space.add(static)\n",
    "        \n",
    "        if cur_mode in [TURN, AVOID, HUNT, PACK]:\n",
    "            \n",
    "            self.obstacles = []\n",
    "            self.cats = []\n",
    "            \n",
    "            if cur_mode in [TURN, AVOID, HUNT]:\n",
    "                # create slow, randomly moving, larger obstacles\n",
    "                self.obstacles.append(self.create_obstacle(random.randint(100, width-100),\n",
    "                                                           random.randint(70, height-100),50))\n",
    "                self.obstacles.append(self.create_obstacle(random.randint(100, width-100),\n",
    "                                                           random.randint(70, height-70),50))\n",
    "                self.obstacles.append(self.create_obstacle(random.randint(100, width-100),\n",
    "                                                           random.randint(70, height-70),63))\n",
    "                self.obstacles.append(self.create_obstacle(random.randint(100, width-100),\n",
    "                                                           random.randint(70, height-70),63))\n",
    "                self.obstacles.append(self.create_obstacle(random.randint(100, width-100),\n",
    "                                                           random.randint(70, height-70),30))\n",
    "                self.obstacles.append(self.create_obstacle(random.randint(100, width-100),\n",
    "                                                           random.randint(70, height-70),30))\n",
    "        \n",
    "                # create faster, randomly moving, smaller obstacles a.k.a. \"cats\"\n",
    "                self.cats.append(self.create_cat(width-950,height-100))\n",
    "                self.cats.append(self.create_cat(width-50,height-600))\n",
    "                self.cats.append(self.create_cat(width-50,height-100))\n",
    "                self.cats.append(self.create_cat(width-50,height-600))\n",
    "\n",
    "        if cur_mode in [ACQUIRE, HUNT, PACK]:\n",
    "            \n",
    "            # set up seach grid and feed first target\n",
    "            self.target_inventory = []\n",
    "            self.acquired_targets = []\n",
    "            self.current_targets = []\n",
    "            self.target_radius = TARGET_RADIUS\n",
    "            self.generate_targets(True)\n",
    "            for targets in range(NUM_TARGETS):\n",
    "                self.assign_target(True, drone_id)\n",
    "            self.target_acquired = False\n",
    "            self.obs_adds = 0\n",
    "            \n",
    "            # initialize structures used to track efficiency of EACH move\n",
    "            # distance to target\n",
    "            self.last_tgt_dist = np.empty([NUM_DRONES, NUM_TARGETS])\n",
    "            self.last_tgt_dist.fill(350) # last target dist held in array for ea drone\n",
    "            tmp = [0.5, 1]; self.tgt_deltas = [] # deltas held in list for ea drone\n",
    "            for drone_id in range(NUM_DRONES): self.tgt_deltas.append(tmp)\n",
    "            \n",
    "            # distance to obstacles\n",
    "            self.last_obs_dist = np.empty([NUM_DRONES, NUM_TARGETS])\n",
    "            self.last_obs_dist.fill(10) # last obstacle dist held in array for ea drone\n",
    "            tmp = [10, 12]; self.obs_dists = [] # distances held in list for ea drone\n",
    "            for drone_id in range(NUM_DRONES): self.obs_dists.append(tmp)\n",
    "\n",
    "            # initialize structures to track efficiency of PACK_EVAL_FRAMES moves\n",
    "            if cur_mode == PACK:\n",
    "                # distance to target\n",
    "                self.last_pack_tgt_dist = np.empty([NUM_DRONES, NUM_TARGETS])\n",
    "                self.last_pack_tgt_dist.fill(350) # last target dist held in array for ea drone\n",
    "                tmp = [0.4, 0.7]; self.pack_tgt_deltas = [] # deltas held in list for ea drone\n",
    "                for drone_id in range(NUM_DRONES): self.pack_tgt_deltas.append(tmp)\n",
    "        \n",
    "                # distance to obstacles\n",
    "                self.last_pack_obs_dist = np.empty([NUM_DRONES, NUM_TARGETS])\n",
    "                self.last_pack_obs_dist.fill(10) # last obstacle dist held in array for ea drone\n",
    "                tmp = [10, 12]; self.pack_obs_dists = [] # distances held in list for ea drone\n",
    "                for drone_id in range(NUM_DRONES): self.pack_obs_dists.append(tmp)\n",
    "\n",
    "                # starting positions\n",
    "                self.pack_cum_rwds = np.zeros([NUM_DRONES, 1])\n",
    "                self.start_positions = [(25,25), (25,675), (975,25), (975,650)]\n",
    "                self.start_angles = [0.8, -0.8, 2.5, 3.9]\n",
    "\n",
    "    # ***** primary logic controller for game play *****\n",
    "    def frame_step(self, drone_id, turn_action, speed_action, pack_action, cur_speed, total_ctr, replay_ctr):\n",
    "\n",
    "        self.total_frame_ctr = total_ctr\n",
    "        self.replay_frame_ctr = replay_ctr\n",
    "        self.acquire_frame_ctr += 1\n",
    "        \n",
    "        # turn drone based on current (active) model prediction\n",
    "        if cur_mode in [TURN, AVOID, ACQUIRE, HUNT, PACK]:\n",
    "            self.set_turn(turn_action, drone_id)\n",
    "    \n",
    "        # set speed based on active model prediction\n",
    "        if cur_mode in [AVOID, HUNT, PACK]: # setting speed values directly see SPEEDS\n",
    "            cur_speed = self.set_speed(speed_action, drone_id)\n",
    "        \n",
    "        # effect move by applying speed and direction as vector on self\n",
    "        driving_direction = Vec2d(1, 0).rotated(self.drones[drone_id].angle)\n",
    "        self.drones[drone_id].velocity = cur_speed * driving_direction\n",
    "        x, y = self.drones[drone_id].position\n",
    "        \n",
    "        # set heading adjustment based on pack model output\n",
    "        if cur_mode == PACK:\n",
    "            heading_adjust = self.set_pack_adjust(pack_action)[drone_id]\n",
    "        else:\n",
    "            heading_adjust = 0\n",
    "        \n",
    "        # move obstacles\n",
    "        if cur_mode in [TURN, AVOID, HUNT, PACK]:\n",
    "            if cur_mode != PACK or self.total_frame_ctr > 400: # 400000\n",
    "                if self.total_frame_ctr % 20 == 0: # 20x more stable than drone\n",
    "                    self.move_obstacles(1)\n",
    "                if self.total_frame_ctr % 40 == 0: # 40x more stable than drone\n",
    "                    self.move_cats(1)\n",
    "            elif cur_mode == PACK and self.total_frame_ctr > 300: # 300000\n",
    "                    # slow obstacles\n",
    "                    if self.total_frame_ctr % 30 == 0: \n",
    "                        self.move_obstacles(2)\n",
    "                    # fast obstacles\n",
    "                    if self.total_frame_ctr % 60 == 0:\n",
    "                        self.move_cats(3)\n",
    "            elif cur_mode == PACK and self.total_frame_ctr > 200: # 200000\n",
    "                    if self.total_frame_ctr % 40 == 0: \n",
    "                        self.move_obstacles(3)\n",
    "                    if self.total_frame_ctr % 80 == 0:\n",
    "                        self.move_cats(6)            \n",
    "        \n",
    "        # update the screen and surfaces\n",
    "        if drone_id == 0:\n",
    "            screen.fill(pygame.color.THECOLORS[BACK_COLOR])\n",
    "        \n",
    "        if cur_mode in [ACQUIRE, HUNT, PACK]:\n",
    "            # draw the path drone has taken on the path grid\n",
    "            if self.acquire_frame_ctr / NUM_DRONES > 1.5:\n",
    "                pygame.draw.lines(path_grid, pygame.color.THECOLORS[PATH_COLOR], True,\n",
    "                                  ((self.last_x[drone_id], height - self.last_y[drone_id]),\n",
    "                                   (x, height - y)), 1)\n",
    "            \n",
    "            # if last drone, bind paths, targets to the screen\n",
    "            if drone_id == (NUM_DRONES - 1):\n",
    "                screen.blit(path_grid, (0,0))\n",
    "                screen.blit(target_grid, (0,0))\n",
    "\n",
    "        # display screen\n",
    "        draw(screen, self.space)\n",
    "        self.space.step(1./10) # one pixel for every 10 SPEED\n",
    "        if draw_screen:\n",
    "            pygame.display.flip()\n",
    "\n",
    "        # get readings, build states\n",
    "        self.last_x[drone_id] = x; self.last_y[drone_id] = y\n",
    "        x, y = self.drones[drone_id].position\n",
    "\n",
    "        turn_state, avoid_state, acquire_state, hunt_state, drone_state, min_sonar_dist, avoid_move_efficiency, acquire_move_efficiency = \\\n",
    "            self.build_states(drone_id, turn_action, heading_adjust, cur_speed)\n",
    "        \n",
    "        # calc rewards based on training mode(s) in effect\n",
    "        reward = self.calc_rwd(drone_id, min_sonar_dist, driving_direction, cur_speed, avoid_move_efficiency, acquire_move_efficiency)\n",
    "\n",
    "        #self.total_frame_ctr += 1\n",
    "        clock.tick()\n",
    "\n",
    "        return turn_state, avoid_state, acquire_state, hunt_state, drone_state, reward, cur_speed\n",
    "                \n",
    "    # ***** turn and speed model functions *****\n",
    "    def create_obstacle(self, x, y, r):\n",
    "        obs_body = pymunk.Body(pymunk.inf, pymunk.inf)\n",
    "        obs_shape = pymunk.Circle(obs_body, r)\n",
    "        obs_shape.elasticity = 1.0\n",
    "        obs_body.position = x, y\n",
    "        obs_shape.color = THECOLORS[OBSTACLE_COLOR]\n",
    "        self.space.add(obs_body, obs_shape)\n",
    "        return obs_body\n",
    "    \n",
    "    def create_cat(self,x,y):\n",
    "        inertia = pymunk.moment_for_circle(1, 0, 14, (0, 0))\n",
    "        cat_body = pymunk.Body(1, inertia)\n",
    "        cat_body.position = x, y\n",
    "        cat_shape = pymunk.Circle(cat_body, 20)\n",
    "        cat_shape.color = THECOLORS[CAT_COLOR]\n",
    "        cat_shape.elasticity = 1.0\n",
    "        cat_shape.angle = 0.5\n",
    "        direction = Vec2d(1, 0).rotated(cat_body.angle)\n",
    "        self.space.add(cat_body, cat_shape)\n",
    "        return cat_body\n",
    "    \n",
    "    def create_drone(self, x, y, r):\n",
    "        inertia = pymunk.moment_for_circle(1, 0, 14, (0, 0))\n",
    "        drone_body = pymunk.Body(1, inertia)\n",
    "        drone_body.position = x, y\n",
    "        drone_shape = pymunk.Circle(drone_body, DRONE_BODY_DIAM) # was 25\n",
    "        drone_shape.color = THECOLORS[DRONE_COLOR]\n",
    "        drone_shape.elasticity = 1.0\n",
    "        drone_body.angle = r\n",
    "        driving_direction = Vec2d(1, 0).rotated(drone_body.angle)\n",
    "        drone_body.apply_impulse(driving_direction)\n",
    "        self.space.add(drone_body, drone_shape)\n",
    "        return drone_body\n",
    "    \n",
    "    def move_obstacles(self, divisor):\n",
    "        # randomly moves large, slow obstacles around\n",
    "        if len(self.obstacles) > 0:\n",
    "            for obstacle in self.obstacles:\n",
    "                speed = int(random.randint(10, 15)/divisor)\n",
    "                direction = Vec2d(1, 0).rotated(self.drones[0].angle + random.randint(-2, 2))\n",
    "                obstacle.velocity = speed * direction\n",
    "    \n",
    "    def move_cats(self, divisor):\n",
    "        # randomly moves small, fast obstacles\n",
    "        if len(self.cats) > 0:\n",
    "            for cat in self.cats:\n",
    "                speed = int(random.randint(60, 80)/divisor)\n",
    "                direction = Vec2d(1, 0).rotated(random.randint(-3, 3))\n",
    "                x, y = cat.position\n",
    "                if x < 0 or x > width or y < 0 or y > height:\n",
    "                    cat.position = int(width/2), int(height/2)\n",
    "                cat.velocity = speed * direction\n",
    "\n",
    "    def set_turn(self, turn_action, drone_id):\n",
    "        # action == 0 is continue current trajectory\n",
    "        if turn_action == 1:  # slight right adjust to current trajectory\n",
    "            self.drones[drone_id].angle -= .2\n",
    "        elif turn_action == 2:  # hard right\n",
    "            self.drones[drone_id].angle -= .4\n",
    "        elif turn_action == 3:  # slight left\n",
    "            self.drones[drone_id].angle += .2\n",
    "        elif turn_action == 4:  # hard left\n",
    "            self.drones[drone_id].angle += .4\n",
    "\n",
    "    def set_speed(self, speed_action, drone_id):\n",
    "        # choose appropriate speed action, including 0 speed\n",
    "        if speed_action == 0:\n",
    "            cur_speed = SPEEDS[0]\n",
    "        elif speed_action == 1:\n",
    "            cur_speed = SPEEDS[1]\n",
    "        elif speed_action == 2:\n",
    "            cur_speed = SPEEDS[2]\n",
    "\n",
    "        return cur_speed\n",
    "\n",
    "    def set_pack_adjust(self, pack_action):\n",
    "        \n",
    "        heading_adjust = []\n",
    "        # pack actions effect +/- 0.8 radian (45 deg) drone heading adjustments (2)\n",
    "        for i in range(NUM_DRONES):\n",
    "            heading = PACK_HEADING_ADJUST[pack_action][i]\n",
    "            heading_adjust.append(heading * (3.14 / 4))\n",
    "                \n",
    "        return heading_adjust\n",
    "\n",
    "    def evaluate_move(self, drone_id, heading_adjust, min_sonar_dist):\n",
    "        '''eventually, will introduce multiple targets but each new target doubles \n",
    "        state variables. so, for now, assuming single target:'''\n",
    "        target_id = 0\n",
    "        avoid_move_efficiency = 0\n",
    "        acquire_move_efficiency = 0\n",
    "\n",
    "        # 1. calc distance and angle to active target(s) relative to drone\n",
    "        target_dist, target_rads = calc_dist_rads(self.current_targets[target_id], \n",
    "                                                  self.drones[drone_id].position,0)\n",
    "        \n",
    "        # 2. get adjusted heading\n",
    "        _, adj_target_rads = calc_dist_rads(self.current_targets[target_id], \n",
    "                                            self.drones[drone_id].position,\n",
    "                                            heading_adjust)\n",
    "        \n",
    "        adj_target_degs = np.round(degrees(adj_target_rads),1)\n",
    "        \n",
    "        # ii. relative to drone's current direction\n",
    "        rads = self.drones[drone_id].angle\n",
    "        rads %= 2*pi\n",
    "        drone_angle_rads = rads\n",
    "        drone_angle_degs = np.round(degrees(rads),1)\n",
    "        \n",
    "        if drone_angle_degs > 360:\n",
    "            drone_angle_degs = drone_angle_degs - 360\n",
    "            \n",
    "        # \"heading\" accounts for angle FROM drone and OF drone netting degrees drone must turn\n",
    "        adj_heading_to_target = adj_target_degs - drone_angle_degs\n",
    "        \n",
    "        if adj_heading_to_target < -180:\n",
    "            adj_heading_to_target = adj_heading_to_target + 360\n",
    "        \n",
    "        if cur_mode != PACK:\n",
    "            # 3. calc normalized efficiency of last move\n",
    "            # vs. target acquisition\n",
    "            dt = int(self.last_tgt_dist[drone_id, target_id] - target_dist)\n",
    "            \n",
    "            if abs(dt) >= 20: # mistakenly thinking crashes are moves. so, ignore moves > 12\n",
    "                dt = np.mean(self.tgt_deltas[drone_id])\n",
    "\n",
    "            # postive distance delta indicates \"closing\" on the target\n",
    "            ndt = np.round((dt - np.mean(self.tgt_deltas[drone_id])) / np.std(self.tgt_deltas[drone_id]),2)\n",
    "            \n",
    "            # save current values\n",
    "            self.last_tgt_dist[drone_id, target_id] = target_dist\n",
    "            self.tgt_deltas[drone_id].append(dt)\n",
    "\n",
    "            if len(self.tgt_deltas[drone_id]) > STATS_BUFFER:\n",
    "                self.tgt_deltas[drone_id].pop(0)\n",
    "\n",
    "            # vs. obstacle avoidance\n",
    "            do = min_sonar_dist\n",
    "            \n",
    "            # positive distance delta indicates \"avoiding\" an obstacle\n",
    "            ndo = np.round((do - np.mean(self.obs_dists[drone_id])) / np.std(self.obs_dists[drone_id]),2)\n",
    "            \n",
    "            # save current values\n",
    "            self.last_obs_dist[drone_id] = do\n",
    "            self.obs_dists[drone_id].append(do)\n",
    "\n",
    "            if len(self.obs_dists[drone_id]) > STATS_BUFFER:\n",
    "                self.obs_dists[drone_id].pop(0)\n",
    "            \n",
    "            # finally, apply calcs to score move\n",
    "            if cur_mode == ACQUIRE:\n",
    "                acquire_move_efficiency = np.round(ndt / target_dist**0.333,2)\n",
    "                # cubed root of the target distance... lessens effect of distance\n",
    "            else:\n",
    "                avoid_move_efficiency = np.round(ndo / target_dist**0.333,2) # was 0.333\n",
    "                acquire_move_efficiency = np.round(ndt / target_dist**0.333,2)\n",
    "                # for balancing avoidance with acquisition\n",
    "        \n",
    "        else:\n",
    "            #if self.total_frame_ctr == 1 or self.replay_frame_ctr % PACK_EVAL_FRAMES == 0:\n",
    "            # 3. calc normalized efficiency of last move\n",
    "            # vs. target acquisition\n",
    "            dt = int(self.last_pack_tgt_dist[drone_id, target_id] - target_dist)\n",
    "            \n",
    "            if abs(dt) >= 20: # mistakenly thinking crashes are moves. so, ignore moves > 12\n",
    "                dt = np.mean(self.pack_tgt_deltas[drone_id])\n",
    "                \n",
    "            # postive distance delta indicates \"closing\" on the target\n",
    "            ndt = np.round((dt - np.mean(self.pack_tgt_deltas[drone_id])) / np.std(self.pack_tgt_deltas[drone_id]),2)\n",
    "\n",
    "            # save current values\n",
    "            self.last_pack_tgt_dist[drone_id, target_id] = target_dist\n",
    "            self.pack_tgt_deltas[drone_id].append(dt)\n",
    "            \n",
    "            if len(self.pack_tgt_deltas[drone_id]) > STATS_BUFFER:\n",
    "                self.pack_tgt_deltas[drone_id].pop(0)\n",
    "\n",
    "            # vs. obstacle avoidance\n",
    "            do = min_sonar_dist\n",
    "            \n",
    "            # positive distance delta indicates \"avoiding\" an obstacle\n",
    "            ndo = np.round((do - np.mean(self.pack_obs_dists[drone_id])) / np.std(self.pack_obs_dists[drone_id]),2)\n",
    "            \n",
    "            # save current values\n",
    "            self.last_pack_obs_dist[drone_id] = do\n",
    "            self.pack_obs_dists[drone_id].append(do)\n",
    "        \n",
    "            if len(self.pack_obs_dists[drone_id]) > STATS_BUFFER:\n",
    "                self.pack_obs_dists[drone_id].pop(0)\n",
    "\n",
    "            # finally, apply calcs to score move\n",
    "            avoid_move_efficiency = np.round(ndo / target_dist**0.333,2)\n",
    "            acquire_move_efficiency = np.round(ndt / target_dist**0.333,2)\n",
    "            # for balancing avoidance with acquisition\n",
    "        \n",
    "        # 4. if w/in reasonable distance, declare victory\n",
    "        if target_dist <= TARGET_RADIUS:\n",
    "            print(\"************** target acquired ************\")\n",
    "            self.target_acquired = True\n",
    "        \n",
    "            # move acquired target to acquired targets\n",
    "            self.acquired_targets.append(self.current_targets[target_id])\n",
    "            self.target_inventory.remove(self.current_targets[target_id])\n",
    "            \n",
    "            print(\"pct complete:\", (len(self.acquired_targets) /\n",
    "                                    (len(self.acquired_targets) + len(self.target_inventory))))\n",
    "                    \n",
    "            if len(self.acquired_targets) % 20 == 1:\n",
    "                take_screen_shot(screen)\n",
    "                time.sleep(0.2) # screen capture takes a bit\n",
    "            \n",
    "            # remove old target\n",
    "            self.current_targets.remove(self.current_targets[target_id])\n",
    "            \n",
    "            # get a new target\n",
    "            self.assign_target(False, drone_id)\n",
    "            \n",
    "            start_dists = []\n",
    "            if cur_mode == PACK:\n",
    "                # find furthest start position...\n",
    "                for i in range(len(self.start_positions)):\n",
    "                    dx = self.start_positions[i][0] - self.current_targets[0][0]\n",
    "                    dy = self.start_positions[i][1] - self.current_targets[0][1]\n",
    "                    start_dist = int(((dx**2 + dy**2)**0.5)) \n",
    "                    \n",
    "                    # ...that doesn't have an obstacle w/in 125\n",
    "                    obstacle_dists = []; cat_dists = []\n",
    "                    if len(self.obstacles) > 0:\n",
    "                        for j in range(len(self.obstacles)):\n",
    "                            dx = self.start_positions[i][0] - self.obstacles[j].position[0]\n",
    "                            dy = self.start_positions[i][1] - self.obstacles[j].position[1]\n",
    "                            obstacle_dists.append(int(((dx**2 + dy**2)**0.5)))\n",
    "                    \n",
    "                    if len(self.cats) > 0:\n",
    "                        for k in range(len(self.cats)):\n",
    "                            dx = self.start_positions[i][0] - self.cats[k].position[0]\n",
    "                            dy = self.start_positions[i][1] - self.cats[k].position[1]\n",
    "                            cat_dists.append(int(((dx**2 + dy**2)**0.5)))\n",
    "                    \n",
    "                    if (len(self.obstacles) > 0 and min(obstacle_dists) < 125) or (len (self.cats) > 0 and min(cat_dists) < 125):\n",
    "                        print(\"-- alternative start location\")\n",
    "                        start_dists.append(0)\n",
    "                    else:\n",
    "                        start_dists.append(start_dist)\n",
    "                \n",
    "                # move drones to start position\n",
    "                for i in range(NUM_DRONES):\n",
    "                    self.drones[i].position = \\\n",
    "                        self.start_positions[start_dists.index(max(start_dists))][0], \\\n",
    "                        self.start_positions[start_dists.index(max(start_dists))][1]\n",
    "                    \n",
    "                    self.drones[i].angle = self.start_angles[start_dists.index(max(start_dists))]\n",
    "                    \n",
    "            # introduce obstacles gradually for HUNT/PACK learning\n",
    "            if cur_mode == PACK and drone_id == (NUM_DRONES - 1):\n",
    "                \n",
    "                if self.total_frame_ctr > 1 and self.obs_adds == 0:\n",
    "                    for i in range(2):\n",
    "                        self.obstacles.append(self.create_obstacle(random.randint(100, width-100),\n",
    "                                                                   random.randint(100, height-100),\n",
    "                                                                   OBSTACLE_SIZES[0]))\n",
    "                    self.target_radius -= 2\n",
    "                    self.obs_adds += 1\n",
    "                    \n",
    "                elif self.total_frame_ctr > 25 and self.obs_adds == 1: # 2500\n",
    "                    for i in range(2):\n",
    "                        self.obstacles.append(self.create_obstacle(random.randint(100, width-100),\n",
    "                                                                   random.randint(100, height-100),\n",
    "                                                                   OBSTACLE_SIZES[1]))\n",
    "                    self.target_radius -= 2\n",
    "                    self.obs_adds += 1\n",
    "                \n",
    "                elif self.total_frame_ctr > 75 and self.obs_adds == 2: # 7500\n",
    "                    for i in range(2):\n",
    "                        self.obstacles.append(self.create_obstacle(random.randint(100, width-100),\n",
    "                                                                   random.randint(100, height-100),\n",
    "                                                                   OBSTACLE_SIZES[2]))\n",
    "                    self.target_radius -= 2\n",
    "                    self.obs_adds += 1\n",
    "                    \n",
    "                elif self.total_frame_ctr > 225 and self.obs_adds == 3: # 22500\n",
    "                    for i in range(2):\n",
    "                        self.cats.append(self.create_cat(random.randint(50, width-50),\n",
    "                                                         random.randint(50, height-50)))\n",
    "                    self.target_radius -= 2\n",
    "                    self.obs_adds += 1\n",
    "                    \n",
    "                elif self.total_frame_ctr > 500 and self.obs_adds == 4: # 500\n",
    "                    for i in range(2):\n",
    "                        self.cats.append(self.create_cat(random.randint(50, width-50),\n",
    "                                                         random.randint(50, height-50)))\n",
    "                    self.target_radius -= 2\n",
    "                    self.obs_adds += 1\n",
    "\n",
    "        return target_dist, target_rads, drone_angle_rads, adj_heading_to_target, \\\n",
    "            avoid_move_efficiency, acquire_move_efficiency\n",
    "            \n",
    "    def build_states(self, drone_id, turn_action, heading_adjust, cur_speed):\n",
    "        turn_state = 0\n",
    "        avoid_state = 0\n",
    "        acquire_state = 0\n",
    "        hunt_state = 0\n",
    "        drone_state = 0\n",
    "        min_sonar_dist = 0\n",
    "        avoid_move_efficiency = 0\n",
    "        acquire_move_efficiency = 0\n",
    "        \n",
    "        # get readings from the various sensors\n",
    "        sonar_dist_readings, sonar_color_readings = \\\n",
    "            self.get_sonar_dist_color_readings(drone_id)\n",
    "        \n",
    "        turn_readings = sonar_dist_readings[:TURN_NUM_SENSOR]\n",
    "        min_sonar_dist = min(turn_readings)\n",
    "        turn_readings = turn_readings + sonar_color_readings[:TURN_NUM_SENSOR]\n",
    "        turn_state = np.array([turn_readings])\n",
    "        \n",
    "        if cur_mode != TURN:\n",
    "            avoid_readings = sonar_dist_readings[:AVOID_NUM_SENSOR]\n",
    "            min_sonar_dist = min(avoid_readings)\n",
    "            avoid_readings = avoid_readings + sonar_color_readings[:AVOID_NUM_SENSOR]\n",
    "            avoid_readings.append(turn_action)\n",
    "            avoid_readings.append(cur_speed)\n",
    "            avoid_state = np.array([avoid_readings])\n",
    "        \n",
    "        if cur_mode in [ACQUIRE, HUNT, PACK]:\n",
    "            # calc distances, headings and efficiency\n",
    "            min_sonar_dist = min(sonar_dist_readings[:HUNT_NUM_SENSOR])\n",
    "            # note: avoid, hunt, pack all using 7 sensors for min dist.\n",
    "            # however, pack will only be seeing four sensors. FIX THIS AT SOME POINT.\n",
    "            # problem is, you can't call evaluate_move twice as it appends readings for mean sd ea time. \n",
    "            # So, some moves will be evaluated based on sensor distances it doesn't see.\n",
    "            target_dist, target_angle_rads, drone_angle_rads, adj_heading_to_target, \\\n",
    "                avoid_move_efficiency, acquire_move_efficiency = \\\n",
    "                    self.evaluate_move(drone_id, heading_adjust, min_sonar_dist)\n",
    "        \n",
    "            acquire_state = np.array([[target_dist, adj_heading_to_target]])\n",
    "\n",
    "            if cur_mode in [HUNT, PACK]:\n",
    "                hunt_readings = sonar_dist_readings[:HUNT_NUM_SENSOR]\n",
    "                hunt_readings = hunt_readings + sonar_color_readings[:HUNT_NUM_SENSOR]\n",
    "                hunt_readings.append(target_dist)\n",
    "                hunt_readings.append(adj_heading_to_target)\n",
    "                hunt_state = np.array([hunt_readings])\n",
    "                min_sonar_dist = min(sonar_dist_readings[:HUNT_NUM_SENSOR])\n",
    "\n",
    "            if cur_mode == PACK: #and (self.total_frame_ctr == 1 or self.replay_frame_ctr % PACK_EVAL_FRAMES == 0)\n",
    "                \n",
    "                '''approach 1: didn't work. lever to input and reward linkage too noisey''' \n",
    "                # pack requires four compas point (above, below, right and left) obs dist readings\n",
    "                #compass_rads = [0, (3.14/2), 3.14, (-3.14/2)]\n",
    "                # it gets readings by adjusting the sonar readings for the drone angle...\n",
    "                #sonar_angles = [0, 0.6, -0.6, 1.2, -1.2, 2.8, -2.8]\n",
    "                #sonar_angles_adj = np.add(sonar_angles, drone_angle_rads)\n",
    "                # ...then finds the sonar reading closest to its required compass direction\n",
    "                #for rad in range(len(compass_rads)):\n",
    "                #    drone_readings.append(sonar_dist_readings[find_nearest(sonar_angles_adj,\n",
    "                #                                                          compass_rads[rad])])\n",
    "                # drone_readings.append(target_dist)\n",
    "                # drone_readings.append(target_angle_rads)\n",
    "                \n",
    "                '''approach 2: using dist/headings to obj. unlikely to work'''\n",
    "                #obj_dist_rads = []\n",
    "                #for obstacle in self.obstacles:\n",
    "                #    obs_dist, obs_rads = calc_dist_rads(obstacle.position,\n",
    "                #                                        self.drones[drone_id].position)\n",
    "                #    obj_dist_rads.append(obs_dist)\n",
    "                #    obj_dist_rads.append(obs_rads)\n",
    "                    \n",
    "                #for cat in self.cats:\n",
    "                #    cat_dist, cat_rads = calc_dist_rads(cat.position,\n",
    "                #                                        self.drones[drone_id].position)\n",
    "                #    obj_dist_rads.append(cat_dist)\n",
    "                #    obj_dist_rads.append(cat_rads)\n",
    "                \n",
    "                #ctr = 0\n",
    "                #for drone in self.drones:\n",
    "                #    if drone_id != ctr:\n",
    "                #        drone_dist, drone_rads = calc_dist_rads(drone.position,\n",
    "                #                                                self.drones[drone_id].position)\n",
    "                #        obj_dist_rads.append(drone_dist)\n",
    "                #        obj_dist_rads.append(drone_rads)\n",
    "                    \n",
    "                #    ctr += 1\n",
    "                \n",
    "                '''approach 3: object coordinates'''\n",
    "                drone_state = np.zeros([1,DRONE_NUM_SENSOR])\n",
    "                \n",
    "                if drone_id == (NUM_DRONES - 1):\n",
    "                    ctr = 0\n",
    "                    for obstacle in self.obstacles:\n",
    "                        x, y = obstacle.position\n",
    "                        drone_state[0, ctr] = int(x)\n",
    "                        drone_state[0, (ctr+1)] = int(y)\n",
    "                        ctr += 2\n",
    "                    \n",
    "                    ctr = 0\n",
    "                    for cat in self.cats:\n",
    "                        x, y = cat.position\n",
    "                        drone_state[0,((2 * NUM_OBSTACLES) + ctr)] = int(x)\n",
    "                        drone_state[0,((2 * NUM_OBSTACLES) + 1 + ctr)] = int(y)\n",
    "                        ctr += 2\n",
    "                    \n",
    "                    ctr = 0\n",
    "                    for drone in self.drones:\n",
    "                        x, y = drone.position\n",
    "                        drone_state[0,((2 * (NUM_OBSTACLES + NUM_CATS)) + ctr)] = int(x)\n",
    "                        drone_state[0,((2 * (NUM_OBSTACLES + NUM_CATS)) + 1 + ctr)] = int(y)\n",
    "                        ctr += 2\n",
    "                        \n",
    "                    ctr = 0\n",
    "                    for target in self.current_targets:\n",
    "                        drone_state[0,((2 * (NUM_OBSTACLES + NUM_CATS + NUM_DRONES)) + ctr)] = target[0]\n",
    "                        drone_state[0,((2 * (NUM_OBSTACLES + NUM_CATS + NUM_DRONES)) + 1 + ctr)] = target[1]\n",
    "                        ctr += 2\n",
    "\n",
    "        return turn_state, avoid_state, acquire_state, hunt_state, drone_state, \\\n",
    "            min_sonar_dist, avoid_move_efficiency, acquire_move_efficiency\n",
    "\n",
    "    def calc_rwd(self, drone_id, min_sonar_dist, driving_direction, cur_speed, \n",
    "                 avoid_move_efficiency, acquire_move_efficiency):\n",
    "\n",
    "        reward = 0\n",
    "        x, y = self.drones[drone_id].position\n",
    "\n",
    "        # check for crash\n",
    "        if min_sonar_dist <= 1: #  and cur_mode != PACK\n",
    "            reward = -500\n",
    "            if x < 0 or x > width or y < 0 or y > height:\n",
    "                self.drones[drone_id].position = int(width/2), int(height/2)\n",
    "                self.num_off_scrn += 1\n",
    "                print(\"off screen. total off screens\", self.num_off_scrn)\n",
    "                reward = -1000\n",
    "            self.recover_from_crash(driving_direction, drone_id)\n",
    "        \n",
    "        else:\n",
    "            if cur_mode == TURN:\n",
    "                # Rewards better spacing from objects\n",
    "                reward = min_sonar_dist\n",
    "            \n",
    "            elif cur_mode == AVOID:\n",
    "                # rewards distance from objects and speed\n",
    "                sd_speeds = np.std(SPEEDS)\n",
    "                sd_dist = np.std(range(20))\n",
    "            \n",
    "                std_speed = cur_speed / sd_speeds\n",
    "                std_dist = min_sonar_dist / sd_dist\n",
    "            \n",
    "                std_max_speed = max(SPEEDS) / sd_speeds\n",
    "                std_max_dist = SONAR_ARM_LEN / sd_dist\n",
    "            \n",
    "                reward = ((std_speed * std_dist) +\n",
    "                          ((std_max_speed - std_speed) * (std_max_dist - std_dist)))\n",
    "            \n",
    "            else: # i.e., cur_mode is acquisition-related (acquire, hunt, pack)\n",
    "                # rewards moving in the right direction and acquiring pixels\n",
    "                if self.target_acquired == True:\n",
    "                    reward = 1000\n",
    "                    self.target_acquired = False\n",
    "                    self.acquire_frame_ctr = 0\n",
    "\n",
    "                else:\n",
    "                    if cur_mode == ACQUIRE:\n",
    "                        reward = 100 * acquire_move_efficiency\n",
    "                    \n",
    "                    elif cur_mode == HUNT:\n",
    "                        reward = 40 * acquire_move_efficiency + 50 * avoid_move_efficiency\n",
    "\n",
    "        if cur_mode == PACK:\n",
    "            # rewards moving all drones in right direction and acquiring pixels\n",
    "            \n",
    "            if reward == 1000:\n",
    "                self.pack_cum_rwds[drone_id, 0] = self.pack_cum_rwds[drone_id, 0] + 1000\n",
    "                \n",
    "            elif reward == -500 or reward == -1000:\n",
    "                self.pack_cum_rwds[drone_id, 0] = self.pack_cum_rwds[drone_id, 0] - 500\n",
    "        \n",
    "            else:\n",
    "                new_rwd = (((60 / (PACK_EVAL_FRAMES * NUM_DRONES)) * acquire_move_efficiency) + \n",
    "                           ((40 / (PACK_EVAL_FRAMES * NUM_DRONES)) * avoid_move_efficiency))\n",
    "                \n",
    "                # two drones. reward each 1/2 of total in acquire/avoid eff proportion\n",
    "                self.pack_cum_rwds[drone_id, 0] += new_rwd\n",
    "            \n",
    "            reward = 0\n",
    "            \n",
    "            #print(\"++ in game / calc rwd. replay frame ctr:\", self.replay_frame_ctr)\n",
    "            #print(\"drone_id:\", drone_id, \"drone reward:\", self.pack_cum_rwds[drone_id,0])\n",
    "            \n",
    "            if self.replay_frame_ctr % PACK_EVAL_FRAMES == 0: # self.total_frame_ctr == 1 or \n",
    "                reward = int(self.pack_cum_rwds[drone_id, 0])\n",
    "                self.pack_cum_rwds[drone_id, 0] = 0\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    def recover_from_crash(self, driving_direction, drone_id):\n",
    "        # back up\n",
    "        crash_adjust = -100\n",
    "        self.drones[drone_id].velocity = crash_adjust * driving_direction\n",
    "        \n",
    "        for i in range(10):\n",
    "            self.drones[drone_id].angle += .2  # Turn a little.\n",
    "            screen.fill(THECOLORS[\"red\"])  # Red is scary!\n",
    "            draw(screen, self.space)\n",
    "            self.space.step(1./10)\n",
    "            if draw_screen:\n",
    "                pygame.display.flip()\n",
    "            clock.tick()\n",
    "\n",
    "    def get_sonar_dist_color_readings(self, drone_id):\n",
    "        sonar_dist_readings = []\n",
    "        sonar_color_readings = []\n",
    "        \"\"\"\n",
    "        sonar readings return N \"distance\" readings, one for each sonar. distance is\n",
    "        a count of the first non-zero color detection reading starting at the object.\n",
    "        \"\"\"\n",
    "        \n",
    "        # make sonar \"arms\"\n",
    "        arm_1 = self.make_sonar_arm(drone_id)\n",
    "        arm_2 = arm_1\n",
    "        arm_3 = arm_1\n",
    "        arm_4 = arm_1\n",
    "        arm_5 = arm_1\n",
    "        arm_6 = arm_1\n",
    "        arm_7 = arm_1\n",
    "        \n",
    "        # rotate arms to get vector of readings\n",
    "        d, c = self.get_arm_dist_color(arm_1, 0, drone_id)\n",
    "        sonar_dist_readings.append(d); sonar_color_readings.append(c)\n",
    "        d, c = self.get_arm_dist_color(arm_2, 0.6, drone_id)\n",
    "        sonar_dist_readings.append(d); sonar_color_readings.append(c)\n",
    "        d, c = self.get_arm_dist_color(arm_3, -0.6, drone_id)\n",
    "        sonar_dist_readings.append(d); sonar_color_readings.append(c)\n",
    "        d, c = self.get_arm_dist_color(arm_4, 1.2, drone_id)\n",
    "        sonar_dist_readings.append(d); sonar_color_readings.append(c)\n",
    "        d, c = self.get_arm_dist_color(arm_5, -1.2, drone_id)\n",
    "        sonar_dist_readings.append(d); sonar_color_readings.append(c)\n",
    "        d, c = self.get_arm_dist_color(arm_6, 2.8, drone_id)\n",
    "        sonar_dist_readings.append(d); sonar_color_readings.append(c)\n",
    "        d, c = self.get_arm_dist_color(arm_7, -2.8, drone_id)\n",
    "        sonar_dist_readings.append(d); sonar_color_readings.append(c)\n",
    "        \n",
    "        if show_sensors:\n",
    "            pygame.display.update()\n",
    "\n",
    "        return sonar_dist_readings, sonar_color_readings\n",
    "\n",
    "    def get_arm_dist_color(self, arm, offset, drone_id):\n",
    "        # count arm length to nearest obstruction\n",
    "        i = 0\n",
    "        x, y = self.drones[drone_id].position\n",
    "        \n",
    "        # evaluate each arm point to see if we've hit something\n",
    "        for point in arm:\n",
    "            i += 1\n",
    "            \n",
    "            # move the point to the right spot\n",
    "            rotated_p = self.get_rotated_point(x, y, point[0], point[1],\n",
    "                                               self.drones[drone_id].angle + offset)\n",
    "            \n",
    "            # return i if rotated point is off screen\n",
    "            if rotated_p[0] <= 0 or rotated_p[1] <= 0 or rotated_p[0] >= width or rotated_p[1] >= height:\n",
    "                return i, 1 # 1 is wall color\n",
    "            else:\n",
    "                obs = screen.get_at(rotated_p)\n",
    "                \n",
    "                # this gets the color of the pixel at the rotated point\n",
    "                obs_color = self.get_track_or_not(obs)\n",
    "                \n",
    "                if obs_color != 0:\n",
    "                    # if pixel not a safe color, return distance\n",
    "                    return i, obs_color\n",
    "\n",
    "            # plots the individual sonar point on the screen\n",
    "            if show_sensors:\n",
    "                pygame.draw.circle(screen, (255, 255, 255), (rotated_p), 2)\n",
    "\n",
    "        return i, 0 # 0 is safe color\n",
    "\n",
    "    def make_sonar_arm(self, drone_id):\n",
    "        x, y = self.drones[drone_id].position\n",
    "        \n",
    "        spread = 10  # gap between points on sonar arm\n",
    "        distance = 10  # number of points on sonar arm\n",
    "        arm_points = []\n",
    "        # builds arm flat. it will be rotated about the center later\n",
    "        for i in range(1, SONAR_ARM_LEN): # was 40\n",
    "            arm_points.append((x + distance + (spread * i), y))\n",
    "\n",
    "        return arm_points\n",
    "    \n",
    "    def get_rotated_point(self, x_1, y_1, x_2, y_2, radians):\n",
    "        # Rotate x_2, y_2 around x_1, y_1 by angle.\n",
    "        x_change = (x_2 - x_1) * math.cos(radians) + \\\n",
    "            (y_2 - y_1) * math.sin(radians)\n",
    "        y_change = (y_1 - y_2) * math.cos(radians) - \\\n",
    "            (x_1 - x_2) * math.sin(radians)\n",
    "        new_x = x_change + x_1\n",
    "        new_y = height - (y_change + y_1)\n",
    "        \n",
    "        return int(new_x), int(new_y)\n",
    "\n",
    "    def get_track_or_not(self, reading):\n",
    "        # check to see if color encountered is safe (i.e., should not be crash)\n",
    "        if reading == pygame.color.THECOLORS[BACK_COLOR] or \\\n",
    "            reading == pygame.color.THECOLORS[DRONE_COLOR] or \\\n",
    "            reading == pygame.color.THECOLORS[SMILE_COLOR] or \\\n",
    "            reading == pygame.color.THECOLORS[ACQUIRE_PIXEL_COLOR] or \\\n",
    "            reading == pygame.color.THECOLORS[ACQUIRED_PIXEL_COLOR] or \\\n",
    "            reading == pygame.color.THECOLORS[PATH_COLOR]:\n",
    "            return 0\n",
    "        else:\n",
    "            if reading == pygame.color.THECOLORS[WALL_COLOR]:\n",
    "                return 1\n",
    "            elif reading == pygame.color.THECOLORS[CAT_COLOR]:\n",
    "                return 2\n",
    "            elif reading == pygame.color.THECOLORS[OBSTACLE_COLOR]:\n",
    "                return 3\n",
    "            else:\n",
    "                return 1\n",
    "\n",
    "    # ***** target and acquire model functions *****\n",
    "    def generate_targets(self, first_iter):\n",
    "        \n",
    "        # calc number of targets that can fit space\n",
    "        num_pxl_x_dir = int((width - 2 * ACQUIRE_MARGIN)/ACQUIRE_PIXEL_SEPARATION)\n",
    "        num_pxl_y_dir = int((height- 2 * ACQUIRE_MARGIN)/ACQUIRE_PIXEL_SEPARATION)\n",
    "        \n",
    "        n = num_pxl_x_dir * num_pxl_y_dir\n",
    "        \n",
    "        ctr = 0\n",
    "        for v in range(num_pxl_y_dir):\n",
    "            for h in range(num_pxl_x_dir):\n",
    "                \n",
    "                # space targets across target grid\n",
    "                x_pxl = (ACQUIRE_MARGIN + (h * ACQUIRE_PIXEL_SEPARATION))\n",
    "                y_pxl = (ACQUIRE_MARGIN + (v * ACQUIRE_PIXEL_SEPARATION))\n",
    "                \n",
    "                if(first_iter == True):\n",
    "                    self.target_inventory.append((x_pxl,y_pxl))\n",
    "                \n",
    "                ctr += 1\n",
    "\n",
    "        return num_pxl_x_dir, num_pxl_y_dir\n",
    "\n",
    "    def assign_target(self, first_iter, drone_id):\n",
    "        \n",
    "        # clear the path surface\n",
    "        path_grid.fill(pygame.color.THECOLORS[BACK_COLOR])\n",
    "        \n",
    "        # mark target as acquired\n",
    "        x, y = self.drones[drone_id].position\n",
    "        \n",
    "        if first_iter == False:\n",
    "            \n",
    "            pygame.draw.rect(target_grid, pygame.color.THECOLORS[ACQUIRED_PIXEL_COLOR],\n",
    "                             ((x, height - y), (ACQUIRE_PIXEL_SIZE, ACQUIRE_PIXEL_SIZE)), 0)\n",
    "        \n",
    "        # randomly select a new target\n",
    "        new_target = random.choice(self.target_inventory)\n",
    "        self.current_targets.append(new_target)\n",
    "\n",
    "        # draw the new target\n",
    "        pygame.draw.rect(target_grid, pygame.color.THECOLORS[ACQUIRE_PIXEL_COLOR],\n",
    "                         ((new_target[0], height - new_target[1]),\n",
    "                          (ACQUIRE_PIXEL_SIZE, ACQUIRE_PIXEL_SIZE)), 0)\n",
    "\n",
    "# ***** global functions *****\n",
    "def find_nearest(array, value):\n",
    "    idx = (np.abs(array-value)).argmin()\n",
    "    return idx\n",
    "\n",
    "def calc_dist_rads(xy1, xy2, rad_adjust):\n",
    "    dx = xy1[0] - xy2[0]\n",
    "    dy = xy1[1] - xy2[1]\n",
    "    dist = int(((dx**2 + dy**2)**0.5))\n",
    "    \n",
    "    rads = atan2(dy,dx) + rad_adjust\n",
    "    rads %= 2*pi\n",
    "        \n",
    "    rads = np.round(rads,2)\n",
    "    if rads > 3.14:\n",
    "        rads = rads - 6.28\n",
    "            \n",
    "    return dist, rads\n",
    "\n",
    "def take_screen_shot(screen):\n",
    "    time_taken = time.asctime(time.localtime(time.time()))\n",
    "    time_taken = time_taken.replace(\" \", \"_\")\n",
    "    time_taken = time_taken.replace(\":\",\".\")\n",
    "    save_file = \"screenshots/\" + time_taken + \".jpeg\"\n",
    "    pygame.image.save(screen,save_file)\n",
    "    print(\"screen shot taken\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Section below trains the neural networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import os.path\n",
    "import timeit\n",
    "import time\n",
    "\n",
    "# initialize\n",
    "GAMMA = 0.9  # Forgetting.\n",
    "\n",
    "def train_net(turn_model, turn_model_30, turn_model_50, turn_model_70, avoid_model, acquire_model,\n",
    "              acquire_model_30, acquire_model_50, acquire_model_70, hunt_model, pack_model, params):\n",
    "    \n",
    "    filename = params_to_filename(params)\n",
    "    \n",
    "    if cur_mode in [ACQUIRE, HUNT]:\n",
    "        observe = 1000  # Number of frames to observe before training.\n",
    "    elif cur_mode == PACK:\n",
    "        observe = 2 * PACK_STATE_FRAMES\n",
    "    else:\n",
    "        observe = 2000\n",
    "\n",
    "    epsilon = 1 # vary this based on pre-learning already occurred in lower models\n",
    "    train_frames = 1000000  # number of flips for training\n",
    "    batchSize = params['batchSize']\n",
    "    buffer = params['buffer']\n",
    "\n",
    "    # initialize variables and structures used below.\n",
    "    max_crash_frame_ctr = 0\n",
    "    crash_frame_ctr = 0\n",
    "    total_frame_ctr = 0\n",
    "    replay_frame_ctr = 0\n",
    "    stop_ctr = 0\n",
    "    avoid_ctr = 0\n",
    "    acquire_ctr = 0\n",
    "    cum_rwd = 0\n",
    "    cum_speed = 0\n",
    "    new_pack_action = pack_action = START_PACK_ACTION\n",
    "    new_pack_rwd = pack_rwd = 0\n",
    "\n",
    "    data_collect = []\n",
    "    replay = []\n",
    "    pack_states = []\n",
    "    loss_log = [] # replay stores state, action, reward, new state\n",
    "    save_init = True\n",
    "    cur_speeds = []\n",
    "    for i in range(NUM_DRONES): cur_speeds.append(START_SPEED)\n",
    "    \n",
    "    # initialize drone state holders\n",
    "    turn_states = np.zeros([NUM_DRONES, TURN_TOTAL_SENSORS * TURN_STATE_FRAMES])\n",
    "    avoid_states = np.zeros([NUM_DRONES, AVOID_TOTAL_SENSORS * AVOID_STATE_FRAMES])\n",
    "    acquire_states = np.zeros([NUM_DRONES, ACQUIRE_TOTAL_SENSORS * ACQUIRE_STATE_FRAMES])\n",
    "    hunt_states = np.zeros([NUM_DRONES, HUNT_TOTAL_SENSORS * HUNT_STATE_FRAMES])\n",
    "    pack_states = np.zeros([PACK_STATE_FRAMES, PACK_NUM_INPUT])\n",
    "    \n",
    "    # create game instance\n",
    "    game_state = GameState()\n",
    "    \n",
    "    # get initial state(s)\n",
    "    turn_state, avoid_state, acquire_state, hunt_state, drone_state, reward, cur_speed = \\\n",
    "        game_state.frame_step(START_DRONE_ID, START_TURN_ACTION, START_SPEED_ACTION,\n",
    "                              START_PACK_ACTION, START_SPEED, START_DISTANCE, 1)\n",
    "\n",
    "    # initialize frame states\n",
    "    if cur_mode in [TURN, AVOID, HUNT, PACK]:\n",
    "        \n",
    "        for i in range(NUM_DRONES): \n",
    "            turn_states[i] = state_frames(turn_state, \n",
    "                                          np.zeros((1, TURN_TOTAL_SENSORS * TURN_STATE_FRAMES)),\n",
    "                                          TURN_TOTAL_SENSORS, TURN_STATE_FRAMES)\n",
    "        \n",
    "        if cur_mode in [AVOID, HUNT, PACK]:\n",
    "            \n",
    "            for i in range(NUM_DRONES): \n",
    "                avoid_states[i] = state_frames(avoid_state, \n",
    "                                               np.zeros((1, AVOID_TOTAL_SENSORS * AVOID_STATE_FRAMES)),\n",
    "                                               AVOID_TOTAL_SENSORS, AVOID_STATE_FRAMES)\n",
    "\n",
    "    if cur_mode in [ACQUIRE, HUNT, PACK]:\n",
    "        \n",
    "        for i in range(NUM_DRONES): \n",
    "            acquire_states[i] = state_frames(acquire_state, \n",
    "                                             np.zeros((1, ACQUIRE_TOTAL_SENSORS * ACQUIRE_STATE_FRAMES)), \n",
    "                                             ACQUIRE_TOTAL_SENSORS, ACQUIRE_STATE_FRAMES)\n",
    "\n",
    "    if cur_mode in [HUNT, PACK]:\n",
    "        \n",
    "        for i in range(NUM_DRONES): \n",
    "            hunt_states[i] = state_frames(hunt_state, \n",
    "                                          np.zeros((1, HUNT_TOTAL_SENSORS * HUNT_STATE_FRAMES)), \n",
    "                                          HUNT_TOTAL_SENSORS, HUNT_STATE_FRAMES)\n",
    "\n",
    "    if cur_mode == PACK:\n",
    "        \n",
    "        for i in range(PACK_STATE_FRAMES):\n",
    "            pack_states[replay_frame_ctr] = drone_state\n",
    "\n",
    "    # time it\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    # run frames\n",
    "    while total_frame_ctr < train_frames:\n",
    "        \n",
    "        total_frame_ctr += 1 # counts total training distance traveled\n",
    "        crash_frame_ctr += 1 # counts distance between crashes\n",
    "        replay_frame_ctr += 1 # counts frames between pack mode replay captures\n",
    "        \n",
    "        # used to slow things down for de-bugging\n",
    "        #time.sleep(1)\n",
    "        \n",
    "        for drone_id in range(NUM_DRONES): # NUM_DRONES = 1, unless you're in PACK mode\n",
    "            \n",
    "            speed_action = START_SPEED_ACTION\n",
    "                \n",
    "            # choose appropriate action(s)\n",
    "            '''note: only generates random inputs for currently training model.\n",
    "            All prior (sub) models provide their best (fully-trained) inputs.'''\n",
    "            \n",
    "            if random.random() < epsilon or total_frame_ctr < observe: # epsilon degrades over flips...\n",
    "                if cur_mode == TURN:\n",
    "                    turn_action, active_turn_model = set_turn_action(True, cur_speeds[drone_id],\n",
    "                                                                     np.array([turn_states[drone_id]]),\n",
    "                                                                     turn_model, turn_model_30,\n",
    "                                                                     turn_model_50, turn_model_70)\n",
    "                else:\n",
    "                    if cur_mode in [AVOID, HUNT, PACK]:\n",
    "                        turn_action, active_turn_model = set_turn_action(False, \n",
    "                                                                         cur_speeds[drone_id],\n",
    "                                                                         np.array([turn_states[drone_id]]),\n",
    "                                                                         turn_model, turn_model_30,\n",
    "                                                                         turn_model_50, turn_model_70)\n",
    "                    if cur_mode == AVOID:\n",
    "                        speed_action = set_avoid_action(True, turn_action,\n",
    "                                                        np.array([avoid_states[drone_id]]))\n",
    "                    else:\n",
    "                        if cur_mode in [HUNT, PACK]:\n",
    "                            speed_action = set_avoid_action(False, turn_action,\n",
    "                                                            np.array([avoid_states[drone_id]]))\n",
    "                        \n",
    "                        if cur_mode == ACQUIRE:\n",
    "                            acquire_action, active_acquire_model = \\\n",
    "                                set_acquire_action(True, cur_speeds[drone_id],\n",
    "                                                   np.array([acquire_states[drone_id,]]),\n",
    "                                                   acquire_model, acquire_model_30,\n",
    "                                                   acquire_model_50, acquire_model_70)\n",
    "                            turn_action = acquire_action\n",
    "                        else:\n",
    "                            acquire_action, active_acquire_model = \\\n",
    "                                set_acquire_action(False, cur_speeds[drone_id], \n",
    "                                                   np.array([acquire_states[drone_id,]]),\n",
    "                                                   acquire_model, acquire_model_30,\n",
    "                                                   acquire_model_50, acquire_model_70)\n",
    "                            \n",
    "                            if cur_mode == HUNT:\n",
    "                                hunt_action, turn_action, speed_action = \\\n",
    "                                    set_hunt_action(True, cur_speeds[drone_id], \n",
    "                                                    turn_action,speed_action, \n",
    "                                                    acquire_action, \n",
    "                                                    np.array([hunt_states[drone_id,]]))\n",
    "                            else:\n",
    "                                hunt_action, turn_action, speed_action = \\\n",
    "                                    set_hunt_action(False, cur_speeds[drone_id], \n",
    "                                                    turn_action, speed_action, acquire_action, \n",
    "                                                    np.array([hunt_states[drone_id,]]))\n",
    "                                \n",
    "                                if cur_mode == PACK and (replay_frame_ctr - 1) % PACK_STATE_FRAMES == 0 and drone_id == 0: #(total_frame_ctr == 1 or \n",
    "                                    new_pack_action = set_pack_action(True, np.array([pack_states[PACK_EVAL_FRAMES:]]))\n",
    "                                    '''note: pack action only changed every PACK_EVAL_FRAMES. \n",
    "                                    For frames in between it is constant'''\n",
    "\n",
    "            else: # ...increasing use of predictions over time\n",
    "                \n",
    "                if cur_mode == TURN:\n",
    "                    turn_action, active_turn_model = set_turn_action(False, cur_speeds[drone_id],\n",
    "                                                                     np.array([turn_states[drone_id]]),\n",
    "                                                                     turn_model, turn_model_30,\n",
    "                                                                     turn_model_50, turn_model_70)\n",
    "                else:\n",
    "                    if cur_mode in [AVOID, HUNT, PACK]:\n",
    "                        turn_action, active_turn_model = set_turn_action(False, cur_speeds[drone_id],\n",
    "                                                                         np.array([turn_states[drone_id]]),\n",
    "                                                                         turn_model, turn_model_30,\n",
    "                                                                         turn_model_50, turn_model_70)\n",
    "                    if cur_mode == AVOID:\n",
    "                        speed_action = set_avoid_action(False, turn_action,\n",
    "                                                        np.array([avoid_states[drone_id]]))\n",
    "                    else:\n",
    "                        if cur_mode in [HUNT, PACK]:\n",
    "                            speed_action = set_avoid_action(False, turn_action,\n",
    "                                                            np.array([avoid_states[drone_id]]))\n",
    "                        \n",
    "                        if cur_mode == ACQUIRE:\n",
    "                            acquire_action, active_acquire_model = \\\n",
    "                                set_acquire_action(False, cur_speeds[drone_id], \n",
    "                                                   np.array([acquire_states[drone_id,]]),\n",
    "                                                   acquire_model, acquire_model_30,\n",
    "                                                   acquire_model_50, acquire_model_70)\n",
    "                            turn_action = acquire_action\n",
    "                        else:\n",
    "                            acquire_action, active_acquire_model = \\\n",
    "                                set_acquire_action(False, cur_speeds[drone_id], \n",
    "                                                   np.array([acquire_states[drone_id,]]),\n",
    "                                                   acquire_model, acquire_model_30,\n",
    "                                                   acquire_model_50, acquire_model_70)\n",
    "                                                                \n",
    "                            if cur_mode == HUNT:\n",
    "                                hunt_action, turn_action, speed_action = \\\n",
    "                                    set_hunt_action(False, cur_speeds[drone_id], \n",
    "                                                    turn_action, speed_action, acquire_action, \n",
    "                                                    np.array([hunt_states[drone_id,]]))\n",
    "                            else:\n",
    "                                hunt_action, turn_action, speed_action = \\\n",
    "                                    set_hunt_action(False, cur_speeds[drone_id], \n",
    "                                                    turn_action, speed_action, acquire_action, \n",
    "                                                    np.array([hunt_states[drone_id,]]))\n",
    "                                                              \n",
    "                                if cur_mode == PACK and (replay_frame_ctr - 1) % PACK_STATE_FRAMES == 0 and drone_id == 0: #(total_frame_ctr == 1 or \n",
    "                                    # get 1 pack action for each set of drones on first drone\n",
    "                                    new_pack_action = set_pack_action(False, np.array([pack_states[PACK_EVAL_FRAMES:]]))\n",
    "            \n",
    "            new_turn_state, new_avoid_state, new_acquire_state, new_hunt_state, new_drone_state, new_reward, new_speed = \\\n",
    "                game_state.frame_step(drone_id, turn_action, speed_action, new_pack_action,\n",
    "                                      cur_speeds[drone_id], total_frame_ctr, replay_frame_ctr)\n",
    "            \n",
    "            # append (horizontally) historical states for learning speed.\n",
    "            '''note: do this concatination even for models that are not learning \n",
    "            (e.g., turn when running search or turn, search and acquire while running hunt) \n",
    "            b/c their preds, performed above, expect the same multi-frame view that was \n",
    "            in place when they trained.'''\n",
    "\n",
    "            if cur_mode in [TURN, AVOID, HUNT, PACK]:\n",
    "                new_turn_state = state_frames(new_turn_state,\n",
    "                                              np.array([turn_states[drone_id]]),\n",
    "                                              TURN_TOTAL_SENSORS, TURN_STATE_FRAMES)\n",
    "        \n",
    "            if cur_mode in [AVOID, HUNT, PACK]:\n",
    "                new_avoid_state = state_frames(new_avoid_state,\n",
    "                                               np.array([avoid_states[drone_id]]),\n",
    "                                               AVOID_TOTAL_SENSORS, AVOID_STATE_FRAMES)\n",
    "        \n",
    "            if cur_mode in [ACQUIRE, HUNT, PACK]:\n",
    "                new_acquire_state = state_frames(new_acquire_state,\n",
    "                                                 np.array([acquire_states[drone_id]]),\n",
    "                                                 ACQUIRE_TOTAL_SENSORS, ACQUIRE_STATE_FRAMES)\n",
    "\n",
    "            if cur_mode in [HUNT, PACK]:\n",
    "                new_hunt_state = state_frames(new_hunt_state,\n",
    "                                              np.array([hunt_states[drone_id]]),\n",
    "                                              HUNT_TOTAL_SENSORS, HUNT_STATE_FRAMES)\n",
    "\n",
    "            if cur_mode == PACK: #and (total_frame_ctr == 1 or replay_frame_ctr % PACK_EVAL_FRAMES == 0):\n",
    "                \n",
    "                new_pack_rwd += new_reward\n",
    "\n",
    "                if drone_id == (NUM_DRONES - 1): # for last drone add pack record\n",
    "                    \n",
    "                    pack_states = np.append(pack_states, new_drone_state, axis = 0)\n",
    "                    \n",
    "                    if pack_states.shape[0] > (2 * PACK_STATE_FRAMES):\n",
    "                        pack_states = np.delete(pack_states, 0, 0)                        \n",
    "                        \n",
    "            # experience replay storage\n",
    "            \"\"\"note: only the model being trained requires event storage as it is \n",
    "            stack that will be sampled for training below.\"\"\"\n",
    "            if cur_mode == TURN:\n",
    "                replay.append((np.array([turn_states[drone_id]]),\n",
    "                              turn_action, new_reward, new_turn_state))\n",
    "\n",
    "            elif cur_mode == AVOID:\n",
    "                replay.append((np.array([avoid_states[drone_id]]),\n",
    "                               speed_action, new_reward, new_avoid_state))\n",
    "\n",
    "            elif cur_mode == ACQUIRE:\n",
    "                replay.append((np.array([acquire_states[drone_id]]),\n",
    "                               turn_action, new_reward, new_acquire_state))\n",
    "\n",
    "            elif cur_mode == HUNT:\n",
    "                replay.append((np.array([hunt_states[drone_id]]),\n",
    "                               hunt_action, new_reward, new_hunt_state))\n",
    "\n",
    "            elif cur_mode == PACK and total_frame_ctr > PACK_EVAL_FRAMES and replay_frame_ctr % PACK_EVAL_FRAMES == 0 and drone_id == (NUM_DRONES - 1): #(total_frame_ctr == 1 or \n",
    "                replay.append((pack_states[0:PACK_EVAL_FRAMES], pack_action, \n",
    "                              pack_rwd, pack_states[PACK_EVAL_FRAMES:]))\n",
    "                \n",
    "                pack_action = new_pack_action\n",
    "                pack_rwd = new_pack_rwd\n",
    "                new_pack_rwd = 0\n",
    "\n",
    "            # If we're done observing, start training.\n",
    "            if total_frame_ctr > observe and (cur_mode != PACK or (replay_frame_ctr % PACK_EVAL_FRAMES == 0 and drone_id == (NUM_DRONES - 1))):\n",
    "                \n",
    "                # If we've stored enough in our buffer, pop the oldest.\n",
    "                if len(replay) > buffer:\n",
    "                    replay.pop(0)\n",
    "            \n",
    "                if cur_mode == PACK:\n",
    "                    minibatch = replay[-1]\n",
    "                else:\n",
    "                    # randomly sample experience replay memory\n",
    "                    minibatch = random.sample(replay, batchSize)\n",
    "\n",
    "                if cur_mode == TURN:\n",
    "                    # Get training values.\n",
    "                    X_train, y_train = process_minibatch(minibatch, active_turn_model,\n",
    "                                                         TURN_NUM_INPUT, TURN_NUM_OUTPUT, cur_mode)\n",
    "                    history = LossHistory()\n",
    "                    active_turn_model.fit(X_train, y_train, batch_size=batchSize,\n",
    "                                          nb_epoch=1, verbose=0, callbacks=[history])\n",
    "                \n",
    "                elif cur_mode == AVOID:\n",
    "                    X_train, y_train = process_minibatch(minibatch, avoid_model,\n",
    "                                                         AVOID_NUM_INPUT, AVOID_NUM_OUTPUT, cur_mode)\n",
    "                    history = LossHistory()\n",
    "                    avoid_model.fit(X_train, y_train, batch_size=batchSize,\n",
    "                                   nb_epoch=1, verbose=0, callbacks=[history])\n",
    "\n",
    "                elif cur_mode == ACQUIRE:\n",
    "                    X_train, y_train = process_minibatch(minibatch, active_acquire_model,\n",
    "                                                         ACQUIRE_NUM_INPUT, ACQUIRE_NUM_OUTPUT, cur_mode)\n",
    "                    history = LossHistory()\n",
    "                    active_acquire_model.fit(X_train, y_train, batch_size=batchSize,\n",
    "                                    nb_epoch=1, verbose=0, callbacks=[history])\n",
    "\n",
    "                elif cur_mode == HUNT:\n",
    "                    X_train, y_train = process_minibatch(minibatch, hunt_model,\n",
    "                                                         HUNT_NUM_INPUT, HUNT_NUM_OUTPUT, cur_mode)\n",
    "                    history = LossHistory()\n",
    "                    hunt_model.fit(X_train, y_train, batch_size=batchSize,\n",
    "                                      nb_epoch=1, verbose=0, callbacks=[history])\n",
    "\n",
    "                elif cur_mode == PACK:\n",
    "                    X_train, y_train = process_pack_batch(minibatch, pack_model, PACK_STATE_FRAMES, \n",
    "                                                          PACK_NUM_INPUT, PACK_NUM_OUTPUT, cur_mode)\n",
    "                    history = LossHistory()\n",
    "                    \n",
    "                    pack_model.fit(X_train, y_train, batch_size=batchSize,\n",
    "                                   nb_epoch=1, verbose=0, callbacks=[history])\n",
    "\n",
    "                loss_log.append(history.losses)\n",
    "\n",
    "            # Update the starting state with S'.\n",
    "            if cur_mode in [TURN, AVOID, HUNT, PACK]:\n",
    "                turn_states[drone_id] = new_turn_state\n",
    "\n",
    "            if cur_mode in [AVOID, HUNT, PACK]:\n",
    "                avoid_states[drone_id] = new_avoid_state\n",
    "\n",
    "            if cur_mode in [ACQUIRE, HUNT, PACK]:\n",
    "                acquire_states[drone_id] = new_acquire_state\n",
    "\n",
    "            if cur_mode in [HUNT, PACK]:\n",
    "                hunt_states[drone_id] = new_hunt_state\n",
    "\n",
    "            if cur_mode == PACK and replay_frame_ctr % PACK_EVAL_FRAMES == 0: #(total_frame_ctr == 1 or \n",
    "                #drone_states[drone_id] = new_drone_state\n",
    "                replay_frame_ctr = 0\n",
    "\n",
    "            cur_speeds[drone_id] = new_speed\n",
    "            cum_rwd += new_reward\n",
    "\n",
    "            # in case of crash, report and initialize\n",
    "            if new_reward == -500 or new_reward == -1000:\n",
    "                # Log the car's distance at this T.\n",
    "                data_collect.append([total_frame_ctr, crash_frame_ctr])\n",
    "\n",
    "                # new max achieved?\n",
    "                if crash_frame_ctr > max_crash_frame_ctr:\n",
    "                    max_crash_frame_ctr = crash_frame_ctr\n",
    "\n",
    "                # Time it.\n",
    "                tot_time = timeit.default_timer() - start_time\n",
    "                fps = crash_frame_ctr / tot_time\n",
    "\n",
    "                # output results to point of crash\n",
    "                print(\"Max: %d at %d\\t eps: %f\\t dist: %d\\t mode: %d\\t cum rwd: %d\\t fps: %d\" %\n",
    "                      (max_crash_frame_ctr, total_frame_ctr, epsilon, crash_frame_ctr, cur_mode, cum_rwd, int(fps)))\n",
    "\n",
    "                # Reset.\n",
    "                crash_frame_ctr = cum_rwd = cum_speed = 0\n",
    "                start_time = timeit.default_timer()\n",
    "    \n",
    "        #print(9)\n",
    "        # decrement epsilon for another frame\n",
    "        if epsilon > 0.1 and total_frame_ctr > observe:\n",
    "            epsilon -= (1/train_frames)\n",
    "\n",
    "        if total_frame_ctr % 10000 == 0:\n",
    "            if crash_frame_ctr != 0:\n",
    "                print(\"Max: %d at %d\\t eps: %f\\t dist: %d\\t mode: %d\\t cum rwd: %d\" % \n",
    "                      (max_crash_frame_ctr, total_frame_ctr, epsilon, crash_frame_ctr, cur_mode, cum_rwd))\n",
    "    \n",
    "        # Save model every 50k frames\n",
    "        if total_frame_ctr % 50000 == 0:\n",
    "            save_init = False\n",
    "            if cur_mode == TURN:\n",
    "                turn_model.save_weights('models/turn/turn-' + filename + '-' +\n",
    "                                        str(START_SPEED) + '-' + str(total_frame_ctr) + '.h5',overwrite=True)\n",
    "                print(\"Saving turn_model %s - %d - %d\" % (filename, START_SPEED,total_frame_ctr))\n",
    "            \n",
    "            elif cur_mode == AVOID:\n",
    "                avoid_model.save_weights('models/avoid/avoid-' + filename + '-' +\n",
    "                                         str(total_frame_ctr) + '.h5', overwrite=True)\n",
    "                print(\"Saving avoid_model %s - %d\" % (filename,total_frame_ctr))\n",
    "            \n",
    "            elif cur_mode == ACQUIRE:\n",
    "                acquire_model.save_weights('models/acquire/acquire-' + filename + '-' +\n",
    "                                           str(START_SPEED) + '-' + str(total_frame_ctr) + '.h5',overwrite=True)\n",
    "                print(\"Saving acquire_model %s - %d\" % (filename,total_frame_ctr))\n",
    "            \n",
    "            elif cur_mode == HUNT:\n",
    "                hunt_model.save_weights('models/hunt/hunt-' + filename + '-' +\n",
    "                                           str(total_frame_ctr) + '.h5', overwrite=True)\n",
    "                print(\"Saving hunt_model %s - %d\" % (filename,total_frame_ctr))\n",
    "\n",
    "            elif cur_mode == PACK:\n",
    "                pack_model.save_weights('models/pack/pack-' + filename + '-' +\n",
    "                            str(total_frame_ctr) + '.h5', overwrite=True)\n",
    "                print(\"Saving pack_model %s - %d\" % (filename, total_frame_ctr))\n",
    "        \n",
    "    # Log results after we're done all frames.\n",
    "    log_results(filename, data_collect, loss_log)\n",
    "\n",
    "def set_turn_action(random_fl, cur_speed, turn_state, turn_model, \n",
    "                    turn_model_30, turn_model_50, turn_model_70):\n",
    "    if random_fl:\n",
    "        turn_action = np.random.randint(0, TURN_NUM_OUTPUT)\n",
    "        if use_existing_model == False:\n",
    "            active_turn_model = turn_model\n",
    "        else:\n",
    "            active_turn_model = turn_model_50\n",
    "    else:\n",
    "        if cur_mode == TURN and use_existing_model == False:\n",
    "            turn_qval = turn_model.predict(turn_state, batch_size=1)\n",
    "            active_turn_model = turn_model\n",
    "        else:\n",
    "            if cur_speed == SPEEDS[0]:\n",
    "                turn_qval = turn_model_30.predict(turn_state, batch_size=1)\n",
    "                active_turn_model = turn_model_30\n",
    "            elif cur_speed == SPEEDS[1]:\n",
    "                turn_qval = turn_model_50.predict(turn_state, batch_size=1)\n",
    "                active_turn_model = turn_model_50\n",
    "            elif cur_speed == SPEEDS[2]:\n",
    "                turn_qval = turn_model_70.predict(turn_state, batch_size=1)\n",
    "                active_turn_model = turn_model_70\n",
    "        turn_action = (np.argmax(turn_qval))\n",
    "    return turn_action, active_turn_model\n",
    "\n",
    "def set_avoid_action(random_fl, turn_action, avoid_state):\n",
    "    if random_fl:\n",
    "        speed_action = np.random.randint(0, AVOID_NUM_OUTPUT)\n",
    "    else:\n",
    "        avoid_state[0][14] = turn_action # ensures AVOID using current turn pred\n",
    "        avoid_qval = avoid_model.predict(avoid_state, batch_size=1)\n",
    "        speed_action = (np.argmax(avoid_qval))\n",
    "            \n",
    "    return speed_action\n",
    "\n",
    "def set_acquire_action(random_fl, cur_speed, acquire_state, acquire_model, \n",
    "                       acquire_model_30, acquire_model_50, acquire_model_70):\n",
    "    if random_fl:\n",
    "        turn_action = np.random.randint(0, ACQUIRE_NUM_OUTPUT)\n",
    "        if use_existing_model == False:\n",
    "            active_acquire_model = acquire_model \n",
    "        else:\n",
    "            active_acquire_model = acquire_model_50\n",
    "    else:\n",
    "        if cur_mode == ACQUIRE and use_existing_model == False:\n",
    "            acquire_qval = acquire_model.predict(acquire_state, batch_size=1)\n",
    "            active_acquire_model = acquire_model\n",
    "        else:\n",
    "            if cur_speed == SPEEDS[0]:\n",
    "                acquire_qval = acquire_model_30.predict(acquire_state, batch_size=1)\n",
    "                active_acquire_model = acquire_model_50\n",
    "            elif cur_speed == SPEEDS[1]:\n",
    "                acquire_qval = acquire_model_50.predict(acquire_state, batch_size=1)\n",
    "                active_acquire_model = acquire_model_50\n",
    "            else:\n",
    "                acquire_qval = acquire_model_70.predict(acquire_state, batch_size=1)\n",
    "                active_acquire_model = acquire_model_70\n",
    "        turn_action = (np.argmax(acquire_qval))\n",
    "    return turn_action, active_acquire_model\n",
    "\n",
    "def set_hunt_action(random_fl, cur_speed, turn_action, speed_action, acquire_action, hunt_state):\n",
    "    if random_fl:\n",
    "        hunt_action = np.random.randint(0, HUNT_NUM_OUTPUT)\n",
    "        if hunt_action == HUNT_AVOID: # accept speed model action\n",
    "            turn_action = turn_action\n",
    "            if cur_speed > 0:\n",
    "                speed_action = speed_action # continue current speed\n",
    "            else:\n",
    "                speed_action = 1 # reset speed to 50, as you were stopped\n",
    "            #avoid_ctr += 1\n",
    "                \n",
    "        elif hunt_action == HUNT_ACQUIRE: # accept acquire model action\n",
    "            turn_action = acquire_action\n",
    "            if cur_speed > 0:\n",
    "                speed_action = 1 # just setting acquire speed to 50 for now\n",
    "            else:\n",
    "                speed_action = 1 # reset speed to 50, as you were stopped\n",
    "                \n",
    "    else:\n",
    "        hunt_qval = hunt_model.predict(hunt_state, batch_size=1)\n",
    "        hunt_action = (np.argmax(hunt_qval))\n",
    "        \n",
    "        if hunt_action == HUNT_AVOID: # accept avoid model action\n",
    "            turn_action = turn_action\n",
    "            if cur_speed > 0:\n",
    "                speed_action = speed_action # continue current speed\n",
    "            else:\n",
    "                speed_action = 1\n",
    "                 \n",
    "        elif hunt_action == HUNT_ACQUIRE: # accept acquire model action\n",
    "            turn_action = acquire_action\n",
    "            if cur_speed > 0:\n",
    "                speed_action = 1 # just setting acquire speed to 50 for now\n",
    "            else:\n",
    "                speed_action = 1 # reset acquire speed to 50, as you were stopped\n",
    "            \n",
    "    return hunt_action, turn_action, speed_action\n",
    "\n",
    "def set_pack_action(random_fl, pack_state):\n",
    "    if random_fl:\n",
    "        pack_action = np.random.randint(0, PACK_NUM_OUTPUT)\n",
    "    else:\n",
    "        pack_qval = pack_model.predict(pack_state, batch_size=1)\n",
    "        pack_action = (np.argmax(pack_qval))\n",
    "\n",
    "    return pack_action\n",
    "\n",
    "def state_frames(new_state, old_state, num_sensor, num_frame):\n",
    "    \"\"\"\n",
    "    Takes a state returned from the game and turns it into a multi-frame state.\n",
    "    Create a new array with the new state and first N of old state,\n",
    "    which was the previous frame's new state.\n",
    "   \"\"\"\n",
    "    # Turn them back into arrays.\n",
    "    new_state = new_state.tolist()[0]\n",
    "    old_state = old_state.tolist()[0][:num_sensor * (num_frame - 1)]\n",
    "\n",
    "    # Combine them.\n",
    "    combined_state = new_state + old_state\n",
    "    \n",
    "    # Re-numpy them on exit.\n",
    "    return np.array([combined_state])\n",
    "\n",
    "def log_results(filename, data_collect, loss_log):\n",
    "    # Save the results to a file so we can graph it later.\n",
    "    with open('results/sonar-frames/learn_data-' + filename + '.csv', 'w') as data_dump:\n",
    "        wr = csv.writer(data_dump)\n",
    "        wr.writerows(data_collect)\n",
    "\n",
    "    with open('results/sonar-frames/loss_data-' + filename + '.csv', 'w') as lf:\n",
    "        wr = csv.writer(lf)\n",
    "        for loss_item in loss_log:\n",
    "            wr.writerow(loss_item)\n",
    "\n",
    "def process_minibatch(minibatch, model, num_input, num_output, cur_mode):\n",
    "    print(\"***** in process minibatch *****\")\n",
    "    \"\"\"This does the heavy lifting, aka, the training. It's super jacked.\"\"\"\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    # Loop through our batch and create arrays for X and y\n",
    "    # so that we can fit our model at every step.\n",
    "    for memory in minibatch:\n",
    "        # break minibatch of replays into component parts\n",
    "        old_state_m, action_m, reward_m, new_state_m = memory\n",
    "\n",
    "        # make a prediction on the prior state. \n",
    "        # this gives you a baseline set of predicted values for each output class. \n",
    "        # these baseline predictions will be \"corrected\" by swapping in the actual reward recived\n",
    "        # for the action previously selected (i.e., when you asked for the pred n frames before).\n",
    "        if cur_mode == PACK:\n",
    "            print(\"old state m dims b4:\")\n",
    "            print(old_state_m.size)\n",
    "            print(\"old state m dims aftr1:\")\n",
    "            print(np.array(old_state_m))\n",
    "            print(\"old state m dims aftr:\")\n",
    "            old_state_m = np.array(old_state_m)\n",
    "            \n",
    "            old_qval = model.predict(old_state_m, batch_size=1)\n",
    "        else:\n",
    "            old_qval = model.predict(old_state_m, batch_size=1)\n",
    "        print(\"old qval:\")\n",
    "        print(old_qval)\n",
    "        \n",
    "        # make prediction on the current state\n",
    "        newQ = model.predict(new_state_m, batch_size=1)\n",
    "        print(\"new qval:\")\n",
    "        print(newQ)\n",
    "        \n",
    "        # select output option w/ on current state w/ highest predicted value - used in update function\n",
    "        maxQ = np.max(newQ)\n",
    "        print(\"maxQ\")\n",
    "        print(maxQ)\n",
    "        \n",
    "        # creat a pseudo Y record with the q-values from the prior state prediction, but...\n",
    "        y = np.zeros((1, num_output)) # was 3.\n",
    "        y[:] = old_qval[:]\n",
    "        \n",
    "        # ...calculate a weighted reward based on:\n",
    "            # the quality of result i.e., reward\n",
    "            # the learning rate i.e., gamma\n",
    "            # how certain you are now (?) i.e., maxQ - rationale for this is not obvious...\n",
    "        if reward_m != -500:  # non-terminal state\n",
    "            update = (reward_m + (GAMMA * maxQ))\n",
    "            # ...normally you'd see gamma * reward. here we're dealing with actual predicted values (not log probs).\n",
    "            # so, this is up'ing the reward based on an amount that is appropriate to the training stage. \n",
    "            # that is, it will increase as the model improves. this seems to be the sole use of the second prediction.\n",
    "            \n",
    "        else:  # terminal state\n",
    "            update = reward_m\n",
    "        print(\"update:\")\n",
    "        print(update)\n",
    "        \n",
    "        # then associate that reward w/ the chosen action value in the y vect for your old pred.\n",
    "        # right... the reward is associated directly w/ the action value that gave rise to it.\n",
    "        y[0][action_m] = update\n",
    "        \n",
    "        # create list of un-flattened x values (e.g., taking [0,n] and giving [num_input, n/num_input])\n",
    "        X_train.append(old_state_m.reshape(num_input,))\n",
    "        \n",
    "        # create list of un-flattend y values (e.g., taking [0,n] and giving [num_output, n/num_output])\n",
    "        y_train.append(y.reshape(num_output,))\n",
    "        \n",
    "        # do this for every record in the minibatch getting a stack of x's and y's\n",
    "        # then return this for the \"fit\" step, basically submitting the batch to the neural network for training\n",
    "        # in the case of TURN, AVOID, ACQUIRE and HUNT models, this minbatch (and therefore x/y's) are selected at random\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    return X_train, y_train\n",
    "    \n",
    "def process_pack_batch(minibatch, model, num_frames, num_input, num_output, cur_mode):\n",
    "    \n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    # break minibatch into component parts\n",
    "    old_state_m = np.array(minibatch[0])\n",
    "    action_m = minibatch[1]\n",
    "    reward_m = minibatch[2]\n",
    "    new_state_m = np.array(minibatch[3])\n",
    "        \n",
    "    # make prediction on the old state\n",
    "    old_qval = model.predict(np.array([old_state_m]), batch_size=1)\n",
    "    # using cross-entropy loss + rms optimizer results in qval probs, not expected values \n",
    "\n",
    "    # make prediction on the current state\n",
    "    newQ = model.predict(np.array([new_state_m]), batch_size=1)\n",
    "        \n",
    "    # select output option w/ on current state w/ highest predicted value - used in update function\n",
    "    maxQ = np.max(newQ)\n",
    "    #maxQ = 1\n",
    "        \n",
    "    # creat a pseudo Y record with the q-values from the prior state prediction, but...\n",
    "    y = np.zeros((1, num_output)) # was 3.\n",
    "    y[:] = old_qval[:]\n",
    "        \n",
    "    y = y * 10 # kludge: translating probs to expected values by multiplying by estimated average reward\n",
    "        \n",
    "    # ...calculate a weighted reward based on:\n",
    "    if reward_m != -500:  # non-terminal state\n",
    "        update = (reward_m + (GAMMA * maxQ))\n",
    "        # ...normally you'd see gamma * reward. here we're dealing with actual predicted values (not log probs).\n",
    "        # so, this is up'ing the reward based on an amount that is appropriate to the training stage. \n",
    "         # that is, it will increase as the model improves. this seems to be the sole use of the second prediction.\n",
    "            \n",
    "    else:  # terminal state\n",
    "        update = reward_m\n",
    "        \n",
    "    # then associate that reward w/ the chosen action value in the y vect for your old pred.\n",
    "    # right... the reward is associated directly w/ the action value that gave rise to it.\n",
    "    y[0][action_m] = update\n",
    "        \n",
    "    # create list of un-flattened x values (e.g., taking [0,n] and giving [num_input, n/num_input])\n",
    "    X_train.append(old_state_m.reshape(num_frames, num_input,))\n",
    "        \n",
    "    # create list of un-flattend y values (e.g., taking [0,n] and giving [num_output, n/num_output])\n",
    "    y_train.append(y.reshape(num_output,))\n",
    "        \n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    return X_train, y_train\n",
    "\n",
    "def params_to_filename(params):\n",
    "    if len(params['nn']) == 1:\n",
    "    \n",
    "        return str(params['nn'][0]) + '-' + str(params['batchSize']) + \\\n",
    "        '-' + str(params['buffer'])\n",
    "    \n",
    "    elif len(params['nn']) == 2:\n",
    "        \n",
    "        return str(params['nn'][0]) + '-' + str(params['nn'][1]) + '-' + \\\n",
    "        str(params['batchSize']) + '-' + str(params['buffer'])\n",
    "\n",
    "    elif len(params['nn']) == 3:\n",
    "\n",
    "        return str(params['nn'][0]) + '-' + str(params['nn'][1]) + '-' + \\\n",
    "            str(params['nn'][2]) + '-' + str(params['batchSize']) + '-' + str(params['buffer'])\n",
    "\n",
    "def launch_learn(params):\n",
    "    filename = params_to_filename(params)\n",
    "    print(\"Trying %s\" % filename)\n",
    "    # Make sure we haven't run this one.\n",
    "    if not os.path.isfile('results/sonar-frames/loss_data-' + filename + '.csv'):\n",
    "        # Create file so we don't double test when we run multiple\n",
    "        # instances of the script at the same time.\n",
    "        open('results/sonar-frames/loss_data-' + filename + '.csv', 'a').close()\n",
    "        print(\"Starting test.\")\n",
    "        # Train.\n",
    "        if cur_mode == TURN:\n",
    "            turn_model = turn_net(NUM_INPUT, params['nn'])\n",
    "            train_net(turn_model, 0, params)\n",
    "        elif cur_mode == AVOID:\n",
    "            avoid_model = avoid_net(NUM_INPUT, params['nn'])\n",
    "            train_net(0, avoid_model, params)\n",
    "    else:\n",
    "        print(\"Already tested.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section below loads trained neural nets as required to support training of higher level networks. For networks being trained, it initializes network by calling appropriate neural network schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "turn_model, turn_model_30, turn_model_50, turn_model_70, avoid_model, acquire_model, acquire_model_30, acquire_model_50, acquire_model_70, hunt_model, pack_model, params = load_models()\n",
    "\n",
    "train_net(turn_model, turn_model_30, turn_model_50, turn_model_70, avoid_model, acquire_model, acquire_model_30, acquire_model_50, acquire_model_70, hunt_model, pack_model, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Play \n",
    "\n",
    "Section below is used to run games "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def play(turn_model, turn_model_30, turn_model_50, turn_model_70, avoid_model, acquire_model,\n",
    "         acquire_model_30, acquire_model_50, acquire_model_70, hunt_model, pack_model, params):\n",
    "\n",
    "    total_frame_ctr = 0\n",
    "    crash_frame_ctr = 0\n",
    "    replay_frame_ctr = 0\n",
    "    crash_ctr = 0\n",
    "    acquire_ctr = 0\n",
    "    cum_speed = 0\n",
    "    stop_ctr = avoid_ctr = acquire_ctr = 0\n",
    "    new_pack_action = pack_action = START_PACK_ACTION\n",
    "    cur_speeds = []\n",
    "    for i in range(NUM_DRONES): cur_speeds.append(START_SPEED)\n",
    "    \n",
    "    # initialize drone state holders\n",
    "    turn_states = np.zeros([NUM_DRONES, TURN_TOTAL_SENSORS * TURN_STATE_FRAMES])\n",
    "    avoid_states = np.zeros([NUM_DRONES, AVOID_TOTAL_SENSORS * AVOID_STATE_FRAMES])\n",
    "    acquire_states = np.zeros([NUM_DRONES, ACQUIRE_TOTAL_SENSORS * ACQUIRE_STATE_FRAMES])\n",
    "    hunt_states = np.zeros([NUM_DRONES, HUNT_TOTAL_SENSORS * HUNT_STATE_FRAMES])\n",
    "    drone_states = np.zeros([NUM_DRONES, DRONE_TOTAL_SENSORS * PACK_STATE_FRAMES])\n",
    "    pack_states = np.zeros([PACK_STATE_FRAMES, PACK_NUM_INPUT])\n",
    "\n",
    "    # create game instance\n",
    "    game_state = GameState()\n",
    "    \n",
    "    # get initial state(s)\n",
    "    turn_state, avoid_state, acquire_state, hunt_state, drone_state, reward, cur_speed = \\\n",
    "        game_state.frame_step(START_DRONE_ID, START_TURN_ACTION, START_SPEED_ACTION,\n",
    "                              START_PACK_ACTION, START_SPEED, START_DISTANCE, 1)\n",
    "\n",
    "    # initialize frame states\n",
    "    if cur_mode in [TURN, AVOID, HUNT, PACK]:\n",
    "        \n",
    "        for i in range(NUM_DRONES): \n",
    "            turn_states[i] = state_frames(turn_state, np.zeros((1, TURN_TOTAL_SENSORS * TURN_STATE_FRAMES)), \n",
    "                                          TURN_TOTAL_SENSORS, TURN_STATE_FRAMES)\n",
    "        \n",
    "        if cur_mode in [AVOID, HUNT, PACK]:\n",
    "            \n",
    "            for i in range(NUM_DRONES): \n",
    "                avoid_states[i] = state_frames(avoid_state, np.zeros((1, AVOID_TOTAL_SENSORS * AVOID_STATE_FRAMES)),\n",
    "                                               AVOID_TOTAL_SENSORS, AVOID_STATE_FRAMES)\n",
    "\n",
    "    if cur_mode in [ACQUIRE, HUNT, PACK]:\n",
    "    \n",
    "        for i in range(NUM_DRONES): \n",
    "            acquire_states[i] = state_frames(acquire_state, np.zeros((1, ACQUIRE_TOTAL_SENSORS * ACQUIRE_STATE_FRAMES)),\n",
    "                                             ACQUIRE_TOTAL_SENSORS, ACQUIRE_STATE_FRAMES)\n",
    "    \n",
    "    if cur_mode in [HUNT, PACK]:\n",
    "        \n",
    "        for i in range(NUM_DRONES): \n",
    "            hunt_states[i] = state_frames(hunt_state, np.zeros((1, HUNT_TOTAL_SENSORS * HUNT_STATE_FRAMES)), \n",
    "                                          HUNT_TOTAL_SENSORS, HUNT_STATE_FRAMES)\n",
    "    \n",
    "    if cur_mode == PACK:\n",
    "        \n",
    "        for i in range(PACK_STATE_FRAMES):\n",
    "            pack_states[replay_frame_ctr] = drone_state\n",
    "    \n",
    "    # Move.\n",
    "    while True:\n",
    "        \n",
    "        total_frame_ctr += 1\n",
    "        crash_frame_ctr += 1\n",
    "        replay_frame_ctr += 1\n",
    "        if total_frame_ctr == 200000:\n",
    "            end\n",
    "        #time.sleep(1)\n",
    "        \n",
    "        for drone_id in range(NUM_DRONES): # NUM_DRONES = 1, unless you're in PACK mode\n",
    "        \n",
    "            speed_action = START_SPEED_ACTION\n",
    "            if cur_mode != PACK:\n",
    "                pack_action = 0\n",
    "            \n",
    "            # choose action\n",
    "            if cur_mode == TURN:\n",
    "                turn_action, active_turn_model = set_turn_action(False, cur_speeds[drone_id],\n",
    "                                                                 np.array([turn_states[drone_id]]),\n",
    "                                                                 turn_model, turn_model_30,\n",
    "                                                                 turn_model_50, turn_model_70)\n",
    "            else:\n",
    "                if cur_mode in [AVOID, HUNT, PACK]:\n",
    "                    turn_action, active_turn_model = set_turn_action(False, cur_speeds[drone_id],\n",
    "                                                                     np.array([turn_states[drone_id]]),\n",
    "                                                                     turn_model, turn_model_30,\n",
    "                                                                     turn_model_50, turn_model_70)\n",
    "                                                                      \n",
    "                if cur_mode == AVOID:\n",
    "                    speed_action = set_avoid_action(False, turn_action, \n",
    "                                                    np.array([avoid_states[drone_id]]))\n",
    "                else:\n",
    "                    if cur_mode in [HUNT, PACK]:\n",
    "                        speed_action = set_avoid_action(False, turn_action, \n",
    "                                                        np.array([avoid_states[drone_id]]))\n",
    "                                                                                          \n",
    "                    if cur_mode == ACQUIRE:\n",
    "                        acquire_action, active_acquire_model = \\\n",
    "                            set_acquire_action(False, cur_speeds[drone_id],\n",
    "                                               np.array([acquire_states[drone_id,]]),\n",
    "                                               acquire_model, acquire_model_30,\n",
    "                                               acquire_model_50, acquire_model_70)\n",
    "                        turn_action = acquire_action\n",
    "                    else:\n",
    "                        acquire_action, active_acquire_model = \\\n",
    "                            set_acquire_action(False, cur_speeds[drone_id],\n",
    "                                               np.array([acquire_states[drone_id,]]),\n",
    "                                               acquire_model, acquire_model_30,\n",
    "                                               acquire_model_50, acquire_model_70)\n",
    "                                                                                                              \n",
    "                        if cur_mode == HUNT:\n",
    "                            hunt_action, turn_action, speed_action = \\\n",
    "                                set_hunt_action(False, cur_speeds[drone_id], turn_action, speed_action, \n",
    "                                                acquire_action, np.array([hunt_states[drone_id,]]))\n",
    "                        else:\n",
    "                            hunt_action, turn_action, speed_action = \\\n",
    "                                set_hunt_action(False, cur_speeds[drone_id], \n",
    "                                                turn_action, speed_action, acquire_action, \n",
    "                                                np.array([hunt_states[drone_id,]]))\n",
    "                            \n",
    "                            if cur_mode == PACK and replay_frame_ctr > PACK_STATE_FRAMES and (replay_frame_ctr - 1) % PACK_STATE_FRAMES == 0 and drone_id == 0: #(total_frame_ctr == 1 or \n",
    "                                # get 1 pack action for each set of drones on first drone\n",
    "                                new_pack_action = set_pack_action(False, np.array([pack_states[PACK_EVAL_FRAMES:]]))\n",
    "\n",
    "            # pass action, receive new state, reward\n",
    "            new_turn_state, new_avoid_state, new_acquire_state, new_hunt_state, new_drone_state, new_reward, new_speed = \\\n",
    "                game_state.frame_step(drone_id, turn_action, speed_action, pack_action, \n",
    "                                      cur_speeds[drone_id], total_frame_ctr, replay_frame_ctr)\n",
    "\n",
    "            # append (horizontally) historical states for learning speed.\n",
    "            if cur_mode in [TURN, AVOID, HUNT, PACK]:\n",
    "                new_turn_state = state_frames(new_turn_state,\n",
    "                                              np.array([turn_states[drone_id]]),\n",
    "                                              TURN_TOTAL_SENSORS,TURN_STATE_FRAMES)\n",
    "            \n",
    "            if cur_mode in [AVOID, HUNT, PACK]:\n",
    "                new_avoid_state = state_frames(new_avoid_state,\n",
    "                                               np.array([avoid_states[drone_id]]),\n",
    "                                               AVOID_TOTAL_SENSORS, AVOID_STATE_FRAMES)\n",
    "            \n",
    "            if cur_mode in [ACQUIRE, HUNT, PACK]:\n",
    "                new_acquire_state = state_frames(new_acquire_state,\n",
    "                                                 np.array([acquire_states[drone_id]]),\n",
    "                                                 ACQUIRE_NUM_SENSOR, ACQUIRE_STATE_FRAMES)\n",
    "            \n",
    "            if cur_mode in [HUNT, PACK]:\n",
    "                new_hunt_state = state_frames(new_hunt_state,\n",
    "                                              np.array([hunt_states[drone_id]]),\n",
    "                                              HUNT_TOTAL_SENSORS, HUNT_STATE_FRAMES)\n",
    "                    \n",
    "            if cur_mode == PACK: #and (total_frame_ctr == 1 or replay_frame_ctr % PACK_EVAL_FRAMES == 0):\n",
    "\n",
    "                if drone_id == (NUM_DRONES - 1): # for last drone add pack record\n",
    "                    \n",
    "                    pack_states = np.append(pack_states, new_drone_state, axis = 0)\n",
    "                    \n",
    "                    if pack_states.shape[0] > (2 * PACK_STATE_FRAMES):\n",
    "                    \n",
    "                        pack_states = np.delete(pack_states, 0, 0) \n",
    "                        \n",
    "            # Update the starting state with S'.\n",
    "            if cur_mode in [TURN, AVOID, HUNT, PACK]:\n",
    "                turn_states[drone_id] = new_turn_state\n",
    "            \n",
    "            if cur_mode in [AVOID, HUNT, PACK]:\n",
    "                avoid_states[drone_id] = new_avoid_state\n",
    "            \n",
    "            if cur_mode in [ACQUIRE, HUNT, PACK]:\n",
    "                acquire_states[drone_id] = new_acquire_state\n",
    "            \n",
    "            if cur_mode in [HUNT, PACK]:\n",
    "                hunt_states[drone_id] = new_hunt_state\n",
    "            \n",
    "            if cur_mode == PACK and replay_frame_ctr % PACK_EVAL_FRAMES == 0:\n",
    "                pack_action = new_pack_action\n",
    "                #pack_rwd = new_pack_rwd\n",
    "                #new_pack_rwd = 0\n",
    "                replay_frame_ctr = 0\n",
    "        \n",
    "            cur_speeds[drone_id] = new_speed\n",
    "        \n",
    "        # give status\n",
    "        if new_reward <= -250:\n",
    "            crash_ctr += 1\n",
    "            print(\"crashes\", crash_ctr, \"frames\", total_frame_ctr)\n",
    "        elif new_reward > 500:\n",
    "            acquire_ctr += 1\n",
    "            print(\"acquisitions:\", acquire_ctr, \"frames\", total_frame_ctr)\n",
    "        \n",
    "        if total_frame_ctr % 5000 == 0:\n",
    "            print(\"+++++ total frames:\", total_frame_ctr)\n",
    "            print(\"+++++ frames between crashes:\", int(total_frame_ctr / crash_ctr))\n",
    "                  \n",
    "            if cur_mode in [ACQUIRE, HUNT, PACK]:\n",
    "                  \n",
    "                print(\"+++++ frames / acquisition:\", int(total_frame_ctr / acquire_ctr))\n",
    "            #stop_ctr = avoid_ctr = acquire_ctr = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************** target acquired ************\n",
      "pct complete: 5.3475935828877e-05\n",
      "screen shot taken\n",
      "************** target acquired ************\n",
      "pct complete: 0.000106951871657754\n",
      "acquisitions: 1 frames 165\n",
      "************** target acquired ************\n",
      "pct complete: 0.00016042780748663101\n",
      "************** target acquired ************\n",
      "pct complete: 0.000213903743315508\n",
      "acquisitions: 2 frames 315\n",
      "************** target acquired ************\n",
      "pct complete: 0.00026737967914438503\n",
      "************** target acquired ************\n",
      "pct complete: 0.00032085561497326203\n",
      "acquisitions: 3 frames 490\n",
      "************** target acquired ************\n",
      "pct complete: 0.000374331550802139\n",
      "************** target acquired ************\n",
      "pct complete: 0.000427807486631016\n",
      "-- alternative start location\n",
      "acquisitions: 4 frames 665\n",
      "************** target acquired ************\n",
      "pct complete: 0.00048128342245989307\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "acquisitions: 5 frames 835\n",
      "************** target acquired ************\n",
      "pct complete: 0.0005347593582887701\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "crashes 1 frames 980\n",
      "************** target acquired ************\n",
      "pct complete: 0.000588235294117647\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "acquisitions: 6 frames 1045\n",
      "************** target acquired ************\n",
      "pct complete: 0.0006417112299465241\n",
      "-- alternative start location\n",
      "crashes 2 frames 1150\n",
      "crashes 3 frames 1155\n",
      "crashes 4 frames 1160\n",
      "crashes 5 frames 1180\n",
      "crashes 6 frames 1200\n",
      "************** target acquired ************\n",
      "pct complete: 0.0006951871657754011\n",
      "-- alternative start location\n",
      "************** target acquired ************\n",
      "pct complete: 0.000748663101604278\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "acquisitions: 7 frames 1340\n",
      "crashes 7 frames 1435\n",
      "************** target acquired ************\n",
      "pct complete: 0.0008021390374331551\n",
      "-- alternative start location\n",
      "************** target acquired ************\n",
      "pct complete: 0.000855614973262032\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "acquisitions: 8 frames 1605\n",
      "crashes 8 frames 1615\n",
      "************** target acquired ************\n",
      "pct complete: 0.0009090909090909091\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "************** target acquired ************\n",
      "pct complete: 0.0009625668449197861\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "crashes 9 frames 1785\n",
      "************** target acquired ************\n",
      "pct complete: 0.0010160427807486632\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "************** target acquired ************\n",
      "pct complete: 0.0010695187165775401\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "************** target acquired ************\n",
      "pct complete: 0.001122994652406417\n",
      "screen shot taken\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "acquisitions: 9 frames 2005\n",
      "crashes 10 frames 2075\n",
      "crashes 11 frames 2115\n",
      "************** target acquired ************\n",
      "pct complete: 0.001176470588235294\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "crashes 12 frames 2160\n",
      "************** target acquired ************\n",
      "pct complete: 0.0012299465240641712\n",
      "-- alternative start location\n",
      "************** target acquired ************\n",
      "pct complete: 0.0012834224598930481\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "acquisitions: 10 frames 2285\n",
      "crashes 13 frames 2325\n",
      "************** target acquired ************\n",
      "pct complete: 0.001336898395721925\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "************** target acquired ************\n",
      "pct complete: 0.0013903743315508022\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "acquisitions: 11 frames 2410\n",
      "************** target acquired ************\n",
      "pct complete: 0.0014438502673796792\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "acquisitions: 12 frames 2460\n",
      "crashes 14 frames 2480\n",
      "crashes 15 frames 2485\n",
      "crashes 16 frames 2490\n",
      "************** target acquired ************\n",
      "pct complete: 0.001497326203208556\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "************** target acquired ************\n",
      "pct complete: 0.0015508021390374333\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "acquisitions: 13 frames 2645\n",
      "************** target acquired ************\n",
      "pct complete: 0.0016042780748663102\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "acquisitions: 14 frames 2865\n",
      "************** target acquired ************\n",
      "pct complete: 0.0016577540106951871\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "acquisitions: 15 frames 2920\n",
      "************** target acquired ************\n",
      "pct complete: 0.001711229946524064\n",
      "acquisitions: 16 frames 2975\n",
      "************** target acquired ************\n",
      "pct complete: 0.0017647058823529412\n",
      "-- alternative start location\n",
      "acquisitions: 17 frames 3055\n",
      "crashes 17 frames 3115\n",
      "crashes 18 frames 3255\n",
      "************** target acquired ************\n",
      "pct complete: 0.0018181818181818182\n",
      "-- alternative start location\n",
      "crashes 19 frames 3290\n",
      "crashes 20 frames 3300\n",
      "crashes 21 frames 3305\n",
      "crashes 22 frames 3320\n",
      "************** target acquired ************\n",
      "pct complete: 0.0018716577540106951\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "************** target acquired ************\n",
      "pct complete: 0.0019251336898395723\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "crashes 23 frames 3530\n",
      "************** target acquired ************\n",
      "pct complete: 0.001978609625668449\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "acquisitions: 18 frames 3545\n",
      "************** target acquired ************\n",
      "pct complete: 0.0020320855614973264\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "-- alternative start location\n",
      "acquisitions: 19 frames 3635\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-755c4873dd43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m play(turn_model, turn_model_30, turn_model_50, turn_model_70, avoid_model,\n\u001b[1;32m      4\u001b[0m      \u001b[0macquire_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macquire_model_30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macquire_model_50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macquire_model_70\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m      hunt_model, pack_model, params)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-2ad2a489a716>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(turn_model, turn_model_30, turn_model_50, turn_model_70, avoid_model, acquire_model, acquire_model_30, acquire_model_50, acquire_model_70, hunt_model, pack_model, params)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# pass action, receive new state, reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             new_turn_state, new_avoid_state, new_acquire_state, new_hunt_state, new_drone_state, new_reward, new_speed =                 game_state.frame_step(drone_id, turn_action, speed_action, pack_action, \n\u001b[0;32m--> 123\u001b[0;31m                                       cur_speeds[drone_id], total_frame_ctr, replay_frame_ctr)\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;31m# append (horizontally) historical states for learning speed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3f5c14ac653f>\u001b[0m in \u001b[0;36mframe_step\u001b[0;34m(self, drone_id, turn_action, speed_action, pack_action, cur_speed, total_ctr, replay_ctr)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# one pixel for every 10 SPEED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdraw_screen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# get readings, build states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "turn_model, turn_model_30, turn_model_50, turn_model_70, avoid_model, acquire_model, acquire_model_30, acquire_model_50, acquire_model_70, hunt_model, pack_model, params = load_models()\n",
    "\n",
    "play(turn_model, turn_model_30, turn_model_50, turn_model_70, avoid_model,\n",
    "     acquire_model, acquire_model_30, acquire_model_50, acquire_model_70,\n",
    "     hunt_model, pack_model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
