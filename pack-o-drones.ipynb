{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "common variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# operating modes drive which nets are loaded, trained\n",
    "TURN = 1\n",
    "AVOID = 2\n",
    "ACQUIRE = 3\n",
    "HUNT = 4\n",
    "PACK = 5\n",
    "cur_mode = PACK\n",
    "\n",
    "# turn model settings\n",
    "TURN_NUM_SENSOR = 5 # front five sonar readings\n",
    "TURN_TOTAL_SENSORS = 10 # distance and color values from sonar readings \n",
    "TURN_NUM_OUTPUT = 5 # do nothing, two right turn, two left turn\n",
    "TURN_STATE_FRAMES = 3\n",
    "TURN_NUM_INPUT = TURN_STATE_FRAMES * TURN_TOTAL_SENSORS\n",
    "\n",
    "# avoid model settings\n",
    "AVOID_NUM_SENSOR = 7 # front five, rear two sonar distance readings\n",
    "AVOID_TOTAL_SENSORS = 16 # distance and color values from sonar readings, turn action, current speed\n",
    "AVOID_NUM_OUTPUT = 3 # 30, 50, 70\n",
    "AVOID_STATE_FRAMES = 3\n",
    "AVOID_NUM_INPUT = AVOID_STATE_FRAMES * AVOID_TOTAL_SENSORS\n",
    "SPEEDS = [30,50,70]\n",
    "\n",
    "# acquire model settings\n",
    "ACQUIRE_NUM_SENSOR = 2 # distance, angle\n",
    "ACQUIRE_NUM_OUTPUT = 5 # nothing, 2 right turn, 2 left turn\n",
    "ACQUIRE_STATE_FRAMES = 2\n",
    "ACQUIRE_NUM_INPUT = ACQUIRE_STATE_FRAMES * ACQUIRE_NUM_SENSOR\n",
    "\n",
    "# hunt model settings\n",
    "HUNT_NUM_SENSOR = 7 # all sonar distance / color readings\n",
    "HUNT_AVOID = 0\n",
    "HUNT_ACQUIRE = 1\n",
    "HUNT_TOTAL_SENSORS = 16 # seven sonar distance + color, target distance + heading\n",
    "HUNT_NUM_OUTPUT = 2 # avoid (model), acquire (model)\n",
    "HUNT_STATE_FRAMES = 3\n",
    "HUNT_NUM_INPUT = HUNT_STATE_FRAMES * HUNT_TOTAL_SENSORS\n",
    "\n",
    "# pack model settings\n",
    "NUM_DRONES = 2\n",
    "DRONE_NUM_SENSOR = 7 # all sonar distance readings, subsequently reduced to four compass readings\n",
    "DRONE_TOTAL_SENSOR = 6 # compass distance readings, true target heading, target distance\n",
    "PACK_TOTAL_SENSORS = DRONE_NUM_SENSOR * NUM_DRONES\n",
    "# these are radian adjustments to first (lhs) and second (rhs) to HTT. pos is left, neg in right.\n",
    "PACK_HEADING_ADJUST = [[0,0],[1,0],[-1,0],[0,1],[0,-1],[1,1],[-1,-1],[1,-1],[-1,1]]\n",
    "NUM_TARGETS = 1\n",
    "PACK_NUM_OUTPUT = 9\n",
    "PACK_STATE_FRAMES = 3\n",
    "PACK_EVAL_FRAMES = 5\n",
    "PACK_NUM_INPUT = PACK_STATE_FRAMES * PACK_TOTAL_SENSORS\n",
    "START_PACK_ACTION = 0\n",
    "START_DRONE_ID = 0\n",
    "\n",
    "# initial settings\n",
    "use_existing_model = True\n",
    "START_SPEED = 50\n",
    "START_TURN_ACTION = 0\n",
    "START_SPEED_ACTION = 1\n",
    "START_DISTANCE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section below defines Keras / Theano neural network schemas. Given function decomp, networks have max three hidden layers. LTSM not implemented here. Network \"memory\" managed in learning module via appended state frames. Base design comes from: http://outlace.com/Reinforcement-Learning-Part-3/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "\n",
    "def turn_net(num_inputs, params, num_outputs, load=''):\n",
    "    model = Sequential()\n",
    "\n",
    "    # First layer.\n",
    "    model.add(Dense(params[0], init='lecun_uniform', input_shape=(num_inputs,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Second layer.\n",
    "    model.add(Dense(params[1], init='lecun_uniform'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Output layer.\n",
    "    model.add(Dense(num_outputs, init='lecun_uniform'))\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    rms = RMSprop()\n",
    "    model.compile(loss='mse', optimizer=rms)\n",
    "\n",
    "    if load:\n",
    "        model.load_weights(load)\n",
    "\n",
    "    return model\n",
    "\n",
    "def avoid_net(num_inputs, params, num_outputs, load=''):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First layer.\n",
    "    model.add(Dense(params[0], init='lecun_uniform', input_shape=(num_inputs,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Second layer.\n",
    "    model.add(Dense(params[1], init='lecun_uniform'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Third layer.\n",
    "    model.add(Dense(params[2], init='lecun_uniform'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Output layer.\n",
    "    model.add(Dense(num_outputs, init='lecun_uniform'))\n",
    "    model.add(Activation('linear'))\n",
    "    \n",
    "    rms = RMSprop()\n",
    "    model.compile(loss='mse', optimizer=rms)\n",
    "    \n",
    "    if load:\n",
    "        model.load_weights(load)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def acquire_net(num_inputs, params, num_outputs, load=''):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First layer.\n",
    "    model.add(Dense(params[0], init='lecun_uniform', input_shape=(num_inputs,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Output layer.\n",
    "    model.add(Dense(num_outputs, init='lecun_uniform'))\n",
    "    model.add(Activation('linear'))\n",
    "    \n",
    "    rms = RMSprop()\n",
    "    model.compile(loss='mse', optimizer=rms)\n",
    "    \n",
    "    if load:\n",
    "        model.load_weights(load)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def hunt_net(num_inputs, params, num_outputs, load=''):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First layer.\n",
    "    model.add(Dense(params[0], init='lecun_uniform', input_shape=(num_inputs,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Second layer.\n",
    "    model.add(Dense(params[1], init='lecun_uniform'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Third layer.\n",
    "    model.add(Dense(params[2], init='lecun_uniform'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Output layer.\n",
    "    model.add(Dense(num_outputs, init='lecun_uniform'))\n",
    "    model.add(Activation('linear'))\n",
    "    \n",
    "    rms = RMSprop()\n",
    "    model.compile(loss='mse', optimizer=rms)\n",
    "    \n",
    "    if load:\n",
    "        model.load_weights(load)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def pack_net(num_inputs, params, num_outputs, load=''):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First layer.\n",
    "    model.add(Dense(params[0], init='lecun_uniform', input_shape=(num_inputs,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    # Output layer.\n",
    "    model.add(Dense(num_outputs, init='lecun_uniform'))\n",
    "    model.add(Activation('linear'))\n",
    "    \n",
    "    rms = RMSprop()\n",
    "    model.compile(loss='mse', optimizer=rms)\n",
    "    \n",
    "    if load:\n",
    "        model.load_weights(load)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section below: \n",
    "1. Establishes the Pygame / Pymunk game state including the board, walls, objects and their characteristics i.e., gravity, elasticity, etc. Pygame \"movements\" are made by updating the \"screen\" (the bottom graphic level) and image \"surfaces\" (layers) above that level. Bind the \"surfaces\" to the screen by \"blit\"-ing. Then \"flip\" the combinded package to print it to the output screen. So, you'll see screen here. You'll also see a variety of surfaces (sometimes called \"grids\") to accomplish those layers.\n",
    "\n",
    "2. Defines changes for each frame (or step). For each frame (or flip or step) of the game, the drone and obstacles are moved based on trajectories and speeds either determined by the game parameters (in the case of \"obstacles\" and \"cats\") or by the neural networks (in the case of \"drones\". Drone moves are in response to game states output from the function below to the neural networks. Game state variables returned depend on the network being trained. For example, the \"turn\" network uses \"sonar\" sensor readings emitted from the drone as it moves. Those sensor readings capture the distance to objects and the color of the objects detected. The network learns risk and specific evasive actions accordingly.\n",
    "\n",
    "Note: while python assumes (0,0) is in the lower left of the screen, pygame assumes (0,0) in the upper left. Therefore, y+ moves DOWN the y axis. Here is example code that illustrates how to handle angles in that environment: https://github.com/mgold/Python-snippets/blob/master/pygame_angles.py. In this implementation, I have flipped the screen so that Y+ moves UP the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pygame\n",
    "from pygame.color import THECOLORS\n",
    "import pymunk\n",
    "from pymunk.vec2d import Vec2d\n",
    "from pymunk.pygame_util import draw\n",
    "import time\n",
    "from math import atan2, degrees, pi, sqrt\n",
    "\n",
    "# ***** initialize variables *****\n",
    "# PyGame init\n",
    "width = 1000\n",
    "height = 700\n",
    "pygame.init()\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "# display surface\n",
    "screen = pygame.display.set_mode((width, height))\n",
    "BACK_COLOR = \"black\"\n",
    "WALL_COLOR = \"red\"\n",
    "DRONE_COLOR = \"green\"\n",
    "SMILE_COLOR = \"blue\"\n",
    "OBSTACLE_COLOR = \"purple\"\n",
    "CAT_COLOR = \"orange\"\n",
    "DRONE_BODY_DIAM = 12\n",
    "SONAR_ARM_LEN = 20\n",
    "OBSTACLE_SIZES = [30, 30, 50, 50, 63, 63]\n",
    "show_sensors = True # Showing sensors and redrawing slows things down.\n",
    "draw_screen = True\n",
    "\n",
    "# acquire model settngs\n",
    "target_grid = pygame.Surface((width, height), pygame.SRCALPHA, 32)\n",
    "target_grid.convert_alpha()\n",
    "path_grid = pygame.Surface((width, height))\n",
    "PATH_COLOR = \"grey\"\n",
    "ACQUIRE_PIXEL_COLOR = \"green\"\n",
    "ACQUIRED_PIXEL_COLOR = \"yellow\"\n",
    "ACQUIRE_PIXEL_SIZE = 2\n",
    "TARGET_RADIUS = 10\n",
    "\n",
    "if cur_mode == ACQUIRE:\n",
    "    ACQUIRE_PIXEL_SEPARATION = 25\n",
    "    ACQUIRE_MARGIN = 50\n",
    "else:\n",
    "    ACQUIRE_PIXEL_SEPARATION = 5\n",
    "    ACQUIRE_MARGIN = 75\n",
    "\n",
    "# hunt model settings\n",
    "STATS_BUFFER = 2000\n",
    "\n",
    "# Turn off alpha since we don't use it.\n",
    "screen.set_alpha(None)\n",
    "\n",
    "# ***** instantiate the game *****\n",
    "class GameState:\n",
    "    def __init__(self):\n",
    "        # initialize space\n",
    "        self.space = pymunk.Space()\n",
    "        self.space.gravity = pymunk.Vec2d(0., 0.)\n",
    "        \n",
    "        # initialize counters\n",
    "        self.total_frame_ctr = 0\n",
    "        self.replay_frame_ctr = 0\n",
    "        self.acquire_frame_ctr = 0\n",
    "        self.num_off_scrn = 0\n",
    "        \n",
    "        # create drones\n",
    "        self.drones = []\n",
    "        \n",
    "        for drone_id in range(NUM_DRONES):\n",
    "            self.drones.append(self.create_drone(random.randint(400,600),\n",
    "                                                 random.randint(300,400), 0.5))\n",
    "        \n",
    "        self.last_x = np.empty([NUM_DRONES,1])\n",
    "        self.last_y = np.empty([NUM_DRONES,1])\n",
    "        for drone_id in range(len(self.drones)):\n",
    "            x, y = self.drones[drone_id].position\n",
    "            self.last_x[drone_id] = x + 2; self.last_y[drone_id] = y + 2\n",
    "        \n",
    "        # create walls\n",
    "        static = [pymunk.Segment(self.space.static_body,(0, 1), (0, height), 1),\n",
    "                  pymunk.Segment(self.space.static_body,(1, height), (width, height), 1),\n",
    "                  pymunk.Segment(self.space.static_body,(width-1, height), (width-1, 1), 1),\n",
    "                  pymunk.Segment(self.space.static_body,(1, 1), (width, 1), 1)]\n",
    "        \n",
    "        for s in static:\n",
    "            s.friction = 1.\n",
    "            s.group = 1\n",
    "            s.collision_type = 1\n",
    "            s.color = THECOLORS[WALL_COLOR]\n",
    "        self.space.add(static)\n",
    "        \n",
    "        if cur_mode in [TURN, AVOID, HUNT, PACK]:\n",
    "            \n",
    "            self.obstacles = []\n",
    "            self.cats = []\n",
    "            \n",
    "            if cur_mode in [TURN, AVOID]: # used to gradually introduce obstacles\n",
    "                # create slow, randomly moving, larger obstacles\n",
    "                self.obstacles.append(self.create_obstacle(random.randint(100, width-100),\n",
    "                                                           random.randint(70, height-100),50))\n",
    "                self.obstacles.append(self.create_obstacle(random.randint(100, width-100),\n",
    "                                                           random.randint(70, height-70),50))\n",
    "                self.obstacles.append(self.create_obstacle(random.randint(100, width-100),\n",
    "                                                           random.randint(70, height-70),63))\n",
    "                self.obstacles.append(self.create_obstacle(random.randint(100, width-100),\n",
    "                                                           random.randint(70, height-70),63))\n",
    "                self.obstacles.append(self.create_obstacle(random.randint(100, width-100),\n",
    "                                                           random.randint(70, height-70),30))\n",
    "                self.obstacles.append(self.create_obstacle(random.randint(100, width-100),\n",
    "                                                           random.randint(70, height-70),30))\n",
    "        \n",
    "                # create faster, randomly moving, smaller obstacles a.k.a. \"cats\"\n",
    "                self.cats.append(self.create_cat(width-950,height-100))\n",
    "                self.cats.append(self.create_cat(width-50,height-600))\n",
    "                self.cats.append(self.create_cat(width-50,height-100))\n",
    "                self.cats.append(self.create_cat(width-50,height-600))\n",
    "\n",
    "        if cur_mode in [ACQUIRE, HUNT, PACK]:\n",
    "            \n",
    "            # set up seach grid and feed first target\n",
    "            self.target_inventory = []\n",
    "            self.acquired_targets = []\n",
    "            self.current_targets = []\n",
    "            self.target_radius = TARGET_RADIUS\n",
    "            self.generate_targets(True)\n",
    "            for targets in range(NUM_TARGETS):\n",
    "                self.assign_target(True, drone_id)\n",
    "            self.target_acquired = False\n",
    "            \n",
    "            # initialize structures used to track efficiency of EACH move\n",
    "            # distance to target\n",
    "            self.last_tgt_dist = np.empty([NUM_DRONES, NUM_TARGETS])\n",
    "            self.last_tgt_dist.fill(350) # last target dist held in array for ea drone\n",
    "            tmp = [0.5, 1]; self.tgt_deltas = [] # deltas held in list for ea drone\n",
    "            for drone_id in range(NUM_DRONES): self.tgt_deltas.append(tmp)\n",
    "            \n",
    "            # distance to obstacles\n",
    "            self.last_obs_dist = np.empty([NUM_DRONES, NUM_TARGETS])\n",
    "            self.last_obs_dist.fill(10) # last obstacle dist held in array for ea drone\n",
    "            tmp = [10, 12]; self.obs_dists = [] # distances held in list for ea drone\n",
    "            for drone_id in range(NUM_DRONES): self.obs_dists.append(tmp)\n",
    "\n",
    "            # initialize structures to track efficiency of PACK_EVAL_FRAMES moves\n",
    "            if cur_mode == PACK:\n",
    "                # distance to target\n",
    "                self.last_pack_tgt_dist = np.empty([NUM_DRONES, NUM_TARGETS])\n",
    "                self.last_pack_tgt_dist.fill(350) # last target dist held in array for ea drone\n",
    "                tmp = [0.4, 0.7]; self.pack_tgt_deltas = [] # deltas held in list for ea drone\n",
    "                for drone_id in range(NUM_DRONES): self.pack_tgt_deltas.append(tmp)\n",
    "        \n",
    "                # distance to obstacles\n",
    "                self.last_pack_obs_dist = np.empty([NUM_DRONES, NUM_TARGETS])\n",
    "                self.last_pack_obs_dist.fill(10) # last obstacle dist held in array for ea drone\n",
    "                tmp = [10, 12]; self.pack_obs_dists = [] # distances held in list for ea drone\n",
    "                for drone_id in range(NUM_DRONES): self.pack_obs_dists.append(tmp)\n",
    "\n",
    "                # starting positions\n",
    "                self.pack_cum_rwds = np.zeros([NUM_DRONES, 1])\n",
    "                self.start_positions = [(25,25), (25,675), (975,25), (975,650)]\n",
    "                self.start_angles = [0.8, -0.8, 2.5, 3.9]\n",
    "\n",
    "    # ***** primary logic controller for game play *****\n",
    "    def frame_step(self, drone_id, turn_action, speed_action, pack_action, cur_speed, total_ctr, replay_ctr):\n",
    "\n",
    "        self.total_frame_ctr = total_ctr\n",
    "        self.replay_frame_ctr = replay_ctr\n",
    "        self.acquire_frame_ctr += 1\n",
    "        \n",
    "        # turn drone based on current (active) model prediction\n",
    "        if cur_mode in [TURN, AVOID, ACQUIRE, HUNT, PACK]:\n",
    "            self.set_turn(turn_action, drone_id)\n",
    "    \n",
    "        # set speed based on active model prediction\n",
    "        if cur_mode in [AVOID, HUNT, PACK]: # setting speed values directly see SPEEDS\n",
    "            cur_speed = self.set_speed(speed_action, drone_id)\n",
    "        \n",
    "        # effect move by applying speed and direction as vector on self\n",
    "        driving_direction = Vec2d(1, 0).rotated(self.drones[drone_id].angle)\n",
    "        self.drones[drone_id].velocity = cur_speed * driving_direction\n",
    "        x, y = self.drones[drone_id].position\n",
    "        \n",
    "        # set heading adjustment based on pack model output\n",
    "        if cur_mode == PACK:\n",
    "            heading_adjust = self.set_pack_adjust(pack_action)[drone_id]\n",
    "        else:\n",
    "            heading_adjust = 0\n",
    "        \n",
    "        # move obstacles\n",
    "        if cur_mode in [TURN, AVOID, HUNT, PACK]:\n",
    "            # slow obstacles\n",
    "            if self.total_frame_ctr % 20 == 0: # 20x slower than self\n",
    "                self.move_obstacles()\n",
    "\n",
    "            # fast obstacles\n",
    "            if self.total_frame_ctr % 40 == 0: # 40 x more stable than self\n",
    "                self.move_cats()\n",
    "        \n",
    "        # update the screen and surfaces\n",
    "        if drone_id == 0:\n",
    "            screen.fill(pygame.color.THECOLORS[BACK_COLOR])\n",
    "        \n",
    "        if cur_mode in [ACQUIRE, HUNT, PACK]:\n",
    "            # draw the path drone has taken on the path grid\n",
    "            if self.acquire_frame_ctr / NUM_DRONES > 1.5:\n",
    "                pygame.draw.lines(path_grid, pygame.color.THECOLORS[PATH_COLOR], True,\n",
    "                                  ((self.last_x[drone_id], height - self.last_y[drone_id]),\n",
    "                                   (x, height - y)), 1)\n",
    "            \n",
    "            # if last drone, bind paths, targets to the screen\n",
    "            if drone_id == (NUM_DRONES - 1):\n",
    "                screen.blit(path_grid, (0,0))\n",
    "                screen.blit(target_grid, (0,0))\n",
    "\n",
    "        # if last drone, display screen\n",
    "        #if(drone_id == (NUM_DRONES - 1)):\n",
    "        draw(screen, self.space)\n",
    "        self.space.step(1./10) # one pixel for every 10 SPEED\n",
    "        if draw_screen:\n",
    "            pygame.display.flip()\n",
    "\n",
    "        # get readings, build states\n",
    "        self.last_x[drone_id] = x; self.last_y[drone_id] = y\n",
    "        x, y = self.drones[drone_id].position\n",
    "\n",
    "        turn_state, avoid_state, acquire_state, hunt_state, drone_state, min_sonar_dist, avoid_move_efficiency, acquire_move_efficiency = \\\n",
    "            self.build_states(drone_id, turn_action, heading_adjust, cur_speed)\n",
    "        \n",
    "        # calc rewards based on training mode(s) in effect\n",
    "        reward = self.calc_rwd(drone_id, min_sonar_dist, driving_direction, cur_speed, avoid_move_efficiency, acquire_move_efficiency)\n",
    "        \n",
    "        # introduce obstacles gradually for HUNT/PACK learning\n",
    "        if cur_mode in [HUNT, PACK] and drone_id == (NUM_DRONES - 1):\n",
    "            if self.total_frame_ctr > 1 and \\\n",
    "                self.total_frame_ctr < 601 and \\\n",
    "                self.total_frame_ctr % 100 == 0:\n",
    "                self.obstacles.append(self.create_obstacle(random.randint(200, width-200),\n",
    "                                                           random.randint(140, height-140),\n",
    "                                                           OBSTACLE_SIZES[int(self.total_frame_ctr / 100)-1]))\n",
    "                self.target_radius -= 1\n",
    "        \n",
    "            if self.total_frame_ctr > 601 and \\\n",
    "                self.total_frame_ctr < 1001 and \\\n",
    "                self.total_frame_ctr % 100 == 0:\n",
    "\n",
    "                self.cats.append(self.create_cat(width-500,height-350))\n",
    "\n",
    "        #self.total_frame_ctr += 1\n",
    "        clock.tick()\n",
    "\n",
    "        return turn_state, avoid_state, acquire_state, hunt_state, drone_state, reward, cur_speed\n",
    "                \n",
    "    # ***** turn and speed model functions *****\n",
    "    def create_obstacle(self, x, y, r):\n",
    "        obs_body = pymunk.Body(pymunk.inf, pymunk.inf)\n",
    "        obs_shape = pymunk.Circle(obs_body, r)\n",
    "        obs_shape.elasticity = 1.0\n",
    "        obs_body.position = x, y\n",
    "        obs_shape.color = THECOLORS[OBSTACLE_COLOR]\n",
    "        self.space.add(obs_body, obs_shape)\n",
    "        return obs_body\n",
    "    \n",
    "    def create_cat(self,x,y):\n",
    "        inertia = pymunk.moment_for_circle(1, 0, 14, (0, 0))\n",
    "        cat_body = pymunk.Body(1, inertia)\n",
    "        cat_body.position = x, y\n",
    "        cat_shape = pymunk.Circle(cat_body, 20)\n",
    "        cat_shape.color = THECOLORS[CAT_COLOR]\n",
    "        cat_shape.elasticity = 1.0\n",
    "        cat_shape.angle = 0.5\n",
    "        direction = Vec2d(1, 0).rotated(cat_body.angle)\n",
    "        self.space.add(cat_body, cat_shape)\n",
    "        return cat_body\n",
    "    \n",
    "    def create_drone(self, x, y, r):\n",
    "        inertia = pymunk.moment_for_circle(1, 0, 14, (0, 0))\n",
    "        drone_body = pymunk.Body(1, inertia)\n",
    "        drone_body.position = x, y\n",
    "        drone_shape = pymunk.Circle(drone_body, DRONE_BODY_DIAM) # was 25\n",
    "        drone_shape.color = THECOLORS[DRONE_COLOR]\n",
    "        drone_shape.elasticity = 1.0\n",
    "        drone_body.angle = r\n",
    "        driving_direction = Vec2d(1, 0).rotated(drone_body.angle)\n",
    "        drone_body.apply_impulse(driving_direction)\n",
    "        self.space.add(drone_body, drone_shape)\n",
    "        return drone_body\n",
    "    \n",
    "    def move_obstacles(self):\n",
    "        # randomly moves large, slow obstacles around\n",
    "        if len(self.obstacles) > 0:\n",
    "            for obstacle in self.obstacles:\n",
    "                speed = random.randint(10, 15)\n",
    "                direction = Vec2d(1, 0).rotated(self.drones[0].angle + random.randint(-2, 2))\n",
    "                obstacle.velocity = speed * direction\n",
    "    \n",
    "    def move_cats(self):\n",
    "        # randomly moves small, fast obstacles\n",
    "        if len(self.cats) > 0:\n",
    "            for cat in self.cats:\n",
    "                speed = random.randint(60, 80)\n",
    "                direction = Vec2d(1, 0).rotated(random.randint(-3, 3)) # -2,2\n",
    "                \n",
    "                x, y = cat.position\n",
    "                if x < 0 or x > width or y < 0 or y > height:\n",
    "                    cat.position = int(width/2), int(height/2)\n",
    "                cat.velocity = speed * direction\n",
    "\n",
    "    def set_turn(self, turn_action, drone_id):\n",
    "        # action == 0 is continue current trajectory\n",
    "        if turn_action == 1:  # slight right adjust to current trajectory\n",
    "            self.drones[drone_id].angle -= .2\n",
    "        elif turn_action == 2:  # hard right\n",
    "            self.drones[drone_id].angle -= .4\n",
    "        elif turn_action == 3:  # slight left\n",
    "            self.drones[drone_id].angle += .2\n",
    "        elif turn_action == 4:  # hard left\n",
    "            self.drones[drone_id].angle += .4\n",
    "\n",
    "    def set_speed(self, speed_action, drone_id):\n",
    "        # choose appropriate speed action, including 0 speed\n",
    "        if speed_action == 0:\n",
    "            cur_speed = SPEEDS[0]\n",
    "        elif speed_action == 1:\n",
    "            cur_speed = SPEEDS[1]\n",
    "        elif speed_action == 2:\n",
    "            cur_speed = SPEEDS[2]\n",
    "\n",
    "        return cur_speed\n",
    "\n",
    "    def set_pack_adjust(self, pack_action):\n",
    "        \n",
    "        heading_adjust = []\n",
    "        # pack actions effect +/- 0.8 radian (45 deg) drone heading adjustments (2)\n",
    "        for i in range(NUM_DRONES):\n",
    "            heading = PACK_HEADING_ADJUST[pack_action][i]\n",
    "            heading_adjust.append(heading * (3.14 / 4))\n",
    "                \n",
    "        return heading_adjust\n",
    "\n",
    "    def evaluate_move(self, drone_id, heading_adjust, min_sonar_dist):\n",
    "        '''eventually, will introduce multiple targets but each new target doubles \n",
    "        state variables. so, for now, assuming single target:'''\n",
    "        target_id = 0\n",
    "        avoid_move_efficiency = 0\n",
    "        acquire_move_efficiency = 0\n",
    "        \n",
    "        # 1. calc distance and angle to active target(s)\n",
    "        # a. euclidean distance traveled\n",
    "        x, y = self.drones[drone_id].position\n",
    "        dx = self.current_targets[target_id][0] - x\n",
    "        dy = self.current_targets[target_id][1] - y\n",
    "        \n",
    "        target_dist = int(((dx**2 + dy**2)**0.5))\n",
    "        # b. calc target angle\n",
    "        # i. relative to drone\n",
    "        rads = atan2(dy,dx)\n",
    "        rads %= 2*pi\n",
    "        \n",
    "        true_target_angle_rads = np.round(rads,1)\n",
    "        if true_target_angle_rads > 3.14:\n",
    "            true_target_angle_rads = true_target_angle_rads - 6.28\n",
    "        \n",
    "        true_target_angle_degs = degrees(rads)\n",
    "        if true_target_angle_degs > 180:\n",
    "            true_target_angle_degs = true_target_angle_degs - 360\n",
    "        \n",
    "        rads = atan2(dy,dx)\n",
    "        rads = rads + heading_adjust\n",
    "        rads %= 2*pi\n",
    "        adj_target_angle_degs = degrees(rads)\n",
    "        \n",
    "        if adj_target_angle_degs > 180:\n",
    "            adj_target_angle_degs = adj_target_angle_degs - 360\n",
    "\n",
    "        # ii. relative to drone's current direction\n",
    "        rads = self.drones[drone_id].angle\n",
    "        rads %= 2*pi\n",
    "        drone_angle_rads = rads\n",
    "        drone_angle_degs = degrees(rads)\n",
    "            \n",
    "        if drone_angle_degs > 360:\n",
    "            drone_angle_degs = drone_angle_degs - 360\n",
    "            \n",
    "        # \"heading\" accounts for angle FROM drone and OF drone netting degrees drone must turn\n",
    "        adj_heading_to_target = adj_target_angle_degs - drone_angle_degs\n",
    "        if adj_heading_to_target < -180:\n",
    "            adj_heading_to_target = adj_heading_to_target + 360\n",
    "        \n",
    "        if cur_mode != PACK:\n",
    "            # 3. calc normalized efficiency of last move\n",
    "            # vs. target acquisition\n",
    "            dt = int(self.last_tgt_dist[drone_id, target_id] - target_dist)\n",
    "            \n",
    "            if abs(dt) >= 12: # mistakenly thinking crashes are moves. so, ignore moves > 12\n",
    "                dt = np.mean(self.tgt_deltas[drone_id])\n",
    "\n",
    "            # postive distance delta indicates \"closing\" on the target\n",
    "            ndt = np.round((dt - np.mean(self.tgt_deltas[drone_id])) / np.std(self.tgt_deltas[drone_id]),2)\n",
    "            \n",
    "            # save current values\n",
    "            self.last_tgt_dist[drone_id, target_id] = target_dist\n",
    "            self.tgt_deltas[drone_id].append(dt)\n",
    "\n",
    "            if len(self.tgt_deltas[drone_id]) > STATS_BUFFER:\n",
    "                self.tgt_deltas[drone_id].pop(0)\n",
    "\n",
    "            # vs. obstacle avoidance\n",
    "            do = min_sonar_dist\n",
    "            \n",
    "            # positive distance delta indicates \"avoiding\" an obstacle\n",
    "            ndo = np.round((do - np.mean(self.obs_dists[drone_id])) / np.std(self.obs_dists[drone_id]),2)\n",
    "            \n",
    "            # save current values\n",
    "            self.last_obs_dist[drone_id] = do\n",
    "            self.obs_dists[drone_id].append(do)\n",
    "\n",
    "            if len(self.obs_dists[drone_id]) > STATS_BUFFER:\n",
    "                self.obs_dists[drone_id].pop(0)\n",
    "            \n",
    "            # finally, apply calcs to score move\n",
    "            if cur_mode == ACQUIRE:\n",
    "                acquire_move_efficiency = np.round(ndt / target_dist**0.333,2)\n",
    "                # cubed root of the target distance... lessens effect of distance\n",
    "            else:\n",
    "                avoid_move_efficiency = np.round(ndo / target_dist**0.333,2) # was 0.333\n",
    "                acquire_move_efficiency = np.round(ndt / target_dist**0.333,2)\n",
    "                # for balancing avoidance with acquisition\n",
    "        \n",
    "        else:\n",
    "            #if self.total_frame_ctr == 1 or self.replay_frame_ctr % PACK_EVAL_FRAMES == 0:\n",
    "            # 3. calc normalized efficiency of last move\n",
    "            # vs. target acquisition\n",
    "            dt = int(self.last_pack_tgt_dist[drone_id, target_id] - target_dist)\n",
    "            #print(\"dt:\", dt)\n",
    "            if abs(dt) >= 12: # mistakenly thinking crashes are moves. so, ignore moves > 12\n",
    "                dt = np.mean(self.pack_tgt_deltas[drone_id])\n",
    "        \n",
    "            # postive distance delta indicates \"closing\" on the target\n",
    "            ndt = np.round((dt - np.mean(self.pack_tgt_deltas[drone_id])) / np.std(self.pack_tgt_deltas[drone_id]),2)\n",
    "            #print(\"ndt:\", ndt)\n",
    "            # save current values\n",
    "            self.last_pack_tgt_dist[drone_id, target_id] = target_dist\n",
    "            self.pack_tgt_deltas[drone_id].append(dt)\n",
    "            \n",
    "            if len(self.pack_tgt_deltas[drone_id]) > STATS_BUFFER:\n",
    "                self.pack_tgt_deltas[drone_id].pop(0)\n",
    "\n",
    "            # vs. obstacle avoidance\n",
    "            do = min_sonar_dist\n",
    "            #print(\"do:\", do)\n",
    "            # positive distance delta indicates \"avoiding\" an obstacle\n",
    "            ndo = np.round((do - np.mean(self.pack_obs_dists[drone_id])) / np.std(self.pack_obs_dists[drone_id]),2)\n",
    "            #print(\"ndo:\", ndo)\n",
    "            # save current values\n",
    "            self.last_pack_obs_dist[drone_id] = do\n",
    "            self.pack_obs_dists[drone_id].append(do)\n",
    "        \n",
    "            if len(self.pack_obs_dists[drone_id]) > STATS_BUFFER:\n",
    "                self.pack_obs_dists[drone_id].pop(0)\n",
    "\n",
    "            # finally, apply calcs to score move\n",
    "            avoid_move_efficiency = np.round(ndo / target_dist**0.333,2)\n",
    "            acquire_move_efficiency = np.round(ndt / target_dist**0.333,2)\n",
    "            # for balancing avoidance with acquisition\n",
    "            \n",
    "        # 4. if w/in reasonable distance, declare victory\n",
    "        if target_dist <= TARGET_RADIUS:\n",
    "            print(\"************** target acquired ************\")\n",
    "            self.target_acquired = True\n",
    "        \n",
    "            # move acquired target to acquired targets\n",
    "            self.acquired_targets.append(self.current_targets[target_id])\n",
    "            self.target_inventory.remove(self.current_targets[target_id])\n",
    "            \n",
    "            print(\"pct complete:\", (len(self.acquired_targets) /\n",
    "                                    (len(self.acquired_targets) + len(self.target_inventory))))\n",
    "                    \n",
    "            if len(self.acquired_targets) % 20 == 1:\n",
    "                take_screen_shot(screen)\n",
    "                time.sleep(0.2) # screen capture takes a bit\n",
    "            \n",
    "            # remove old target\n",
    "            self.current_targets.remove(self.current_targets[target_id])\n",
    "            \n",
    "            # get a new target\n",
    "            self.assign_target(False, drone_id)\n",
    "\n",
    "            start_dists = []\n",
    "            if cur_mode == PACK:\n",
    "                # find furthest two start positions\n",
    "                for i in range(len(self.start_positions)):\n",
    "                    dx = self.start_positions[i][0] - self.current_targets[0][0]\n",
    "                    dy = self.start_positions[i][1] - self.current_targets[0][1]\n",
    "                    start_dists.append(int(((dx**2 + dy**2)**0.5)))\n",
    "                \n",
    "                # move drones to start position\n",
    "                for i in range(NUM_DRONES):\n",
    "                    self.drones[i].position = \\\n",
    "                        self.start_positions[start_dists.index(max(start_dists))][0], \\\n",
    "                        self.start_positions[start_dists.index(max(start_dists))][1]\n",
    "                    \n",
    "                    self.drones[i].angle = self.start_angles[start_dists.index(max(start_dists))]\n",
    "\n",
    "        return target_dist, true_target_angle_rads, drone_angle_rads, adj_heading_to_target, \\\n",
    "            avoid_move_efficiency, acquire_move_efficiency\n",
    "            \n",
    "    def build_states(self, drone_id, turn_action, heading_adjust, cur_speed):\n",
    "        turn_state = 0\n",
    "        avoid_state = 0\n",
    "        acquire_state = 0\n",
    "        hunt_state = 0\n",
    "        drone_state = 0\n",
    "        min_sonar_dist = 0\n",
    "        avoid_move_efficiency = 0\n",
    "        acquire_move_efficiency = 0\n",
    "        \n",
    "        # get readings from the various sensors\n",
    "        sonar_dist_readings, sonar_color_readings = \\\n",
    "            self.get_sonar_dist_color_readings(drone_id)\n",
    "        \n",
    "        turn_readings = sonar_dist_readings[:TURN_NUM_SENSOR]\n",
    "        min_sonar_dist = min(turn_readings)\n",
    "        turn_readings = turn_readings + sonar_color_readings[:TURN_NUM_SENSOR]\n",
    "        turn_state = np.array([turn_readings])\n",
    "        \n",
    "        if cur_mode != TURN:\n",
    "            avoid_readings = sonar_dist_readings[:AVOID_NUM_SENSOR]\n",
    "            min_sonar_dist = min(avoid_readings)\n",
    "            avoid_readings = avoid_readings + sonar_color_readings[:AVOID_NUM_SENSOR]\n",
    "            avoid_readings.append(turn_action)\n",
    "            avoid_readings.append(cur_speed)\n",
    "            avoid_state = np.array([avoid_readings])\n",
    "        \n",
    "        if cur_mode in [ACQUIRE, HUNT, PACK]:\n",
    "            # calc distances, headings and efficiency\n",
    "            min_sonar_dist = min(sonar_dist_readings[:HUNT_NUM_SENSOR])\n",
    "            # note: avoid, hunt, pack all using 7 sensors for min dist.\n",
    "            # however, pack will only be seeing four sensors. FIX THIS AT SOME POINT.\n",
    "            # problem is, you can't call evaluate_move twice as it appends readings for mean sd ea time. \n",
    "            # So, some moves will be evaluated based on sensor distances it doesn't see.\n",
    "            target_dist, target_angle_rads, drone_angle_rads, adj_heading_to_target, \\\n",
    "                avoid_move_efficiency, acquire_move_efficiency = \\\n",
    "                self.evaluate_move(drone_id, heading_adjust, min_sonar_dist)\n",
    "        \n",
    "            acquire_state = np.array([[target_dist, adj_heading_to_target]])\n",
    "\n",
    "            if cur_mode in [HUNT, PACK]:\n",
    "                hunt_readings = sonar_dist_readings[:HUNT_NUM_SENSOR]\n",
    "                hunt_readings = hunt_readings + sonar_color_readings[:HUNT_NUM_SENSOR]\n",
    "                hunt_readings.append(target_dist)\n",
    "                hunt_readings.append(adj_heading_to_target)\n",
    "                hunt_state = np.array([hunt_readings])\n",
    "                min_sonar_dist = min(sonar_dist_readings[:HUNT_NUM_SENSOR])\n",
    "\n",
    "            if cur_mode == PACK and (self.total_frame_ctr == 1 or self.replay_frame_ctr % PACK_EVAL_FRAMES == 0):\n",
    "                # pack requires four compas point (above, below, right and left) obs dist readings\n",
    "                compass_rads = [0, (3.14/2), 3.14, (-3.14/2)]\n",
    "                drone_readings = []\n",
    "                \n",
    "                # it gets readings by adjusting the sonar readings for the drone angle...\n",
    "                #print(drone_angle_rads)\n",
    "                sonar_angles = [0, 0.6, -0.6, 1.2, -1.2, 2.8, -2.8]\n",
    "                sonar_angles_adj = np.add(sonar_angles, drone_angle_rads)\n",
    "                #print(sonar_angles_adj)\n",
    "                \n",
    "                # ...then finds the sonar reading closest to its required compass direction\n",
    "                for rad in range(len(compass_rads)):\n",
    "                    drone_readings.append(sonar_dist_readings[find_nearest(sonar_angles_adj,\n",
    "                                                                          compass_rads[rad])])\n",
    "                drone_readings.append(target_dist)\n",
    "                drone_readings.append(target_angle_rads)\n",
    "                drone_state = np.array([drone_readings])\n",
    "\n",
    "        return turn_state, avoid_state, acquire_state, hunt_state, drone_state, \\\n",
    "            min_sonar_dist, avoid_move_efficiency, acquire_move_efficiency\n",
    "\n",
    "    def calc_rwd(self, drone_id, min_sonar_dist, driving_direction, cur_speed, \n",
    "                 avoid_move_efficiency, acquire_move_efficiency):\n",
    "\n",
    "        reward = 0\n",
    "        x, y = self.drones[drone_id].position\n",
    "\n",
    "        # check for crash\n",
    "        if min_sonar_dist <= 1: #  and cur_mode != PACK\n",
    "            reward = -500\n",
    "            if x < 0 or x > width or y < 0 or y > height:\n",
    "                self.drones[drone_id].position = int(width/2), int(height/2)\n",
    "                self.num_off_scrn += 1\n",
    "                print(\"off screen. total off screens\", self.num_off_scrn)\n",
    "                reward = -1000\n",
    "            self.recover_from_crash(driving_direction, drone_id)\n",
    "        \n",
    "        else:\n",
    "            if cur_mode == TURN:\n",
    "                # Rewards better spacing from objects\n",
    "                reward = min_sonar_dist\n",
    "            \n",
    "            elif cur_mode == AVOID:\n",
    "                # rewards distance from objects and speed\n",
    "                sd_speeds = np.std(SPEEDS)\n",
    "                sd_dist = np.std(range(20))\n",
    "            \n",
    "                std_speed = cur_speed / sd_speeds\n",
    "                std_dist = min_sonar_dist / sd_dist\n",
    "            \n",
    "                std_max_speed = max(SPEEDS) / sd_speeds\n",
    "                std_max_dist = SONAR_ARM_LEN / sd_dist\n",
    "            \n",
    "                reward = ((std_speed * std_dist) +\n",
    "                          ((std_max_speed - std_speed) * (std_max_dist - std_dist)))\n",
    "            \n",
    "            else: # i.e., cur_mode is acquisition-related (acquire, hunt, pack)\n",
    "                # rewards moving in the right direction and acquiring pixels\n",
    "                if self.target_acquired == True:\n",
    "                    reward = 1000\n",
    "                    self.target_acquired = False\n",
    "                    self.acquire_frame_ctr = 0\n",
    "\n",
    "                else:\n",
    "                    if cur_mode == ACQUIRE:\n",
    "                        reward = 100 * acquire_move_efficiency\n",
    "                    \n",
    "                    elif cur_mode == HUNT:\n",
    "                        reward = 40 * acquire_move_efficiency + 50 * avoid_move_efficiency\n",
    "\n",
    "        if cur_mode == PACK:\n",
    "            # rewards moving all drones in right direction and acquiring pixels\n",
    "            \n",
    "            if reward == 1000:\n",
    "                self.pack_cum_rwds[drone_id, 0] += 1000\n",
    "                \n",
    "            elif reward == -500 or reward == -1000:\n",
    "                self.pack_cum_rwds[drone_id, 0] -= 500\n",
    "        \n",
    "            else:\n",
    "                # two drones. reward each 1/2 of total in acquire/avoid eff proportion\n",
    "                self.pack_cum_rwds[drone_id, 0] += \\\n",
    "                    ((30 / PACK_EVAL_FRAMES) * acquire_move_efficiency) + \\\n",
    "                    ((20 / PACK_EVAL_FRAMES) * avoid_move_efficiency)\n",
    "            \n",
    "            if self.total_frame_ctr == 1 or self.replay_frame_ctr % PACK_EVAL_FRAMES == 0:\n",
    "                reward = int(self.pack_cum_rwds[drone_id, 0])\n",
    "                self.pack_cum_rwds[drone_id, 0] = 0\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    def recover_from_crash(self, driving_direction, drone_id):\n",
    "        # back up\n",
    "        crash_adjust = -100\n",
    "        self.drones[drone_id].velocity = crash_adjust * driving_direction\n",
    "        \n",
    "        for i in range(10):\n",
    "            self.drones[drone_id].angle += .2  # Turn a little.\n",
    "            screen.fill(THECOLORS[\"red\"])  # Red is scary!\n",
    "            draw(screen, self.space)\n",
    "            self.space.step(1./10)\n",
    "            if draw_screen:\n",
    "                pygame.display.flip()\n",
    "            clock.tick()\n",
    "\n",
    "    def get_sonar_dist_color_readings(self, drone_id):\n",
    "        sonar_dist_readings = []\n",
    "        sonar_color_readings = []\n",
    "        \"\"\"\n",
    "        sonar readings return N \"distance\" readings, one for each sonar. distance is\n",
    "        a count of the first non-zero color detection reading starting at the object.\n",
    "        \"\"\"\n",
    "        \n",
    "        # make sonar \"arms\"\n",
    "        arm_1 = self.make_sonar_arm(drone_id)\n",
    "        arm_2 = arm_1\n",
    "        arm_3 = arm_1\n",
    "        arm_4 = arm_1\n",
    "        arm_5 = arm_1\n",
    "        arm_6 = arm_1\n",
    "        arm_7 = arm_1\n",
    "        \n",
    "        # rotate arms to get vector of readings\n",
    "        d, c = self.get_arm_dist_color(arm_1, 0, drone_id)\n",
    "        sonar_dist_readings.append(d); sonar_color_readings.append(c)\n",
    "        d, c = self.get_arm_dist_color(arm_2, 0.6, drone_id)\n",
    "        sonar_dist_readings.append(d); sonar_color_readings.append(c)\n",
    "        d, c = self.get_arm_dist_color(arm_3, -0.6, drone_id)\n",
    "        sonar_dist_readings.append(d); sonar_color_readings.append(c)\n",
    "        d, c = self.get_arm_dist_color(arm_4, 1.2, drone_id)\n",
    "        sonar_dist_readings.append(d); sonar_color_readings.append(c)\n",
    "        d, c = self.get_arm_dist_color(arm_5, -1.2, drone_id)\n",
    "        sonar_dist_readings.append(d); sonar_color_readings.append(c)\n",
    "        d, c = self.get_arm_dist_color(arm_6, 2.8, drone_id)\n",
    "        sonar_dist_readings.append(d); sonar_color_readings.append(c)\n",
    "        d, c = self.get_arm_dist_color(arm_7, -2.8, drone_id)\n",
    "        sonar_dist_readings.append(d); sonar_color_readings.append(c)\n",
    "        \n",
    "        if show_sensors:\n",
    "            pygame.display.update()\n",
    "\n",
    "        return sonar_dist_readings, sonar_color_readings\n",
    "\n",
    "\n",
    "    def get_arm_dist_color(self, arm, offset, drone_id):\n",
    "        # count arm length to nearest obstruction\n",
    "        i = 0\n",
    "        x, y = self.drones[drone_id].position\n",
    "        \n",
    "        # evaluate each arm point to see if we've hit something\n",
    "        for point in arm:\n",
    "            i += 1\n",
    "            \n",
    "            # move the point to the right spot\n",
    "            rotated_p = self.get_rotated_point(x, y, point[0], point[1],\n",
    "                                               self.drones[drone_id].angle + offset)\n",
    "            \n",
    "            # return i if rotated point is off screen\n",
    "            if rotated_p[0] <= 0 or rotated_p[1] <= 0 or rotated_p[0] >= width or rotated_p[1] >= height:\n",
    "                return i, 1 # 1 is wall color\n",
    "            else:\n",
    "                obs = screen.get_at(rotated_p)\n",
    "                \n",
    "                # this gets the color of the pixel at the rotated point\n",
    "                obs_color = self.get_track_or_not(obs)\n",
    "                \n",
    "                if obs_color != 0:\n",
    "                    # if pixel not a safe color, return distance\n",
    "                    return i, obs_color\n",
    "\n",
    "            # plots the individual sonar point on the screen\n",
    "            if show_sensors:\n",
    "                pygame.draw.circle(screen, (255, 255, 255), (rotated_p), 2)\n",
    "\n",
    "        return i, 0 # 0 is safe color\n",
    "\n",
    "\n",
    "    def make_sonar_arm(self, drone_id):\n",
    "        x, y = self.drones[drone_id].position\n",
    "        \n",
    "        spread = 10  # gap between points on sonar arm\n",
    "        distance = 10  # number of points on sonar arm\n",
    "        arm_points = []\n",
    "        # builds arm flat. it will be rotated about the center later\n",
    "        for i in range(1, SONAR_ARM_LEN): # was 40\n",
    "            arm_points.append((x + distance + (spread * i), y))\n",
    "\n",
    "        return arm_points\n",
    "    \n",
    "    \n",
    "    def get_rotated_point(self, x_1, y_1, x_2, y_2, radians):\n",
    "        # Rotate x_2, y_2 around x_1, y_1 by angle.\n",
    "        x_change = (x_2 - x_1) * math.cos(radians) + \\\n",
    "            (y_2 - y_1) * math.sin(radians)\n",
    "        y_change = (y_1 - y_2) * math.cos(radians) - \\\n",
    "            (x_1 - x_2) * math.sin(radians)\n",
    "        new_x = x_change + x_1\n",
    "        new_y = height - (y_change + y_1)\n",
    "        \n",
    "        return int(new_x), int(new_y)\n",
    "\n",
    "\n",
    "    def get_track_or_not(self, reading):\n",
    "        # check to see if color encountered is safe (i.e., should not be crash)\n",
    "        if reading == pygame.color.THECOLORS[BACK_COLOR] or \\\n",
    "            reading == pygame.color.THECOLORS[DRONE_COLOR] or \\\n",
    "            reading == pygame.color.THECOLORS[SMILE_COLOR] or \\\n",
    "            reading == pygame.color.THECOLORS[ACQUIRE_PIXEL_COLOR] or \\\n",
    "            reading == pygame.color.THECOLORS[ACQUIRED_PIXEL_COLOR] or \\\n",
    "            reading == pygame.color.THECOLORS[PATH_COLOR]:\n",
    "            return 0\n",
    "        else:\n",
    "            if reading == pygame.color.THECOLORS[WALL_COLOR]:\n",
    "                return 1\n",
    "            elif reading == pygame.color.THECOLORS[CAT_COLOR]:\n",
    "                return 2\n",
    "            elif reading == pygame.color.THECOLORS[OBSTACLE_COLOR]:\n",
    "                return 3\n",
    "            else:\n",
    "                return 1\n",
    "\n",
    "    # ***** target and acquire model functions *****\n",
    "    def generate_targets(self, first_iter):\n",
    "        \n",
    "        # calc number of targets that can fit space\n",
    "        num_pxl_x_dir = int((width - 2 * ACQUIRE_MARGIN)/ACQUIRE_PIXEL_SEPARATION)\n",
    "        num_pxl_y_dir = int((height- 2 * ACQUIRE_MARGIN)/ACQUIRE_PIXEL_SEPARATION)\n",
    "        \n",
    "        n = num_pxl_x_dir * num_pxl_y_dir\n",
    "        \n",
    "        ctr = 0\n",
    "        for v in range(num_pxl_y_dir):\n",
    "            for h in range(num_pxl_x_dir):\n",
    "                \n",
    "                # space targets across target grid\n",
    "                x_pxl = (ACQUIRE_MARGIN + (h * ACQUIRE_PIXEL_SEPARATION))\n",
    "                y_pxl = (ACQUIRE_MARGIN + (v * ACQUIRE_PIXEL_SEPARATION))\n",
    "                \n",
    "                if(first_iter == True):\n",
    "                    self.target_inventory.append((x_pxl,y_pxl))\n",
    "                \n",
    "                ctr += 1\n",
    "\n",
    "        return num_pxl_x_dir, num_pxl_y_dir\n",
    "\n",
    "    def assign_target(self, first_iter, drone_id):\n",
    "        \n",
    "        # clear the path surface\n",
    "        path_grid.fill(pygame.color.THECOLORS[BACK_COLOR])\n",
    "        \n",
    "        # mark target as acquired\n",
    "        x, y = self.drones[drone_id].position\n",
    "        \n",
    "        if first_iter == False:\n",
    "            \n",
    "            pygame.draw.rect(target_grid, pygame.color.THECOLORS[ACQUIRED_PIXEL_COLOR],\n",
    "                             ((x, height - y), (ACQUIRE_PIXEL_SIZE, ACQUIRE_PIXEL_SIZE)), 0)\n",
    "        \n",
    "        # randomly select a new target\n",
    "        new_target = random.choice(self.target_inventory)\n",
    "        self.current_targets.append(new_target)\n",
    "\n",
    "        # draw the new target\n",
    "        pygame.draw.rect(target_grid, pygame.color.THECOLORS[ACQUIRE_PIXEL_COLOR],\n",
    "                         ((new_target[0], height - new_target[1]),\n",
    "                          (ACQUIRE_PIXEL_SIZE, ACQUIRE_PIXEL_SIZE)), 0)\n",
    "\n",
    "# ***** global functions *****\n",
    "def find_nearest(array, value):\n",
    "    idx = (np.abs(array-value)).argmin()\n",
    "    return idx\n",
    "\n",
    "def take_screen_shot(screen):\n",
    "    time_taken = time.asctime(time.localtime(time.time()))\n",
    "    time_taken = time_taken.replace(\" \", \"_\")\n",
    "    time_taken = time_taken.replace(\":\",\".\")\n",
    "    save_file = \"screenshots/\" + time_taken + \".jpeg\"\n",
    "    pygame.image.save(screen,save_file)\n",
    "    print(\"screen shot taken\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section below trains the neural networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import os.path\n",
    "import timeit\n",
    "import time\n",
    "\n",
    "# initialize\n",
    "GAMMA = 0.9  # Forgetting.\n",
    "\n",
    "def train_net(turn_model, turn_model_30, turn_model_50, turn_model_70, avoid_model, acquire_model,\n",
    "              acquire_model_30, acquire_model_50, acquire_model_70, hunt_model, pack_model, params):\n",
    "    \n",
    "    filename = params_to_filename(params)\n",
    "    \n",
    "    if cur_mode in [TURN, HUNT, PACK]:\n",
    "        observe = 2000  # Number of frames to observe before training.\n",
    "    else:\n",
    "        observe = 2000\n",
    "\n",
    "    epsilon = 1 # vary this based on pre-learning already occurred in lower models\n",
    "    train_frames = 750000  # number of flips for training\n",
    "    batchSize = params['batchSize']\n",
    "    buffer = params['buffer']\n",
    "\n",
    "    # initialize variables and structures used below.\n",
    "    max_crash_frame_ctr = 0\n",
    "    crash_frame_ctr = 0\n",
    "    total_frame_ctr = 0\n",
    "    replay_frame_ctr = 0\n",
    "    stop_ctr = 0\n",
    "    avoid_ctr = 0\n",
    "    acquire_ctr = 0\n",
    "    cum_rwd = 0\n",
    "    cum_speed = 0\n",
    "\n",
    "    data_collect = []\n",
    "    replay = []\n",
    "    loss_log = [] # replay stores state, action, reward, new state\n",
    "    save_init = True\n",
    "    cur_speeds = []\n",
    "    for i in range(NUM_DRONES): cur_speeds.append(START_SPEED)\n",
    "    \n",
    "    # initialize drone state holders\n",
    "    turn_states = np.zeros([NUM_DRONES, TURN_TOTAL_SENSORS * TURN_STATE_FRAMES])\n",
    "    avoid_states = np.zeros([NUM_DRONES, AVOID_TOTAL_SENSORS * AVOID_STATE_FRAMES])\n",
    "    acquire_states = np.zeros([NUM_DRONES, ACQUIRE_NUM_SENSOR * ACQUIRE_STATE_FRAMES])\n",
    "    hunt_states = np.zeros([NUM_DRONES, HUNT_TOTAL_SENSORS * HUNT_STATE_FRAMES])\n",
    "    drone_states = np.zeros([NUM_DRONES, DRONE_TOTAL_SENSOR * PACK_STATE_FRAMES])\n",
    "    \n",
    "    # create game instance\n",
    "    game_state = GameState()\n",
    "    \n",
    "    # get initial state(s)\n",
    "    turn_state, avoid_state, acquire_state, hunt_state, drone_state, reward, cur_speed = \\\n",
    "        game_state.frame_step(START_DRONE_ID, START_TURN_ACTION, START_SPEED_ACTION,\n",
    "                              START_PACK_ACTION, START_SPEED, START_DISTANCE, 1)\n",
    "\n",
    "    # initialize frame states\n",
    "    if cur_mode in [TURN, AVOID, HUNT, PACK]:\n",
    "        \n",
    "        for i in range(NUM_DRONES): \n",
    "            turn_states[i] = state_frames(turn_state, \n",
    "                                          np.zeros((1, TURN_TOTAL_SENSORS * TURN_STATE_FRAMES)),\n",
    "                                          TURN_TOTAL_SENSORS, TURN_STATE_FRAMES)\n",
    "        \n",
    "        if cur_mode in [AVOID, HUNT, PACK]:\n",
    "            \n",
    "            for i in range(NUM_DRONES): \n",
    "                avoid_states[i] = state_frames(avoid_state, \n",
    "                                               np.zeros((1, AVOID_TOTAL_SENSORS * AVOID_STATE_FRAMES)),\n",
    "                                               AVOID_TOTAL_SENSORS, AVOID_STATE_FRAMES)\n",
    "\n",
    "    if cur_mode in [ACQUIRE, HUNT, PACK]:\n",
    "        \n",
    "        for i in range(NUM_DRONES): \n",
    "            acquire_states[i] = state_frames(acquire_state, \n",
    "                                             np.zeros((1, ACQUIRE_NUM_SENSOR * ACQUIRE_STATE_FRAMES)), \n",
    "                                             ACQUIRE_NUM_SENSOR, ACQUIRE_STATE_FRAMES)\n",
    "\n",
    "    if cur_mode in [HUNT, PACK]:\n",
    "        \n",
    "        for i in range(NUM_DRONES): \n",
    "            hunt_states[i] = state_frames(hunt_state, \n",
    "                                          np.zeros((1, HUNT_TOTAL_SENSORS * HUNT_STATE_FRAMES)), \n",
    "                                          HUNT_TOTAL_SENSORS, HUNT_STATE_FRAMES)\n",
    "\n",
    "    if cur_mode == PACK:\n",
    "        \n",
    "        for i in range(NUM_DRONES): \n",
    "            drone_states[i] = state_frames(drone_state, \n",
    "                                           np.zeros((1, DRONE_TOTAL_SENSOR * PACK_STATE_FRAMES)),\n",
    "                                           DRONE_TOTAL_SENSOR, PACK_STATE_FRAMES)\n",
    "\n",
    "        pack_state = state_frames(drone_state,\n",
    "                                  np.zeros((1, PACK_TOTAL_SENSORS * PACK_STATE_FRAMES)),\n",
    "                                  PACK_TOTAL_SENSORS, PACK_STATE_FRAMES)\n",
    "\n",
    "    # time it\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    # run frames\n",
    "    while total_frame_ctr < train_frames:\n",
    "        \n",
    "        total_frame_ctr += 1 # counts total training distance traveled\n",
    "        crash_frame_ctr += 1 # counts distance between crashes\n",
    "        replay_frame_ctr += 1 # counts frames between pack mode replay captures\n",
    "        \n",
    "        # used to slow things down for de-bugging\n",
    "        #time.sleep(0.25)\n",
    "        \n",
    "        for drone_id in range(NUM_DRONES): # NUM_DRONES = 1, unless you're in PACK mode\n",
    "            \n",
    "            speed_action = START_SPEED_ACTION\n",
    "            \n",
    "            # choose appropriate action(s)\n",
    "            '''note: only generates random inputs for currently training model.\n",
    "            All prior (sub) models provide their best (fully-trained) inputs.'''\n",
    "            if random.random() < epsilon or total_frame_ctr < observe: # epsilon degrades over flips...\n",
    "                if cur_mode == TURN:\n",
    "                    turn_action = set_turn_action(True, cur_speeds[drone_id],\n",
    "                                                  np.array([turn_states[drone_id]]))\n",
    "                else:\n",
    "                    if cur_mode in [AVOID, HUNT, PACK]:\n",
    "                        turn_action, turn_model = set_turn_action(False, cur_speeds[drone_id],\n",
    "                                                                  np.array([turn_states[drone_id]]))\n",
    "                    \n",
    "                    if cur_mode == AVOID:\n",
    "                        speed_action = set_avoid_action(True, turn_action,\n",
    "                                                        np.array([avoid_states[drone_id]]))\n",
    "                    else:\n",
    "                        if cur_mode in [HUNT, PACK]:\n",
    "                            speed_action = set_avoid_action(False, turn_action,\n",
    "                                                            np.array([avoid_states[drone_id]]))\n",
    "                        \n",
    "                        if cur_mode == ACQUIRE:\n",
    "                            acquire_action = set_acquire_action(True, cur_speeds[drone_id],\n",
    "                                                                np.array([acquire_states[drone_id,]]))\n",
    "                            turn_action = acquire_action\n",
    "                        else:\n",
    "                            acquire_action, acquire_model = \\\n",
    "                                set_acquire_action(False, cur_speeds[drone_id], \n",
    "                                                   np.array([acquire_states[drone_id,]]))\n",
    "                            \n",
    "                            if cur_mode == HUNT:\n",
    "                                hunt_action, turn_action, speed_action = \\\n",
    "                                    set_hunt_action(True, cur_speeds[drone_id], \n",
    "                                                    turn_action,speed_action, \n",
    "                                                    acquire_action, \n",
    "                                                    np.array([hunt_states[drone_id,]]))\n",
    "                            else:\n",
    "                                hunt_action, turn_action, speed_action = \\\n",
    "                                    set_hunt_action(False, cur_speeds[drone_id], \n",
    "                                                    turn_action, speed_action, acquire_action, \n",
    "                                                    np.array([hunt_states[drone_id,]]))\n",
    "                                \n",
    "                                if cur_mode == PACK and (total_frame_ctr == 1 or (replay_frame_ctr - 1) % PACK_EVAL_FRAMES == 0) and drone_id == 0:\n",
    "                                    pack_action = set_pack_action(True, pack_state)\n",
    "                                    '''note: pack action only changed every PACK_EVAL_FRAMES. \n",
    "                                    For frames in between it's constant'''\n",
    "\n",
    "            else: # ...increasing use of predictions over time\n",
    "                if cur_mode == TURN:\n",
    "                    turn_action, turn_model = set_turn_action(False, cur_speeds[drone_id],\n",
    "                                                              np.array([turn_states[drone_id]]))\n",
    "                else:\n",
    "                    if cur_mode in [AVOID, HUNT, PACK]:\n",
    "                        turn_action, turn_model = set_turn_action(False, cur_speeds[drone_id],\n",
    "                                                                  np.array([turn_states[drone_id]]))\n",
    "                    \n",
    "                    if cur_mode == AVOID:\n",
    "                        speed_action = set_avoid_action(False, turn_action,\n",
    "                                                        np.array([avoid_states[drone_id]]))\n",
    "                    else:\n",
    "                        if cur_mode in [HUNT, PACK]:\n",
    "                            speed_action = set_avoid_action(False, turn_action,\n",
    "                                                            np.array([avoid_states[drone_id]]))\n",
    "                        \n",
    "                        if cur_mode == ACQUIRE:\n",
    "                            acquire_action, acquire_model = \\\n",
    "                                set_acquire_action(False, cur_speeds[drone_id], \n",
    "                                                   np.array([acquire_states[drone_id,]]))\n",
    "                            turn_action = acquire_action\n",
    "                        else:\n",
    "                            acquire_action, acquire_model = \\\n",
    "                                set_acquire_action(False, cur_speeds[drone_id], \n",
    "                                                   np.array([acquire_states[drone_id,]]))\n",
    "                                                                \n",
    "                            if cur_mode == HUNT:\n",
    "                                hunt_action, turn_action, speed_action = \\\n",
    "                                    set_hunt_action(False, cur_speeds[drone_id], \n",
    "                                                    turn_action, speed_action, acquire_action, \n",
    "                                                    np.array([hunt_states[drone_id,]]))\n",
    "                            else:\n",
    "                                hunt_action, turn_action, speed_action = \\\n",
    "                                    set_hunt_action(False, cur_speeds[drone_id], \n",
    "                                                    turn_action, speed_action, acquire_action, \n",
    "                                                    np.array([hunt_states[drone_id,]]))\n",
    "                                                              \n",
    "                                if cur_mode == PACK and (total_frame_ctr == 1 or (replay_frame_ctr - 1) % PACK_EVAL_FRAMES == 0) and drone_id == 0:\n",
    "                                    # get 1 pack action for each set of drones on first drone\n",
    "                                    pack_action = set_pack_action(False, pack_state)\n",
    "        \n",
    "            new_turn_state, new_avoid_state, new_acquire_state, new_hunt_state, new_drone_state, new_reward, new_speed = \\\n",
    "                game_state.frame_step(drone_id, turn_action, speed_action, pack_action,\n",
    "                                      cur_speeds[drone_id], total_frame_ctr, replay_frame_ctr)\n",
    "            \n",
    "            # append (horizontally) historical states for learning speed.\n",
    "            '''note: do this concatination even for models that are not learning \n",
    "            (e.g., turn when running search or turn, search and acquire while running hunt) \n",
    "            b/c their preds, performed above, expect the same multi-frame view that was \n",
    "            in place when they trained.'''\n",
    "\n",
    "            if cur_mode in [TURN, AVOID, HUNT, PACK]:\n",
    "                new_turn_state = state_frames(new_turn_state,\n",
    "                                              np.array([turn_states[drone_id]]),\n",
    "                                              TURN_TOTAL_SENSORS,TURN_STATE_FRAMES)\n",
    "        \n",
    "            if cur_mode in [AVOID, HUNT, PACK]:\n",
    "                new_avoid_state = state_frames(new_avoid_state,\n",
    "                                               np.array([avoid_states[drone_id]]),\n",
    "                                               AVOID_TOTAL_SENSORS, AVOID_STATE_FRAMES)\n",
    "        \n",
    "            if cur_mode in [ACQUIRE, HUNT, PACK]:\n",
    "                new_acquire_state = state_frames(new_acquire_state,\n",
    "                                                 np.array([acquire_states[drone_id]]),\n",
    "                                                 ACQUIRE_NUM_SENSOR, ACQUIRE_STATE_FRAMES)\n",
    "\n",
    "            if cur_mode in [HUNT, PACK]:\n",
    "                new_hunt_state = state_frames(new_hunt_state,\n",
    "                                              np.array([hunt_states[drone_id]]),\n",
    "                                              HUNT_TOTAL_SENSORS, HUNT_STATE_FRAMES)\n",
    "\n",
    "            #print(4)\n",
    "            if cur_mode == PACK and (total_frame_ctr == 1 or replay_frame_ctr % PACK_EVAL_FRAMES == 0):\n",
    "                if drone_id == 0: # for 1st drone, pack state = drone state\n",
    "                    new_pack_state = new_drone_state\n",
    "                    pack_rwd = new_reward\n",
    "                \n",
    "                else: # otherwise, append drone record to prior drone state\n",
    "                    new_pack_state = state_frames(new_pack_state, new_drone_state,\n",
    "                                                  DRONE_TOTAL_SENSOR, 2)\n",
    "                    pack_rwd += new_reward\n",
    "                \n",
    "                new_drone_state = state_frames(new_drone_state,\n",
    "                                               np.array([drone_states[drone_id]]),\n",
    "                                               DRONE_TOTAL_SENSOR, PACK_STATE_FRAMES)\n",
    "\n",
    "                if drone_id == (NUM_DRONES - 1): # for last drone build pack record\n",
    "                    if total_frame_ctr == 1:\n",
    "                        pack_state = np.zeros((1, PACK_TOTAL_SENSORS * PACK_STATE_FRAMES))\n",
    "                        \n",
    "                    new_pack_state = state_frames(new_pack_state, pack_state, \n",
    "                                                  PACK_TOTAL_SENSORS, PACK_STATE_FRAMES) \n",
    "                    #may need to add 1 to PACK_STATE_FRAMES\n",
    "\n",
    "            #print(5)\n",
    "            # experience replay storage\n",
    "            \"\"\"note: only the model being trained requires event storage as it is \n",
    "            stack that will be sampled for training below.\"\"\"\n",
    "            if cur_mode == TURN:\n",
    "                replay.append((np.array([turn_states[drone_id]]),\n",
    "                              turn_action, new_reward, new_turn_state))\n",
    "\n",
    "            elif cur_mode == AVOID:\n",
    "                replay.append((np.array([avoid_states[drone_id]]),\n",
    "                               speed_action, new_reward, new_avoid_state))\n",
    "\n",
    "            elif cur_mode == ACQUIRE:\n",
    "                replay.append((np.array([acquire_states[drone_id]]),\n",
    "                               turn_action, new_reward, new_acquire_state))\n",
    "\n",
    "            elif cur_mode == HUNT:\n",
    "                replay.append((np.array([hunt_states[drone_id]]),\n",
    "                               hunt_action, new_reward, new_hunt_state))\n",
    "\n",
    "            elif cur_mode == PACK and (total_frame_ctr == 1 or replay_frame_ctr % PACK_EVAL_FRAMES == 0) and drone_id == (NUM_DRONES - 1):\n",
    "                replay.append((pack_state, pack_action, pack_rwd, new_pack_state))\n",
    "                #print(replay[-1])\n",
    "\n",
    "            #print(\"6a\")\n",
    "            # If we're done observing, start training.\n",
    "            if total_frame_ctr > observe and (cur_mode != PACK or (replay_frame_ctr % PACK_EVAL_FRAMES == 0 and drone_id == (NUM_DRONES - 1))):\n",
    "\n",
    "                # If we've stored enough in our buffer, pop the oldest.\n",
    "                if len(replay) > buffer:\n",
    "                    replay.pop(0)\n",
    "            \n",
    "                # Randomly sample our experience replay memory\n",
    "                minibatch = random.sample(replay, batchSize)\n",
    "\n",
    "                if cur_mode == TURN:\n",
    "                    # Get training values.\n",
    "                    X_train, y_train = process_minibatch(minibatch, turn_model,\n",
    "                                                         TURN_NUM_INPUT, TURN_NUM_OUTPUT)\n",
    "                    history = LossHistory()\n",
    "                    turn_model.fit(X_train, y_train, batch_size=batchSize,\n",
    "                                   nb_epoch=1, verbose=0, callbacks=[history])\n",
    "                \n",
    "                elif cur_mode == AVOID:\n",
    "                    X_train, y_train = process_minibatch(minibatch, avoid_model,\n",
    "                                                         AVOID_NUM_INPUT, AVOID_NUM_OUTPUT)\n",
    "                    history = LossHistory()\n",
    "                    avoid_model.fit(X_train, y_train, batch_size=batchSize,\n",
    "                                   nb_epoch=1, verbose=0, callbacks=[history])\n",
    "\n",
    "                elif cur_mode == ACQUIRE:\n",
    "                    X_train, y_train = process_minibatch(minibatch, acquire_model,\n",
    "                                                         ACQUIRE_NUM_INPUT, ACQUIRE_NUM_OUTPUT)\n",
    "                    history = LossHistory()\n",
    "                    acquire_model.fit(X_train, y_train, batch_size=batchSize,\n",
    "                                    nb_epoch=1, verbose=0, callbacks=[history])\n",
    "\n",
    "                elif cur_mode == HUNT:\n",
    "                    X_train, y_train = process_minibatch(minibatch, hunt_model,\n",
    "                                                         HUNT_NUM_INPUT, HUNT_NUM_OUTPUT)\n",
    "                    history = LossHistory()\n",
    "                    hunt_model.fit(X_train, y_train, batch_size=batchSize,\n",
    "                                      nb_epoch=1, verbose=0, callbacks=[history])\n",
    "\n",
    "                elif cur_mode == PACK:\n",
    "                    X_train, y_train = process_minibatch(minibatch, pack_model,\n",
    "                                                         PACK_NUM_INPUT, PACK_NUM_OUTPUT)\n",
    "                    history = LossHistory()\n",
    "                    pack_model.fit(X_train, y_train, batch_size=batchSize,\n",
    "                                   nb_epoch=1, verbose=0, callbacks=[history])\n",
    "\n",
    "                loss_log.append(history.losses)\n",
    "\n",
    "            # Update the starting state with S'.\n",
    "            if cur_mode in [TURN, AVOID, HUNT, PACK]:\n",
    "                turn_states[drone_id] = new_turn_state\n",
    "\n",
    "            if cur_mode in [AVOID, HUNT, PACK]:\n",
    "                avoid_states[drone_id] = new_avoid_state\n",
    "\n",
    "            if cur_mode in [ACQUIRE, HUNT, PACK]:\n",
    "                acquire_states[drone_id] = new_acquire_state\n",
    "\n",
    "            if cur_mode in [HUNT, PACK]:\n",
    "                hunt_states[drone_id] = new_hunt_state\n",
    "\n",
    "            if cur_mode == PACK and (total_frame_ctr == 1 or replay_frame_ctr % PACK_EVAL_FRAMES == 0):\n",
    "                drone_states[drone_id] = new_drone_state\n",
    "                \n",
    "                if drone_id == (NUM_DRONES - 1):\n",
    "                    pack_state = new_pack_state\n",
    "                    replay_frame_ctr = 0\n",
    "\n",
    "            cur_speeds[drone_id] = new_speed\n",
    "            cum_rwd += new_reward\n",
    "\n",
    "            # in case of crash, report and initialize\n",
    "            if new_reward == -500 or new_reward == -1000:\n",
    "                # Log the car's distance at this T.\n",
    "                data_collect.append([total_frame_ctr, crash_frame_ctr])\n",
    "\n",
    "                # new max achieved?\n",
    "                if crash_frame_ctr > max_crash_frame_ctr:\n",
    "                    max_crash_frame_ctr = crash_frame_ctr\n",
    "\n",
    "                # Time it.\n",
    "                tot_time = timeit.default_timer() - start_time\n",
    "                fps = crash_frame_ctr / tot_time\n",
    "\n",
    "                # output results to point of crash\n",
    "                #try:\n",
    "                print(\"Max: %d at %d\\t eps: %f\\t dist: %d\\t mode: %d\\t cum rwd: %d\\t fps: %d\" %\n",
    "                      (max_crash_frame_ctr, total_frame_ctr, epsilon, crash_frame_ctr, cur_mode, cum_rwd, int(fps)))\n",
    "                #    break\n",
    "                #except (RuntimeError, TypeError, NameError):\n",
    "                #    pass\n",
    "\n",
    "                # Reset.\n",
    "                crash_frame_ctr = cum_rwd = cum_speed = 0\n",
    "                start_time = timeit.default_timer()\n",
    "    \n",
    "        #print(9)\n",
    "        # decrement epsilon for another frame\n",
    "        if epsilon > 0.1 and total_frame_ctr > observe:\n",
    "            epsilon -= (1/train_frames)\n",
    "\n",
    "        if total_frame_ctr % 10000 == 0:\n",
    "            if crash_frame_ctr != 0:\n",
    "                #try:\n",
    "                print(\"Max: %d at %d\\t eps: %f\\t dist: %d\\t mode: %d\\t cum rwd: %d\" % (max_crash_frame_ctr, total_frame_ctr, epsilon, crash_frame_ctr, cur_mode,\n",
    "                       cum_rwd))\n",
    "                    #break\n",
    "                #except (RuntimeError, TypeError, NameError):\n",
    "                    #pass\n",
    "    \n",
    "        # Save model every 50k frames\n",
    "        if total_frame_ctr % 50000 == 0:\n",
    "            save_init = False\n",
    "            if cur_mode == TURN:\n",
    "                turn_model.save_weights('models/turn/turn-' + filename + '-' +\n",
    "                                        str(START_SPEED) + '-' + str(total_frame_ctr) + '.h5',overwrite=True)\n",
    "                print(\"Saving turn_model %s - %d - %d\" % (filename, START_SPEED,total_frame_ctr))\n",
    "            \n",
    "            elif cur_mode == AVOID:\n",
    "                avoid_model.save_weights('models/avoid/avoid-' + filename + '-' +\n",
    "                                         str(total_frame_ctr) + '.h5', overwrite=True)\n",
    "                print(\"Saving avoid_model %s - %d\" % (filename,total_frame_ctr))\n",
    "            \n",
    "            elif cur_mode == ACQUIRE:\n",
    "                acquire_model.save_weights('models/acquire/acquire-' + filename + '-' +\n",
    "                                           str(START_SPEED) + '-' + str(total_frame_ctr) + '.h5',overwrite=True)\n",
    "                print(\"Saving acquire_model %s - %d\" % (filename,total_frame_ctr))\n",
    "            \n",
    "            elif cur_mode == HUNT:\n",
    "                hunt_model.save_weights('models/hunt/hunt-' + filename + '-' +\n",
    "                                           str(total_frame_ctr) + '.h5', overwrite=True)\n",
    "                print(\"Saving hunt_model %s - %d\" % (filename,total_frame_ctr))\n",
    "\n",
    "            elif cur_mode == PACK:\n",
    "                pack_model.save_weights('models/pack/pack-' + filename + '-' +\n",
    "                            str(total_frame_ctr) + '.h5', overwrite=True)\n",
    "                print(\"Saving pack_model %s - %d\" % (filename, total_frame_ctr))\n",
    "\n",
    "    # Log results after we're done all frames.\n",
    "    log_results(filename, data_collect, loss_log)\n",
    "\n",
    "def set_turn_action(random_fl, cur_speed, turn_state):\n",
    "    if random_fl:\n",
    "        turn_action = np.random.randint(0, TURN_NUM_OUTPUT)\n",
    "        return turn_action\n",
    "    else:\n",
    "        if cur_mode == TURN and use_existing_model == False:\n",
    "            turn_qval = turn_model.predict(turn_state, batch_size=1)\n",
    "            turn_model = turn_model\n",
    "        else:\n",
    "            if cur_speed == SPEEDS[0]:\n",
    "                turn_qval = turn_model_30.predict(turn_state, batch_size=1)\n",
    "                turn_model = turn_model_30\n",
    "            elif cur_speed == SPEEDS[1]:\n",
    "                turn_qval = turn_model_50.predict(turn_state, batch_size=1)\n",
    "                turn_model = turn_model_50\n",
    "            elif cur_speed == SPEEDS[2]:\n",
    "                turn_qval = turn_model_70.predict(turn_state, batch_size=1)\n",
    "                turn_model = turn_model_70\n",
    "        turn_action = (np.argmax(turn_qval))\n",
    "        return turn_action, turn_model\n",
    "\n",
    "def set_avoid_action(random_fl, turn_action, avoid_state):\n",
    "    if random_fl:\n",
    "        speed_action = np.random.randint(0, AVOID_NUM_OUTPUT)\n",
    "    else:\n",
    "        avoid_state[0][14] = turn_action # ensures AVOID using current turn pred\n",
    "        avoid_qval = avoid_model.predict(avoid_state, batch_size=1)\n",
    "        speed_action = (np.argmax(avoid_qval))\n",
    "            \n",
    "    return speed_action\n",
    "\n",
    "def set_acquire_action(random_fl, cur_speed, acquire_state):\n",
    "    if random_fl:\n",
    "        turn_action = np.random.randint(0, ACQUIRE_NUM_OUTPUT)\n",
    "        return turn_action\n",
    "    else:\n",
    "        if cur_mode == ACQUIRE and use_existing_model == False:\n",
    "            acquire_qval = acquire_model.predict(acquire_state, batch_size=1)\n",
    "            acquire_model = acquire_model\n",
    "        else:\n",
    "            if cur_speed == SPEEDS[0]:\n",
    "                acquire_qval = acquire_model_50.predict(acquire_state, batch_size=1)\n",
    "                acquire_model = acquire_model_50\n",
    "\n",
    "            elif cur_speed == SPEEDS[1]:\n",
    "                acquire_qval = acquire_model_50.predict(acquire_state, batch_size=1)\n",
    "                acquire_model = acquire_model_50\n",
    "\n",
    "            else:\n",
    "                acquire_qval = acquire_model_70.predict(acquire_state, batch_size=1)\n",
    "                acquire_model = acquire_model_70\n",
    "\n",
    "        turn_action = (np.argmax(acquire_qval))\n",
    "        return turn_action, acquire_model\n",
    "\n",
    "def set_hunt_action(random_fl, cur_speed, turn_action, speed_action, acquire_action, hunt_state):\n",
    "    if random_fl:\n",
    "        hunt_action = np.random.randint(0, HUNT_NUM_OUTPUT)\n",
    "        if hunt_action == HUNT_AVOID: # accept speed model action\n",
    "            turn_action = turn_action\n",
    "            if cur_speed > 0:\n",
    "                speed_action = speed_action # continue current speed\n",
    "            else:\n",
    "                speed_action = 1 # reset speed to 50, as you were stopped\n",
    "            #avoid_ctr += 1\n",
    "                \n",
    "        elif hunt_action == HUNT_ACQUIRE: # accept acquire model action\n",
    "            turn_action = acquire_action\n",
    "            if cur_speed > 0:\n",
    "                speed_action = 1 # just setting acquire speed to 50 for now\n",
    "            else:\n",
    "                speed_action = 1 # reset speed to 50, as you were stopped\n",
    "                \n",
    "    else:\n",
    "        hunt_qval = hunt_model.predict(hunt_state, batch_size=1)\n",
    "        hunt_action = (np.argmax(hunt_qval))\n",
    "        \n",
    "        if hunt_action == HUNT_AVOID: # accept avoid model action\n",
    "            turn_action = turn_action\n",
    "            if cur_speed > 0:\n",
    "                speed_action = speed_action # continue current speed\n",
    "            else:\n",
    "                speed_action = 1\n",
    "                 \n",
    "        elif hunt_action == HUNT_ACQUIRE: # accept acquire model action\n",
    "            turn_action = acquire_action\n",
    "            if cur_speed > 0:\n",
    "                speed_action = 1 # just setting acquire speed to 50 for now\n",
    "            else:\n",
    "                speed_action = 1 # reset acquire speed to 50, as you were stopped\n",
    "            \n",
    "    return hunt_action, turn_action, speed_action\n",
    "\n",
    "\n",
    "def set_pack_action(random_fl, pack_state):\n",
    "    if random_fl:\n",
    "        pack_action = np.random.randint(0, PACK_NUM_OUTPUT)\n",
    "    else:\n",
    "        pack_qval = pack_model.predict(pack_state, batch_size=1)\n",
    "        pack_action = (np.argmax(pack_qval))\n",
    "\n",
    "    return pack_action\n",
    "\n",
    "\n",
    "def state_frames(new_state, old_state, num_sensor, num_frame):\n",
    "    \"\"\"\n",
    "    Takes a state returned from the game and turns it into a multi-frame state.\n",
    "    Create a new array with the new state and first N of old state,\n",
    "    which was the previous frame's new state.\n",
    "   \"\"\"\n",
    "    # Turn them back into arrays.\n",
    "    new_state = new_state.tolist()[0]\n",
    "    old_state = old_state.tolist()[0][:num_sensor * (num_frame - 1)]\n",
    "\n",
    "    # Combine them.\n",
    "    combined_state = new_state + old_state\n",
    "    \n",
    "    # Re-numpy them on exit.\n",
    "    return np.array([combined_state])\n",
    "\n",
    "def log_results(filename, data_collect, loss_log):\n",
    "    # Save the results to a file so we can graph it later.\n",
    "    with open('results/sonar-frames/learn_data-' + filename + '.csv', 'w') as data_dump:\n",
    "        wr = csv.writer(data_dump)\n",
    "        wr.writerows(data_collect)\n",
    "\n",
    "    with open('results/sonar-frames/loss_data-' + filename + '.csv', 'w') as lf:\n",
    "        wr = csv.writer(lf)\n",
    "        for loss_item in loss_log:\n",
    "            wr.writerow(loss_item)\n",
    "\n",
    "def process_minibatch(minibatch, model, num_input, num_output):\n",
    "    \"\"\"This does the heavy lifting, aka, the training. It's super jacked.\"\"\"\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    # Loop through our batch and create arrays for X and y\n",
    "    # so that we can fit our model at every step.\n",
    "    for memory in minibatch:\n",
    "        # Get stored values.\n",
    "        old_state_m, action_m, reward_m, new_state_m = memory\n",
    "\n",
    "        # Get prediction on old state.\n",
    "        old_qval = model.predict(old_state_m, batch_size=1)\n",
    "\n",
    "        # Get prediction on new state.\n",
    "        newQ = model.predict(new_state_m, batch_size=1)\n",
    "        \n",
    "        # Get our best move. I think?\n",
    "        maxQ = np.max(newQ)\n",
    "        \n",
    "        y = np.zeros((1, num_output)) # was 3.\n",
    "        y[:] = old_qval[:]\n",
    "        \n",
    "        # Check for terminal state.\n",
    "        if reward_m != -500:  # non-terminal state\n",
    "            update = (reward_m + (GAMMA * maxQ))\n",
    "        \n",
    "        else:  # terminal state\n",
    "            update = reward_m\n",
    "        \n",
    "        # Update the value for the action we took.\n",
    "        y[0][action_m] = update\n",
    "        X_train.append(old_state_m.reshape(num_input,))\n",
    "        y_train.append(y.reshape(num_output,))\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    return X_train, y_train\n",
    "\n",
    "def params_to_filename(params):\n",
    "    if len(params['nn']) == 1:\n",
    "    \n",
    "        return str(params['nn'][0]) + '-' + str(params['batchSize']) + \\\n",
    "        '-' + str(params['buffer'])\n",
    "    \n",
    "    elif len(params['nn']) == 2:\n",
    "        \n",
    "        return str(params['nn'][0]) + '-' + str(params['nn'][1]) + '-' + \\\n",
    "        str(params['batchSize']) + '-' + str(params['buffer'])\n",
    "\n",
    "    elif len(params['nn']) == 3:\n",
    "\n",
    "        return str(params['nn'][0]) + '-' + str(params['nn'][1]) + '-' + \\\n",
    "            str(params['nn'][2]) + '-' + str(params['batchSize']) + '-' + str(params['buffer'])\n",
    "\n",
    "def launch_learn(params):\n",
    "    filename = params_to_filename(params)\n",
    "    print(\"Trying %s\" % filename)\n",
    "    # Make sure we haven't run this one.\n",
    "    if not os.path.isfile('results/sonar-frames/loss_data-' + filename + '.csv'):\n",
    "        # Create file so we don't double test when we run multiple\n",
    "        # instances of the script at the same time.\n",
    "        open('results/sonar-frames/loss_data-' + filename + '.csv', 'a').close()\n",
    "        print(\"Starting test.\")\n",
    "        # Train.\n",
    "        if cur_mode == TURN:\n",
    "            turn_model = turn_net(NUM_INPUT, params['nn'])\n",
    "            train_net(turn_model, 0, params)\n",
    "        elif cur_mode == AVOID:\n",
    "            avoid_model = avoid_net(NUM_INPUT, params['nn'])\n",
    "            train_net(0, avoid_model, params)\n",
    "    else:\n",
    "        print(\"Already tested.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section below loads trained neural nets as required to support training of higher level networks. For networks being trained, it initializes network by calling appropriate neural network schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    turn_model = turn_model_30 = turn_model_50 = turn_model_70 = avoid_model = \\\n",
    "    acquire_model = acquire_model_30 = acquire_model_50 = acquire_model_70 = \\\n",
    "    hunt_model = pack_model = 0\n",
    "        \n",
    "    if cur_mode in [TURN, AVOID, HUNT, PACK]:\n",
    "        nn_param = [TURN_NUM_INPUT*25, TURN_NUM_INPUT*10]\n",
    "        params = {\"batchSize\": 100, \"buffer\": 50000, \"nn\": nn_param}\n",
    "            \n",
    "        if cur_mode == TURN and use_existing_model == False:\n",
    "            turn_model = turn_net(TURN_NUM_INPUT, nn_param, TURN_NUM_OUTPUT)\n",
    "        \n",
    "        else:\n",
    "            saved_model = 'models/turn/saved/turn-750-300-100-50000-30-600000.h5'\n",
    "            turn_model_30 = turn_net(TURN_NUM_INPUT, nn_param, TURN_NUM_OUTPUT, saved_model)\n",
    "            saved_model = 'models/turn/saved/turn-750-300-100-50000-50-600000.h5'\n",
    "            turn_model_50 = turn_net(TURN_NUM_INPUT, nn_param, TURN_NUM_OUTPUT, saved_model)\n",
    "            saved_model = 'models/turn/saved/turn-750-300-100-50000-70-600000.h5'\n",
    "            turn_model_70 = turn_net(TURN_NUM_INPUT, nn_param,TURN_NUM_OUTPUT, saved_model)\n",
    "    \n",
    "    if cur_mode in [AVOID, HUNT, PACK]:\n",
    "        nn_param = [AVOID_NUM_INPUT * 25, AVOID_NUM_INPUT * 5, AVOID_NUM_INPUT]\n",
    "        params = {\"batchSize\": 100, \"buffer\": 50000, \"nn\": nn_param}\n",
    "            \n",
    "        if cur_mode == AVOID and use_existing_model == False:\n",
    "            avoid_model = avoid_net(AVOID_NUM_INPUT, nn_param, AVOID_NUM_OUTPUT)\n",
    "            \n",
    "        else:\n",
    "            saved_model = 'models/avoid/saved/avoid-1200-240-48-100-50000-700000-old-3L-2022.h5'\n",
    "            avoid_model = avoid_net(AVOID_NUM_INPUT, nn_param, AVOID_NUM_OUTPUT, saved_model)\n",
    "\n",
    "    if cur_mode in [ACQUIRE, HUNT, PACK]:\n",
    "        nn_param = [ACQUIRE_NUM_INPUT * 15, ACQUIRE_NUM_INPUT * 5]\n",
    "        params = {\"batchSize\": 100, \"buffer\": 50000, \"nn\": nn_param}\n",
    "            \n",
    "        if cur_mode == ACQUIRE and use_existing_model == False:\n",
    "            acquire_model = acquire_net(ACQUIRE_NUM_INPUT, nn_param, ACQUIRE_NUM_OUTPUT)\n",
    "        \n",
    "        else:\n",
    "            saved_model = 'models/acquire/saved/acquire-60-20-100-50000-50-350000.h5'\n",
    "            # using 50 until time to re-train at 30\n",
    "            acquire_model_30 = acquire_net(ACQUIRE_NUM_INPUT, nn_param,\n",
    "                                           ACQUIRE_NUM_OUTPUT, saved_model)\n",
    "            saved_model = 'models/acquire/saved/acquire-60-20-100-50000-50-350000.h5'\n",
    "            acquire_model_50 = acquire_net(ACQUIRE_NUM_INPUT, nn_param,\n",
    "                                           ACQUIRE_NUM_OUTPUT, saved_model)\n",
    "            saved_model = 'models/acquire/saved/acquire-60-20-100-50000-70-350000.h5'\n",
    "            acquire_model_70 = acquire_net(ACQUIRE_NUM_INPUT, nn_param,\n",
    "                                           ACQUIRE_NUM_OUTPUT, saved_model)\n",
    "        \n",
    "    if cur_mode in [HUNT, PACK]:\n",
    "        nn_param = [HUNT_NUM_INPUT * 25, HUNT_NUM_INPUT * 5, HUNT_NUM_INPUT]\n",
    "        params = {\"batchSize\": 100, \"buffer\": 50000, \"nn\": nn_param}\n",
    "            \n",
    "        if cur_mode == HUNT and use_existing_model == False:\n",
    "            hunt_model = hunt_net(HUNT_NUM_INPUT, nn_param, HUNT_NUM_OUTPUT)\n",
    "            \n",
    "        else:\n",
    "            saved_model = 'models/hunt/saved/hunt-1200-240-48-100-50000-300000-40-50-avoid.h5'\n",
    "            hunt_model = hunt_net(HUNT_NUM_INPUT, nn_param, HUNT_NUM_OUTPUT, saved_model)\n",
    "        \n",
    "    if cur_mode == PACK:\n",
    "        nn_param = [PACK_NUM_INPUT * 10]\n",
    "        params = {\"batchSize\": 100, \"buffer\": 50000, \"nn\": nn_param}\n",
    "            \n",
    "        if cur_mode == PACK and use_existing_model == False:\n",
    "            pack_model = pack_net(PACK_NUM_INPUT, nn_param, PACK_NUM_OUTPUT)\n",
    "    \n",
    "        else:\n",
    "            saved_model = 'models/pack/pack-360-100-50000-200000.h5'\n",
    "            pack_model = pack_net(PACK_NUM_INPUT, nn_param, PACK_NUM_OUTPUT, saved_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************** target acquired ************\n",
      "pct complete: 5.3475935828877e-05\n",
      "screen shot taken\n",
      "************** target acquired ************\n",
      "pct complete: 0.000106951871657754\n",
      "************** target acquired ************\n",
      "pct complete: 0.00016042780748663101\n",
      "************** target acquired ************\n",
      "pct complete: 0.000213903743315508\n",
      "************** target acquired ************\n",
      "pct complete: 0.00026737967914438503\n",
      "************** target acquired ************\n",
      "pct complete: 0.00032085561497326203\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-c0c355b65f72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m train_net(turn_model, turn_model_30, turn_model_50, turn_model_70, avoid_model,\n\u001b[1;32m      4\u001b[0m           \u001b[0macquire_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macquire_model_30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macquire_model_50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macquire_model_70\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           hunt_model, pack_model, params)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-8a3e3c97402b>\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(turn_model, turn_model_30, turn_model_50, turn_model_70, avoid_model, acquire_model, acquire_model_30, acquire_model_50, acquire_model_70, hunt_model, pack_model, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpack_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mnew_turn_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_avoid_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_acquire_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_hunt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_drone_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_speed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrone_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mturn_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeed_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpack_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_speeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdrone_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_frame_ctr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_frame_ctr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;31m# append (horizontally) historical states for learning speed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-8fcf6f1d81f8>\u001b[0m in \u001b[0;36mframe_step\u001b[0;34m(self, drone_id, turn_action, speed_action, pack_action, cur_speed, total_ctr, replay_ctr)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdrone_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mturn_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavoid_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macquire_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhunt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrone_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_sonar_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavoid_move_efficiency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macquire_move_efficiency\u001b[0m \u001b[0;34m=\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrone_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mturn_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheading_adjust\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_speed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;31m# calc rewards based on training mode(s) in effect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-8fcf6f1d81f8>\u001b[0m in \u001b[0;36mbuild_states\u001b[0;34m(self, drone_id, turn_action, heading_adjust, cur_speed)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;31m# get readings from the various sensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0msonar_dist_readings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msonar_color_readings\u001b[0m \u001b[0;34m=\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sonar_dist_color_readings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrone_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0mturn_readings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msonar_dist_readings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTURN_NUM_SENSOR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-8fcf6f1d81f8>\u001b[0m in \u001b[0;36mget_sonar_dist_color_readings\u001b[0;34m(self, drone_id)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshow_sensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m             \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msonar_dist_readings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msonar_color_readings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "load_models()\n",
    "\n",
    "train_net(turn_model, turn_model_30, turn_model_50, turn_model_70, avoid_model,\n",
    "          acquire_model, acquire_model_30, acquire_model_50, acquire_model_70,\n",
    "          hunt_model, pack_model, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Section below is used to run games "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "def play(turn_model, turn_model_30, turn_model_50, turn_model_70, avoid_model, acquire_model,\n",
    "         acquire_model_30, acquire_model_50, acquire_model_70, hunt_model, pack_model, params):\n",
    "\n",
    "    total_frame_ctr = 0\n",
    "    crash_frame_ctr = 0\n",
    "    replay_frame_ctr = 0\n",
    "    crash_ctr = 0\n",
    "    acquire_ctr = 0\n",
    "    cum_speed = 0\n",
    "    stop_ctr = avoid_ctr = acquire_ctr = 0\n",
    "    cur_speeds = []\n",
    "    for i in range(NUM_DRONES): cur_speeds.append(START_SPEED)\n",
    "    \n",
    "    # initialize drone state holders\n",
    "    turn_states = np.zeros([NUM_DRONES, TURN_TOTAL_SENSORS * TURN_STATE_FRAMES])\n",
    "    avoid_states = np.zeros([NUM_DRONES, AVOID_TOTAL_SENSORS * AVOID_STATE_FRAMES])\n",
    "    acquire_states = np.zeros([NUM_DRONES, ACQUIRE_NUM_SENSOR * ACQUIRE_STATE_FRAMES])\n",
    "    hunt_states = np.zeros([NUM_DRONES, HUNT_TOTAL_SENSORS * HUNT_STATE_FRAMES])\n",
    "    drone_states = np.zeros([NUM_DRONES, DRONE_TOTAL_SENSOR * PACK_STATE_FRAMES])\n",
    "\n",
    "    # create game instance\n",
    "    game_state = GameState()\n",
    "\n",
    "    # get initial state(s)\n",
    "    turn_state, avoid_state, acquire_state, hunt_state, drone_state, reward, cur_speed = \\\n",
    "        game_state.frame_step(START_DRONE_ID, START_TURN_ACTION, START_SPEED_ACTION,\n",
    "                              START_PACK_ACTION, START_SPEED, START_DISTANCE, 1)\n",
    "    \n",
    "    # initialize frame states\n",
    "    if cur_mode in [TURN, AVOID, HUNT, PACK]:\n",
    "        \n",
    "        for i in range(NUM_DRONES): turn_states[i] = state_frames(turn_state, np.zeros((1, TURN_TOTAL_SENSORS * TURN_STATE_FRAMES)), TURN_TOTAL_SENSORS, TURN_STATE_FRAMES)\n",
    "        \n",
    "        if cur_mode in [AVOID, HUNT, PACK]:\n",
    "            \n",
    "            for i in range(NUM_DRONES): avoid_states[i] = state_frames(avoid_state, np.zeros((1, AVOID_TOTAL_SENSORS * AVOID_STATE_FRAMES)),AVOID_TOTAL_SENSORS, AVOID_STATE_FRAMES)\n",
    "\n",
    "    if cur_mode in [ACQUIRE, HUNT, PACK]:\n",
    "    \n",
    "        for i in range(NUM_DRONES): acquire_states[i] = state_frames(acquire_state, np.zeros((1, ACQUIRE_NUM_SENSOR * ACQUIRE_STATE_FRAMES)), ACQUIRE_NUM_SENSOR, ACQUIRE_STATE_FRAMES)\n",
    "    \n",
    "    if cur_mode in [HUNT, PACK]:\n",
    "        \n",
    "        for i in range(NUM_DRONES): hunt_states[i] = state_frames(hunt_state, np.zeros((1, HUNT_TOTAL_SENSORS * HUNT_STATE_FRAMES)), HUNT_TOTAL_SENSORS, HUNT_STATE_FRAMES)\n",
    "    \n",
    "    if cur_mode == PACK:\n",
    "        \n",
    "        for i in range(NUM_DRONES): drone_states[i] = state_frames(drone_state, np.zeros((1, DRONE_TOTAL_SENSOR * PACK_STATE_FRAMES)),DRONE_TOTAL_SENSOR, PACK_STATE_FRAMES)\n",
    "        \n",
    "        #pack_state = state_frames(drone_state,\n",
    "        #                          np.zeros((1, PACK_TOTAL_SENSORS * PACK_STATE_FRAMES)),\n",
    "        #                          PACK_TOTAL_SENSORS, PACK_STATE_FRAMES)\n",
    "                                  \n",
    "        pack_state = state_frames(drone_state, np.zeros((1, 30)),10, 4)\n",
    "    \n",
    "    # Move.\n",
    "    while True:\n",
    "        \n",
    "        total_frame_ctr += 1\n",
    "        crash_frame_ctr += 1\n",
    "        replay_frame_ctr += 1\n",
    "        \n",
    "        #time.sleep(1)\n",
    "        \n",
    "        for drone_id in range(NUM_DRONES): # NUM_DRONES = 1, unless you're in PACK mode\n",
    "        \n",
    "            speed_action = START_SPEED_ACTION\n",
    "            \n",
    "            # choose action\n",
    "            if cur_mode == TURN:\n",
    "                turn_action, turn_model = set_turn_action(False, cur_speeds[drone_id], np.array([turn_states[drone_id]]))\n",
    "            else:\n",
    "                if cur_mode in [AVOID, HUNT, PACK]:\n",
    "                    turn_action, turn_model = set_turn_action(False, cur_speeds[drone_id],np.array([turn_states[drone_id]]))\n",
    "                                                                      \n",
    "                if cur_mode == AVOID:\n",
    "                    speed_action = set_avoid_action(False, turn_action, np.array([avoid_states[drone_id]]))\n",
    "                else:\n",
    "                    if cur_mode in [HUNT, PACK]:\n",
    "                        speed_action = set_avoid_action(False, turn_action, np.array([avoid_states[drone_id]]))\n",
    "                                                                                          \n",
    "                    if cur_mode == ACQUIRE:\n",
    "                        acquire_action, acquire_model = set_acquire_action(False, cur_speeds[drone_id], np.array([acquire_states[drone_id,]]))\n",
    "                        turn_action = acquire_action\n",
    "                    else:\n",
    "                        acquire_action, acquire_model = set_acquire_action(False, cur_speeds[drone_id], np.array([acquire_states[drone_id,]]))\n",
    "                                                                                                              \n",
    "                        if cur_mode == HUNT:\n",
    "                            hunt_action, turn_action, speed_action = set_hunt_action(False, cur_speeds[drone_id], turn_action, speed_action, acquire_action, np.array([hunt_states[drone_id,]]))\n",
    "                        else:\n",
    "                            hunt_action, turn_action, speed_action = set_hunt_action(False, cur_speeds[drone_id], turn_action, speed_action, acquire_action, np.array([hunt_states[drone_id,]]))\n",
    "                            \n",
    "                            if cur_mode == PACK and (total_frame_ctr == 1 or replay_frame_ctr % PACK_EVAL_FRAMES == 0) and drone_id == 0:\n",
    "                                # get 1 pack action for each set of drones on first drone\n",
    "                                pack_action = set_pack_action(False, pack_state)\n",
    "\n",
    "            # pass action, receive new state, reward\n",
    "            new_turn_state, new_avoid_state, new_acquire_state, new_hunt_state, new_drone_state, new_reward, new_speed = game_state.frame_step(drone_id, turn_action, speed_action, pack_action, cur_speeds[drone_id], total_frame_ctr, replay_frame_ctr)\n",
    "\n",
    "            # append (horizontally) historical states for learning speed.\n",
    "            if cur_mode in [TURN, AVOID, HUNT, PACK]:\n",
    "                new_turn_state = state_frames(new_turn_state,\n",
    "                                              np.array([turn_states[drone_id]]),\n",
    "                                              TURN_TOTAL_SENSORS,TURN_STATE_FRAMES)\n",
    "            \n",
    "            if cur_mode in [AVOID, HUNT, PACK]:\n",
    "                new_avoid_state = state_frames(new_avoid_state,\n",
    "                                               np.array([avoid_states[drone_id]]),\n",
    "                                               AVOID_TOTAL_SENSORS, AVOID_STATE_FRAMES)\n",
    "            \n",
    "            if cur_mode in [ACQUIRE, HUNT, PACK]:\n",
    "                new_acquire_state = state_frames(new_acquire_state,\n",
    "                                                 np.array([acquire_states[drone_id]]),\n",
    "                                                 ACQUIRE_NUM_SENSOR, ACQUIRE_STATE_FRAMES)\n",
    "            \n",
    "            if cur_mode in [HUNT, PACK]:\n",
    "                new_hunt_state = state_frames(new_hunt_state,\n",
    "                                              np.array([hunt_states[drone_id]]),\n",
    "                                              HUNT_TOTAL_SENSORS, HUNT_STATE_FRAMES)\n",
    "                    \n",
    "            if cur_mode == PACK and (total_frame_ctr == 1 or replay_frame_ctr % PACK_EVAL_FRAMES == 0):\n",
    "                if drone_id == 0: # for 1st drone, pack state = drone state\n",
    "                    new_pack_state = new_drone_state\n",
    "                    pack_rwd = new_reward\n",
    "                        \n",
    "                else: # otherwise, append drone record to prior drone state\n",
    "                    new_pack_state = state_frames(new_pack_state, new_drone_state,\n",
    "                                                  DRONE_TOTAL_SENSOR, PACK_STATE_FRAMES - 1)\n",
    "                    pack_rwd += new_reward\n",
    "                        \n",
    "                new_drone_state = state_frames(new_drone_state,\n",
    "                                               np.array([drone_states[drone_id]]),\n",
    "                                               DRONE_TOTAL_SENSOR, PACK_STATE_FRAMES)\n",
    "            \n",
    "                if drone_id == (NUM_DRONES - 1): # for last drone build pack record\n",
    "                    if total_frame_ctr == 1:\n",
    "                        pack_state = np.zeros((1, PACK_TOTAL_SENSORS * PACK_STATE_FRAMES))\n",
    "                    \n",
    "                    new_pack_state = state_frames(new_pack_state, pack_state, PACK_TOTAL_SENSORS, PACK_STATE_FRAMES) #may need to add 1 to PACK_STATE_FRAMES\n",
    "                        \n",
    "            # Update the starting state with S'.\n",
    "            if cur_mode in [TURN, AVOID, HUNT, PACK]:\n",
    "                turn_states[drone_id] = new_turn_state\n",
    "            \n",
    "            if cur_mode in [AVOID, HUNT, PACK]:\n",
    "                avoid_states[drone_id] = new_avoid_state\n",
    "            \n",
    "            if cur_mode in [ACQUIRE, HUNT, PACK]:\n",
    "                acquire_states[drone_id] = new_acquire_state\n",
    "            \n",
    "            if cur_mode in [HUNT, PACK]:\n",
    "                hunt_states[drone_id] = new_hunt_state\n",
    "            \n",
    "            if cur_mode == PACK and (total_frame_ctr == 1 or replay_frame_ctr % PACK_EVAL_FRAMES == 0):\n",
    "                drone_states[drone_id] = new_drone_state\n",
    "                \n",
    "                if drone_id == (NUM_DRONES - 1):\n",
    "                    pack_state = new_pack_state\n",
    "                    replay_frame_ctr = 0\n",
    "        \n",
    "            cur_speeds[drone_id] = new_speed\n",
    "        \n",
    "        # give status\n",
    "        if new_reward == -500 or new_reward == -1000:\n",
    "            crash_ctr += 1\n",
    "            print(\"crashes\", crash_ctr, \"frames\", total_frame_ctr)\n",
    "        elif new_reward == 1000:\n",
    "            acquire_ctr += 1\n",
    "            print(\"acquisitions:\", acquire_ctr, \"frames\", total_frame_ctr)\n",
    "        \n",
    "        if total_frame_ctr % 5000 == 0:\n",
    "            print(\"***** total frames:\", total_frame_ctr)\n",
    "            print(\"***** frames between crashes:\", int(total_frame_ctr / crash_ctr))\n",
    "                  \n",
    "            if cur_mode in [ACQUIRE, HUNT, PACK]:\n",
    "                  \n",
    "                print(\"***** frames / acquisition:\", int(total_frame_ctr / acquire_ctr))\n",
    "            #stop_ctr = avoid_ctr = acquire_ctr = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('shapes (1,36) and (42,420) not aligned: 36 (dim 1) != 42 (dim 0)', (1, 36), (42, 420))\nApply node that caused the error: Dot22(<TensorType(float32, matrix)>, <TensorType(float32, matrix)>)\nToposort index: 2\nInputs types: [TensorType(float32, matrix), TensorType(float32, matrix)]\nInputs shapes: [(1, 36), (42, 420)]\nInputs strides: [(144, 4), (1680, 4)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[Elemwise{Composite{(i0 * (Abs((i1 + i2)) + i1 + i2))}}[(0, 1)](TensorConstant{(1, 1) of 0.5}, Dot22.0, InplaceDimShuffle{x,0}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    911\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/theano/tensor/blas.py\u001b[0m in \u001b[0;36mperform\u001b[0;34m(self, node, inp, out)\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1530\u001b[0;31m             \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1531\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('shapes (1,36) and (42,420) not aligned: 36 (dim 1) != 42 (dim 0)', (1, 36), (42, 420))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-33074f623e8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m play(turn_model, turn_model_30, turn_model_50, turn_model_70, avoid_model,\n\u001b[1;32m      5\u001b[0m      \u001b[0macquire_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macquire_model_30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macquire_model_50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macquire_model_70\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m      hunt_model, pack_model, params)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-62b92c3f92cc>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(turn_model, turn_model_30, turn_model_50, turn_model_70, avoid_model, acquire_model, acquire_model_30, acquire_model_50, acquire_model_70, hunt_model, pack_model, params)\u001b[0m\n\u001b[1;32m     99\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mcur_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPACK\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_frame_ctr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreplay_frame_ctr\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mPACK_EVAL_FRAMES\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdrone_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                                 \u001b[0;31m# get 1 pack action for each set of drones on first drone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                                 \u001b[0mpack_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_pack_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpack_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;31m# pass action, receive new state, reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-8a3e3c97402b>\u001b[0m in \u001b[0;36mset_pack_action\u001b[0;34m(random_fl, pack_state)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0mpack_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPACK_NUM_OUTPUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0mpack_qval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpack_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0mpack_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpack_qval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, batch_size, verbose)\u001b[0m\n\u001b[1;32m    659\u001b[0m         '''\n\u001b[1;32m    660\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    869\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[1;32m    872\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/theano/gof/link.py\u001b[0m in \u001b[0;36mraise_with_op\u001b[0;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;31m# extra long error message in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/theano/tensor/blas.py\u001b[0m in \u001b[0;36mperform\u001b[0;34m(self, node, inp, out)\u001b[0m\n\u001b[1;32m   1528\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1530\u001b[0;31m             \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1531\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m             \u001b[0;31m# The error raised by numpy has no shape information, we mean to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('shapes (1,36) and (42,420) not aligned: 36 (dim 1) != 42 (dim 0)', (1, 36), (42, 420))\nApply node that caused the error: Dot22(<TensorType(float32, matrix)>, <TensorType(float32, matrix)>)\nToposort index: 2\nInputs types: [TensorType(float32, matrix), TensorType(float32, matrix)]\nInputs shapes: [(1, 36), (42, 420)]\nInputs strides: [(144, 4), (1680, 4)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[Elemwise{Composite{(i0 * (Abs((i1 + i2)) + i1 + i2))}}[(0, 1)](TensorConstant{(1, 1) of 0.5}, Dot22.0, InplaceDimShuffle{x,0}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "load_models()\n",
    "\n",
    "play(turn_model, turn_model_30, turn_model_50, turn_model_70, avoid_model,\n",
    "     acquire_model, acquire_model_30, acquire_model_50, acquire_model_70,\n",
    "     hunt_model, pack_model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
